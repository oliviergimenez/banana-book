---
title: "Finite-mixture capture-recapture model: A simulation study."
output: html_document
date: "2023-08-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Motivation

I use simulations to explore various methods to fix a capture-recapture model with finite mixtures à la Shirley Pledger. 

# Simulations

```{r}
r <- set.seed(1234) # for reproducibility

# Generate heterogeneous detection probabilities
phi <- 0.7 # survival
prop_class1 <- 0.2 # pi
p_class1 <- 0.8 # low prop are highly detectable
p_class2 <- 0.3
nind <- 400 # nb of ind
nyear <- 10 # duration of the study
expit <- function(x){exp(x)/(1+exp(x))} # reciprocal of the logit function 
z <- data <- x <- matrix(NA, nrow = nind, ncol = nyear)
first <- rep(1,nind)
detection <- rep(NA,nind)
which_mixture <- rep(NA,nind)
# first assign ind to a class, then use corresponding detection
for (i in 1:nind){
  which_mixture[i] <- rbinom(1,1,prop_class1) # assign ind i to a class with prob pi
  if (which_mixture[i] == 1){ detection[i] <- p_class1  
  } else {
    detection[i] <- p_class2} 
}

# Generate encounter histories
for(i in 1:nind){
  z[i,first[i]] <- x[i,first[i]] <- 1 
  for(j in (first[i]+1):nyear){
    z[i,j] <- rbinom(1,1,phi*z[i,j-1])
    x[i,j] <- rbinom(1,1,z[i,j]*detection[i]) }
}
y <- x 
y[is.na(y)] <- 0
```

# Maximum-likelihood

## Useful functions

```{r}
########################################################################
# calcul nouveaux marques par occasion
########################################################################
premieredetection= function(x){
  b=sort(x,index.return=T,decreasing=T)
  c=b$ix[1]
  return(c)
}

########################################################################
# fonction pour éviter l'explosion de log(0)
########################################################################
logprot <- function(v){
  eps <- 2.2204e-016
  u <- log(eps) * (1+vector(length=length(v)))
  index <- (v>eps)
  u[index] <- log(v[index])
  u
}

############################################################
# fonction pour eviter backward sequence
# CC from wrapr package
seqi <- function (a, b) 
{
  a <- ceiling(a)
  b <- floor(b)
  if (a > b) {
    return(integer(0))
  }
  seq(a, b, by = 1L)
}

#################################################################################
# pi, phi, p(mix) ##########################
#################################################################################
devCJSmix <- function(b,data,eff,e,garb,nh,km1)
{
  # data est le bloc de CH, eff les effectifs (on peut imaginer grouper les individus qui ont des histoires communes)
  # e le vecteur des dates des premières capture,
  # lc le vecteur des dates de dernières captures (LECA vs. PASS et ANTA)
  # garb le vecteur des états de départ, 
  # s le nombre d'états (cachés),
  # m le nombre d'événements (observations)
  # km1 le nombre d'occasions de capture - 1
  # nh nb individus
  
  #---------------------------------------------
  # lien logit pour les survies (transience)
  phi <- 1/(1+exp(-b[1]))
  pp1 <- 1/(1+exp(-b[2]))
  pp2 <- 1/(1+exp(-b[3]))
  prop <- 1/(1+exp(-b[4]))
  #---------------------------------------------
  
  
  #---------------------------------------------
  # Probabilités des événements (lignes) conditionnellement aux états (colonnes)
  B <- matrix(c(1-pp1,1-pp2,1,
                pp1,pp2,0),
              nrow=2,ncol=3,byrow=T)
  
  # Output initiaux
  BE <- matrix(c(0,0,1,
                 1,1,0),nrow=2,ncol=3,byrow=T) 
  
  # Matrice stochastique de transitions entre états (processus Markovien)
  # age 1
  PHI <- matrix(c(phi,0,1-phi,
                   0,phi,1-phi,
                   0,0,1),nrow=3,ncol=3,byrow=T)

  # Distribution des états initiaux
  PI <- c(prop,1-prop,0)
  #---------------------------------------------
  
  #---------------------------------------------
  # on forme la vraisemblance, ou plutôt la log(vraisemblance) 
  # on linéarise car c'est plus stable numériquement de maximiser une somme qu'un produit
  # en fait, on forme -log(vraisemblance) plutôt, car R permet seulement de minimiser des fonctions
  # voir Pradel 2005, 2009
  
  l <- 0
  for (i in 1:nh) # boucles sur histoires de capture
  {
    ei <- e[i] # date marquage
    oe <- garb[i] + 1 # événement initial
    evennt <- data[,i] + 1 # les non-détections deviennent des 1, et les détections des 2
    ALPHA <- PI*BE[oe,]
    for (j in seqi(ei+1,km1+1)) # on conditionne par rapport à la première capture
    {
      ALPHA <- (ALPHA %*% PHI)*B[evennt[j],]
    }
    l <- l + logprot(sum(ALPHA))*eff[i]
  }
  l <- -l
  l
  #---------------------------------------------
  
} # fin fn vrais
```

## Model fitting w/ RMark

```{r}
k <- ncol(y)	
n <- nrow(y)	
out <- array(dim = n)	
for (i in 1:n){	
	his <- (y[i,] > 0) * 1	
	out[i] <- paste(his,collapse="")	
}	
capt.hist <- data.frame(ch = out)	

# load RMark package	
library(RMark)	
dat.proc <- process.data(capt.hist, model = "CJSMixture")	
dat.ddl <- make.design.data(dat.proc)	

# detection	
p.mix <- list(formula=~mixture) # mixture	
# proportion	
pi.dot <- list(formula=~1) # constant	
# survival	
phi.dot <- list(formula=~1) # constant	

model.het <- mark(data = dat.proc,
                 ddl = dat.ddl,
                 model.parameters = list(Phi = phi.dot,
                                         p = p.mix,
                                         pi = pi.dot),
                 output = FALSE,
                 delete = T)	

prop <- as.numeric(model.het$results$real[1,1:4])
phi <- as.numeric(model.het$results$real[2,1:4])
det1 <- as.numeric(model.het$results$real[3,1:4])
det2 <- as.numeric(model.het$results$real[4,1:4])

data.frame(param = c("phi","prop","pp1","pp2"),
           true = c(0.7, prop_class1, p_class1, p_class2),
           mle = round(c(phi[1],1-prop[1],det2[1],det1[1]),2),
           ci = round(rbind(phi[3:4],rev(1-prop[3:4]),det2[3:4],det1[3:4]),2))

```


## Model fitting with MLE by hand

```{r}
s <- 3 # nb etats
m <- 2 # nb events
k <- dim(y)[2]
km1 <- k-1
nh <- dim(y)[1]
# effectifs
eff <- rep(1,nh)

# compute the date of first capture fc, and state at initial capture init.state
fc <- NULL
init.state <- NULL
for (jj in 1:nh){
  temp <- 1:k
  fc <- c(fc,min(temp[y[jj,]==1]))
  init.state <- c(init.state, y[jj,fc[jj]])
}

# transpose data
y <- t(y)

# nb parametres des 21 modeles
nb_param <- 4

# pick current model
mod <- 'devCJSmix'

binit <- runif(nb_param)
tpmin <- optim(par = binit,
               fn = eval(parse(text = mod)),
               gr = NULL,
               hessian = TRUE, 
               y,eff,fc,init.state,nh,km1,
               method = "BFGS",
               control = list(trace = 1, REPORT = 1, maxit = 500))

x <- tpmin$par
fisher_info <- solve(0.5* tpmin$hessian)
#fisher_info <- MASS::ginv(0.5* res_mdl$hessian)
prop_sigma <- sqrt(diag(fisher_info))

2 * tpmin$value + 2 * nb_param

# Get estimates and conf intervals
estim <- function(bb, prop_sigma){
  param <- plogis(bb)
  IClower <- plogis(bb - 1.96 * prop_sigma)
  ICupper <- plogis(bb + 1.96 * prop_sigma)
  list(mle = param, CI = c(IClower, ICupper))
}

phi <- estim(x[1], prop_sigma[1])
prop <- estim(x[4], prop_sigma[4])
pp1 <- estim(x[2], prop_sigma[2])
pp2 <- estim(x[3], prop_sigma[3])

data.frame(param = c("phi","prop","pp1","pp2"),
           true = c(0.7, prop_class1, p_class1, p_class2),
           mle = round(c(phi$mle,prop$mle,pp1$mle,pp2$mle),2),
           ci = round(rbind(phi$CI,
                            prop$CI,
                            pp1$CI,
                            pp2$CI),2))

```

Cool thing is that I get exactly same estimates than with RMark. Reassuring re: my likelihood coded by hand.

# Bayesian fitting

## Naive implementation with default samplers and full likelihood

```{r}
library(nimble)
```


```{r}
hmm.phipmix <- nimbleCode({
  
# -------------------------------------------------
# Parameters:
# pi: initial state probability C1
# phi: survival probability
# pp1: recapture probability C1
# pp2: recapture probability C2
# -------------------------------------------------
# States (S):
# 1 alive (A1)
# 2 alive (A2)
# 5 dead (D)
# Observations (O):
# 1 neither seen nor recovered (0)
# 2 seen alive (1)
# -------------------------------------------------

  # priors
  phi ~ dunif(0, 1) # prior survival
  one ~ dconstraint(pp1 < pp2) # to avoid label switching
  pp1 ~ dunif(0, 1) # prior detection
  pp2 ~ dunif(0, 1) # prior detection
  pi ~ dunif(0, 1) # prob init state 1
  # transition matrix
  gamma[1,1] <- phi      # A1(t)->A1(t+1)
  gamma[1,2] <- 0        # A1(t)->A2(t+1)
  gamma[1,3] <- 1 - phi  # A1(t)->D(t+1)
  gamma[2,1] <- 0        # A2(t)->A1(t+1)
  gamma[2,2] <- phi      # A2(t)->A2(t+1)
  gamma[2,3] <- 1 - phi  # A2(t)->D(t+1)
  gamma[3,1] <- 0        # D(t)->A1(t+1)
  gamma[3,2] <- 0        # D(t)->A2(t+1)
  gamma[3,3] <- 1        # D(t)->D(t+1)

  # vector of initial state probs
  delta[1] <- pi         # A1(first)
  delta[2] <- 1 - pi     # A2(first)
  delta[3] <- 0          # D(first)
  
  # observation matrix
  omega[1,1] <- 1 - pp1   # A1(t)->0(t)
  omega[1,2] <- pp1       # A1(t)->1(t)
  omega[2,1] <- 1 - pp2   # A2(t)->0(t)
  omega[2,2] <- pp2       # A2(t)->1(t)
  omega[3,1] <- 1        # D(t)->0(t)
  omega[3,2] <- 0        # D(t)->1(t)

  # first encounter t = first
  omegae[1,1] <- 0   # A1(t)->0(t)
  omegae[1,2] <- 1       # A1(t)->1(t)
  omegae[2,1] <- 0   # A2(t)->0(t)
  omegae[2,2] <- 1       # A2(t)->1(t)
  omegae[3,1] <- 1        # D(t)->0(t)
  omegae[3,2] <- 0        # D(t)->1(t)
    
  # likelihood
  for (i in 1:N){
    z[i,first[i]] ~ dcat(delta[1:3])
    y[i,first[i]] ~ dcat(omegae[z[i,first[i]], 1:2])
    for (j in (first[i]+1):K){
      z[i,j] ~ dcat(gamma[z[i,j-1], 1:3])
      y[i,j] ~ dcat(omega[z[i,j], 1:2])
    }
  }
})
```

```{r}
# Data w/ righ dimensions
if (nrow(y) < ncol(y)) y <- t(y)

# Constants in a list. 
my.constants <- list(N = nrow(y), 
                     K = ncol(y), 
                     first = fc)
# Data in a list. 
my.data <- list(y = y + 1, one = 1)
# Initial values. 
zinit <- y
for (i in 1:nrow(y)) {
  for (j in first[i]:ncol(y)) {
    if (j == first[i]) zinit[i,j] <- which(rmultinom(1, 1, c(2/3,1/3))==1) # pick alive state
    #if (j == first[i]) zinit[i,j] <- 2 # pick alive state
    if (j > first[i]) zinit[i,j] <- zinit[i,j-1]
  }     
}
zinit <- as.matrix(zinit)
initial.values <- function() list(phi = runif(1,0,1),
                                  pp1 = 0.3,
                                  pp2 = 0.8,
                                  pi = runif(1,0,1),
                                  z = as.matrix(zinit))
# Parameters to be monitored. 
parameters.to.save <- c("phi", "pp1", "pp2", "pi")
# MCMC details. 
n.iter <- 5000
n.burnin <- 1000
n.chains <- 2
```

Run NIMBLE:
```{r}
mcmc.phipmix <- nimbleMCMC(code = hmm.phipmix, 
                           constants = my.constants,
                           data = my.data,              
                           inits = initial.values,
                           monitors = parameters.to.save,
                           niter = n.iter,
                           nburnin = n.burnin, 
                           nchains = n.chains)
# Numerical summaries. 
MCMCvis::MCMCsummary(mcmc.phipmix, round = 2)
```

Results are not satisfying. True survival is 0.7 so we're fine. But proportion and detection probs in each class are far from 0.2 with detection 0.8 and 0.8 with detection 0.3. Suspect it's the categorical sampler for the latent states and the initial values that are playing us nasty tricks. For example, if we set all initial values for latent states to alive in the same class for all individuals, then we get:

```{r}
zinit <- y
for (i in 1:nrow(y)) {
  for (j in first[i]:ncol(y)) {
    #if (j == first[i]) zinit[i,j] <- which(rmultinom(1, 1, c(2/3,1/3))==1) # pick alive state
    if (j == first[i]) zinit[i,j] <- 2 # pick alive state
    if (j > first[i]) zinit[i,j] <- zinit[i,j-1]
  }     
}
zinit <- as.matrix(zinit)
initial.values <- function() list(phi = runif(1,0,1),
                                  pp1 = 0.3,
                                  pp2 = 0.8,
                                  pi = runif(1,0,1),
                                  z = as.matrix(zinit))
mcmc.phipmix <- nimbleMCMC(code = hmm.phipmix, 
                           constants = my.constants,
                           data = my.data,              
                           inits = initial.values,
                           monitors = parameters.to.save,
                           niter = n.iter,
                           nburnin = n.burnin, 
                           nchains = n.chains)
# Numerical summaries. 
MCMCvis::MCMCsummary(mcmc.phipmix, round = 2)
```

Well, detection in class 1 gets estimated near 0, which is consistent with the chains being stuck in latent state class 2 as we set up the initial values for latent states. Proportion tells same story.  

## Change samplers

Let's change the samplers on latent states for slice. First we're going back to random inits.

```{r}
zinit <- y
for (i in 1:nrow(y)) {
  for (j in first[i]:ncol(y)) {
    if (j == first[i]) zinit[i,j] <- which(rmultinom(1, 1, c(2/3,1/3))==1) # pick alive state
    #if (j == first[i]) zinit[i,j] <- 2 # pick alive state
    if (j > first[i]) zinit[i,j] <- zinit[i,j-1]
  }     
}
zinit <- as.matrix(zinit)
initial.values <- function() list(phi = runif(1,0,1),
                                  pp1 = 0.3,
                                  pp2 = 0.8,
                                  pi = runif(1,0,1),
                                  z = as.matrix(zinit))

```

Then detailed workflow.
```{r}
# Model code
survival <- nimbleModel(code = hmm.phipmix,
                        constants = my.constants,
                        data = my.data,              
                        inits = initial.values()
)
survival$calculate()
# Compile
Csurvival <- compileNimble(survival)
# Assign samplers
#survivalConf <- configureMCMC(survival,onlySlice = T)
survivalConf <- configureMCMC(survival)
#survivalConf$printSamplers()
# Remplace all categorical samplers by slice samples
# https://groups.google.com/g/nimble-users/c/Yvwu2idBRMM/m/uoU0ppU5AAAJ
samplerConfs <- survivalConf$getSamplers()
CATbool <- sapply(samplerConfs, `[[`, 'name') == 'categorical'
CATtargets <- sapply(samplerConfs[CATbool], `[[`, 'target')
survivalConf$replaceSamplers(CATtargets, type = 'slice', expandTarget = TRUE)
# double check that everything went well
#survivalConf$printSamplers()
survivalConf$printSamplers(c("phi", "pp1", "pp2", "pi"))
# Resume the workflow 
survivalMCMC <- buildMCMC(survivalConf)
CsurvivalMCMC <- compileNimble(survivalMCMC, project = survival)
# Fit model
samples <- runMCMC(mcmc = CsurvivalMCMC, 
                   niter = n.iter,
                   nburnin = n.burnin,
                   nchains = 2)
MCMCvis::MCMCsummary(samples)
```

It's giving funky results. Sometimes the chains do not converge, sometimes they do. When they do, survival is estimated close to 1, the proportion is 0.80 and detection probs are 0.10 and 0.30. SD for survival and detection in classe 1 are 0.004 and 0.006 respectively !! As a reminder, true survival is 0.7, and re: observation, we have prop 0.2 with detection 0.8 and 0.8 with detection 0.3. Again, far from what we should get. 

## Marginalized likelihood

I write the marginalized likelihood using the forward algo for HMM. And I pooled the encounter histories. 

```{r}
# get rid of individuals for which first==K
mask <- which(fc!=ncol(y)) # individuals that are not first encountered at last occasion
y <- y[mask, ]                # keep only these
fc <- fc[mask]
# pool encounter histories
y_weighted <- y %>% 
  as_tibble() %>% 
  group_by_all() %>% 
  summarise(size = n()) %>% 
  relocate(size) %>% 
  as.matrix()
head(y_weighted)
size <- y_weighted[,1] # nb of individuals w/ a particular encounter history
y <- y_weighted[,-1] # pooled data
my.data <- list(y = y + 1)
my.constants <- list(N = nrow(y), 
                     K = ncol(y), 
                     first = fc,
                     size = size,
                     one = 1)
```

NIMBLE functions
```{r}
dwolfHMM <- nimbleFunction(
  run = function(x = double(1), 
                 probInit = double(1), # vector of initial states
                 probObs = double(2), #observation matrix
                 probObse = double(2),
                 probTrans = double(2), # transition matrix
                 size = double(0),
                 len = double(0, default = 0), # number of sampling occasions
                 log = integer(0, default = 0)) {
    alpha <- probInit[1:3] * probObse[1:3,x[1]]# * probObs[1:3,x[1]] == 1 due to conditioning on first detection
    for (t in 2:len) {
      alpha[1:3] <- (alpha[1:3] %*% probTrans[1:3,1:3]) * probObs[1:3,x[t]]
    }
    logL <- size * log(sum(alpha[1:3]))
    returnType(double(0))
    if (log) return(logL)
    return(exp(logL))
  }
)

rwolfHMM <- nimbleFunction(
  run = function(n = integer(),
                 probInit = double(1),
                 probObs = double(2),
                 probObse = double(2),
                 probTrans = double(2),
                 size = double(0),
                 len = double(0, default = 0)) {
    returnType(double(1))
    z <- numeric(len)
    z[1] <- rcat(n = 1, prob = probInit[1:3]) # all individuals alive at t = 0
    y <- z
    y[1] <- 2 # all individuals are detected at t = 0
    for (t in 2:len){
      # state at t given state at t-1
      z[t] <- rcat(n = 1, prob = probTrans[z[t-1],1:3]) 
      # observation at t given state at t
      y[t] <- rcat(n = 1, prob = probObs[z[t],1:2]) 
    }
    return(y)
  })

assign('dwolfHMM', dwolfHMM, .GlobalEnv)
assign('rwolfHMM', rwolfHMM, .GlobalEnv)
```

NIMBLE code again
```{r}
hmm.phipmix <- nimbleCode({
  
# -------------------------------------------------
# Parameters:
# pi: initial state probability C1
# phi: survival probability
# pp1: recapture probability C1
# pp2: recapture probability C2
# -------------------------------------------------
# States (S):
# 1 alive (A1)
# 2 alive (A2)
# 5 dead (D)
# Observations (O):
# 1 neither seen nor recovered (0)
# 2 seen alive (1)
# -------------------------------------------------

  # priors
  phi ~ dunif(0, 1) # prior survival
  one ~ dconstraint(pp1 < pp2) # to avoid label switching
  pp1 ~ dunif(0, 1) # prior detection
  pp2 ~ dunif(0, 1) # prior detection
  pi ~ dunif(0, 1) # prob init state 1
  # transition matrix
  gamma[1,1] <- phi      # A1(t)->A1(t+1)
  gamma[1,2] <- 0        # A1(t)->A2(t+1)
  gamma[1,3] <- 1 - phi  # A1(t)->D(t+1)
  gamma[2,1] <- 0        # A2(t)->A1(t+1)
  gamma[2,2] <- phi      # A2(t)->A2(t+1)
  gamma[2,3] <- 1 - phi  # A2(t)->D(t+1)
  gamma[3,1] <- 0        # D(t)->A1(t+1)
  gamma[3,2] <- 0        # D(t)->A2(t+1)
  gamma[3,3] <- 1        # D(t)->D(t+1)

  # vector of initial state probs
  delta[1] <- pi         # A1(first)
  delta[2] <- 1 - pi     # A2(first)
  delta[3] <- 0          # D(first)
  
  # observation matrix
  omega[1,1] <- 1 - pp1   # A1(t)->0(t)
  omega[1,2] <- pp1       # A1(t)->1(t)
  omega[2,1] <- 1 - pp2   # A2(t)->0(t)
  omega[2,2] <- pp2       # A2(t)->1(t)
  omega[3,1] <- 1        # D(t)->0(t)
  omega[3,2] <- 0        # D(t)->1(t)

  omegae[1,1] <- 0   # A1(t)->0(t)
  omegae[1,2] <- 1       # A1(t)->1(t)
  omegae[2,1] <- 0   # A2(t)->0(t)
  omegae[2,2] <- 1       # A2(t)->1(t)
  omegae[3,1] <- 1        # D(t)->0(t)
  omegae[3,2] <- 0        # D(t)->1(t)

    # likelihood
  for(i in 1:N) {
    y[i,first[i]:K] ~ dwolfHMM(probInit = delta[1:3],  # count data from first[i] + 1
                               probObs = omega[1:3,1:2],     # observation matrix
                               probObse = omegae[1:3,1:2],     # observation matrix
                               probTrans = gamma[1:3,1:3],   # transition matrix
                               size = size[i],
                               len = K - first[i] + 1)           # nb of occasions
  }
})
```

Some stuff
```{r}
# initial values (cool thing, we do not need inits for the latent states anymore!!)
initial.values <- function() list(phi = runif(1,0,1),
                                  pp1 = 0.3,
                                  pp2 = 0.8,
                                  pi = runif(1,0,1))
# Parameters to be monitored. 
parameters.to.save <- c("phi", "pp1", "pp2", "pi")
```

Run NIMBLE:
```{r}
mcmc.phipmix <- nimbleMCMC(code = hmm.phipmix, 
                           constants = my.constants,
                           data = my.data,              
                           inits = initial.values,
                           monitors = parameters.to.save,
                           niter = n.iter,
                           nburnin = n.burnin, 
                           nchains = n.chains)
# Numerical summaries. 
MCMCvis::MCMCsummary(mcmc.phipmix, round = 2)
```

We're back in business! Remember, true survival is 0.7 and we have prop 0.2 with detection 0.8 and 0.8 with detection 0.3. 

## Feedbacks from Daniel Turek

Daniel was kind enough to have a look, here's what he says:


> Bottom line, the way this model is written is completely destined for failure. The state-space formulation that's written, with the two different groups for individuals (high detection and low detection) sets the model up for failure, and it will never work correctly when written this way. Briefly:

> For any individual that is seen at any time t > 1 (that is, individuals seen again after the first sighting), the detection history looks something like:
1 0 0 1 .....  (1 represents detection).
The initial values for the latent z state are either :
1 1 1 1 1.....
or
2 2 2 2 2
putting that individual into one of the two groups (1 or 2 always).
when sampling, the latent z can *never * transition to the other group, from group 2 to group 1, or from group 1 to group 2.  It will be stuck where ever it started.
If the categorical sampler tries to change the *final* state from the initial value of (say) 2 to 1,
then this transition is deemed to be impossible by the prior (defined by the gamma state transition matrix), since the state at the previous time was 2, and 2 in one period (the previous period) does not permit a state of 1 in the next time period.
SImilarly, if the first (or any intermediate) value of z attempts to transition from (say) 2 to 1, then the *following* state is still 1, and that dependency does not allow the state in question to change to 1, because a state of 1 cannot have the next state be 2.
Even if some "dead" states (3's) are added to the end of the z vector over the course of MCMC sampling, and say z becomes:
1 1 1 1 3 3
The 3's can never propagate "earlier" than shown here (since there are detections at t=1 and t=4, so the individual cannot be in state 3 at time t=4), so the problem described above will always be the case, and this individual will *always* remain in group 1, no matter how long you run the MCMC.

> The only time an individual (with the model written as such) could change between groups (1 -> 2, or 2 -> 1) from their initial group assignment of zinit, would be if the individual is *only* observed on the first time period, the detection history is:
1 0 0 0 0 0,
Then say the initial value for z is:
1 1 1 1 1 1 (always in group 1),
then the sampler at some point could begin transitioning the final 1 into state 3 (dead), so after an MCMC iteration, the z for this individual could be:
1 1 1 1 1 3,
then if we're lucky, the sampler on the final 1 would some time change it to a 3:
1 1 1 1 3 3
then this could happen again later:
1 1 1 3 3 3
and again:
1 1 3 3 3 3
and once again:
1 3 3 3 3 3
and *only now*, finally, if we're lucky, the sampler operating on the first value of the z vector could change this group assignment from 1 to 2:
2 3 3 3 3 3
And that's the only situation when individuals can possibly change groups in this model.

> The problem again, is that any individual seen > 1 time (it's resighted after the first observation occasion) can *never* change group assignments away from their initial value group assignment.  So, this model is destined for failure.

> There are many ways one could fix this:
- Marginalize over the z's as you did (perhaps using nimbleEcology)
- Write the model differently, using a binary latent indicator variable to represent the group assignment of each individual.  This could also work for > 2 groups, where the group indicator variable follows a categorical prior
- Use a latent state formulation as you have, but write a custom MCMC sampler to update entire rows of the z matrix (the latent state variables for one individual for all time periods) simultaneously, thus enabling transitions between groups
- Probably other ways, also.

To which I answered: 

> This is awesome, thank you very much for your clear explanation. This is gonna be a great example actually, which will allow me to illustrate when things go wrong, the interest of using simulations, marginalization and write your own sampler. Regarding the latter I will probably give it a try next month, and if you don’t mind, I’ll certainly bother you again.

> Lionel, for your specific case study, I’ve already recommended nimbleEcology ;-) There are examples in Daniel’s paper on nonparametric Bayes stats specific to the heterogeneity context, see here for some code https://github.com/danielturek/bnp-ecology-examples. The simplest approach might be to use a latent indicator to assign animals to classes, this is actually what we’ve done here https://onlinelibrary.wiley.com/doi/10.1002/ece3.5139. 

