<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.4 Approximating posteriors via numerical integration | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models" />
<meta property="og:type" content="book" />

<meta property="og:description" content="This is a textbook on the analysis of capture-recapture data with hidden Markov models (HMM) implemented in the Bayesian framework with R." />
<meta name="github-repo" content="oliviergimenez/banana-book" />

<meta name="author" content="Olivier Gimenez" />

<meta name="date" content="2022-12-23" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This is a textbook on the analysis of capture-recapture data with hidden Markov models (HMM) implemented in the Bayesian framework with R.">

<title>1.4 Approximating posteriors via numerical integration | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#welcome" id="toc-welcome">Welcome</a>
<ul>
<li><a href="license.html#license" id="toc-license">License</a></li>
</ul></li>
<li class="has-sub"><a href="preface.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="why-this-book.html#why-this-book" id="toc-why-this-book">Why this book?</a></li>
<li><a href="who-should-read-this-book.html#who-should-read-this-book" id="toc-who-should-read-this-book">Who should read this book?</a></li>
<li><a href="what-will-you-learn.html#what-will-you-learn" id="toc-what-will-you-learn">What will you learn?</a></li>
<li><a href="what-wont-you-learn.html#what-wont-you-learn" id="toc-what-wont-you-learn">What won’t you learn?</a></li>
<li><a href="prerequisites.html#prerequisites" id="toc-prerequisites">Prerequisites</a></li>
<li><a href="acknowledgements.html#acknowledgements" id="toc-acknowledgements">Acknowledgements</a></li>
<li><a href="how-this-book-was-written.html#how-this-book-was-written" id="toc-how-this-book-was-written">How this book was written</a></li>
</ul></li>
<li><a href="about-the-author.html#about-the-author" id="toc-about-the-author">About the author</a></li>
<li class="part"><span><b>I I. Fundations</b></span></li>
<li><a href="introduction.html#introduction" id="toc-introduction">Introduction</a></li>
<li class="has-sub"><a href="1-crashcourse.html#crashcourse" id="toc-crashcourse"><span class="toc-section-number">1</span> Bayesian statistics &amp; MCMC</a>
<ul>
<li><a href="1.1-introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">1.1</span> Introduction</a></li>
<li><a href="1.2-bayes-theorem.html#bayes-theorem" id="toc-bayes-theorem"><span class="toc-section-number">1.2</span> Bayes’ theorem</a></li>
<li><a href="1.3-what-is-the-bayesian-approach.html#what-is-the-bayesian-approach" id="toc-what-is-the-bayesian-approach"><span class="toc-section-number">1.3</span> What is the Bayesian approach?</a></li>
<li><a href="1.4-numerical-approx.html#numerical-approx" id="toc-numerical-approx"><span class="toc-section-number">1.4</span> Approximating posteriors via numerical integration</a></li>
<li class="has-sub"><a href="1.5-markov-chain-monte-carlo-mcmc.html#markov-chain-monte-carlo-mcmc" id="toc-markov-chain-monte-carlo-mcmc"><span class="toc-section-number">1.5</span> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#monte-carlo-integration" id="toc-monte-carlo-integration"><span class="toc-section-number">1.5.1</span> Monte Carlo integration</a></li>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#markovmodelmcmc" id="toc-markovmodelmcmc"><span class="toc-section-number">1.5.2</span> Markov chains</a></li>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#metropolis-algorithm" id="toc-metropolis-algorithm"><span class="toc-section-number">1.5.3</span> Metropolis algorithm</a></li>
</ul></li>
<li class="has-sub"><a href="1.6-convergence-diag.html#convergence-diag" id="toc-convergence-diag"><span class="toc-section-number">1.6</span> Assessing convergence</a>
<ul>
<li><a href="1.6-convergence-diag.html#burn-in" id="toc-burn-in"><span class="toc-section-number">1.6.1</span> Burn-in</a></li>
<li><a href="1.6-convergence-diag.html#chain-length" id="toc-chain-length"><span class="toc-section-number">1.6.2</span> Chain length</a></li>
<li><a href="1.6-convergence-diag.html#what-if-you-have-issues-of-convergence" id="toc-what-if-you-have-issues-of-convergence"><span class="toc-section-number">1.6.3</span> What if you have issues of convergence?</a></li>
</ul></li>
<li><a href="1.7-summary.html#summary" id="toc-summary"><span class="toc-section-number">1.7</span> Summary</a></li>
<li><a href="1.8-suggested-reading.html#suggested-reading" id="toc-suggested-reading"><span class="toc-section-number">1.8</span> Suggested reading</a></li>
</ul></li>
<li><a href="faq.html#faq" id="toc-faq">FAQ</a></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="numerical-approx" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Approximating posteriors via numerical integration</h2>
<p>Let’s take an example to illustrate Bayes’ theorem. Say we capture, mark and release <span class="math inline">\(n = 57\)</span> animals at the beginning of a winter, out of which we recapture <span class="math inline">\(y = 19\)</span> animals alive<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. We’d like to estimate winter survival <span class="math inline">\(\theta\)</span>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="1.4-numerical-approx.html#cb2-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">19</span> <span class="co"># nb of success</span></span>
<span id="cb2-2"><a href="1.4-numerical-approx.html#cb2-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">57</span> <span class="co"># nb of attempts</span></span></code></pre></div>
<p>We build our model first. Assuming all animals are independent of each other and have the same survival probability, then <span class="math inline">\(y\)</span> the number of alive animals at the end of the winter is a binomial distribution<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> with <span class="math inline">\(n\)</span> trials and <span class="math inline">\(\theta\)</span> the probability of success:</p>
<p><span class="math display">\[\begin{align*}
y &amp;\sim \text{Binomial}(n, \theta) &amp;\text{[likelihood]}
\end{align*}\]</span></p>
<p>This likelihood can be visualised in <code>R</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="1.4-numerical-approx.html#cb3-1" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>) <span class="co"># grid of values for survival</span></span>
<span id="cb3-2"><a href="1.4-numerical-approx.html#cb3-2" aria-hidden="true" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="fu">dbinom</span>(y, n, grid) <span class="co"># compute binomial likelihood</span></span>
<span id="cb3-3"><a href="1.4-numerical-approx.html#cb3-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">survival =</span> grid, <span class="at">likelihood =</span> likelihood) </span>
<span id="cb3-4"><a href="1.4-numerical-approx.html#cb3-4" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span></span>
<span id="cb3-5"><a href="1.4-numerical-approx.html#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb3-6"><a href="1.4-numerical-approx.html#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> survival, <span class="at">y =</span> likelihood) <span class="sc">+</span> </span>
<span id="cb3-7"><a href="1.4-numerical-approx.html#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:binlik"></span>
<img src="banana-book_files/figure-html/binlik-1.png" alt="Binomial likelihood with $n = 57$ released animals and $y = 19$ survivors after winter. The value of survival (on the x-axis) that corresponds to the maximum of the likelihood function (on the y-axis) is the MLE, or the proportion of success in this example, close to 0.33." width="672" />
<p class="caption">
Figure 1.3: Binomial likelihood with <span class="math inline">\(n = 57\)</span> released animals and <span class="math inline">\(y = 19\)</span> survivors after winter. The value of survival (on the x-axis) that corresponds to the maximum of the likelihood function (on the y-axis) is the MLE, or the proportion of success in this example, close to 0.33.
</p>
</div>
<p>Besides the likelihood, priors are another component of the model in the Bayesian approach. For a parameter that is a probability, the one thing we know is that the prior should be a continuous random variable that lies between 0 and 1. To reflect that, we often go for the uniform distribution <span class="math inline">\(U(0,1)\)</span> to imply <em>vague</em> priors. Here vague means that survival has, before we see the data, the same probability of falling between 0.1 and 0.2 and between 0.8 and 0.9, for example.</p>
<p><span class="math display">\[\begin{align*}
\theta &amp;\sim \text{Uniform}(0, 1) &amp;\text{[prior for }\theta \text{]}
\end{align*}\]</span></p>
<p>Now we apply Bayes’ theorem. We write a <code>R</code> function that computes the product of the likelihood times the prior, or the numerator in Bayes’ theorem: <span class="math inline">\(\Pr(\text{data} \mid \theta) \times \Pr(\theta)\)</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="1.4-numerical-approx.html#cb4-1" aria-hidden="true" tabindex="-1"></a>numerator <span class="ot">&lt;-</span> <span class="cf">function</span>(theta) <span class="fu">dbinom</span>(y, n, theta) <span class="sc">*</span> <span class="fu">dunif</span>(theta, <span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>We write another function that calculates the denominator, the average likelihood: <span class="math inline">\(\Pr(\text{data}) = \int{L(\theta \mid \text{data}) \Pr(\theta) d\theta}\)</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="1.4-numerical-approx.html#cb5-1" aria-hidden="true" tabindex="-1"></a>denominator <span class="ot">&lt;-</span> <span class="fu">integrate</span>(numerator,<span class="dv">0</span>,<span class="dv">1</span>)<span class="sc">$</span>value</span></code></pre></div>
<p>We use the <code>R</code> function <code>integrate</code> to calculate the integral in the denominator, which implements quadrature techniques to divide in little squares the area underneath the curve delimited by the function to integrate (here the numerator), and count them.</p>
<p>Then we get a numerical approximation of the posterior in Figure <a href="1.4-numerical-approx.html#fig:numapprox">1.4</a> by applying Bayes’ theorem.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="1.4-numerical-approx.html#cb6-1" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>) <span class="co"># grid of values for theta</span></span>
<span id="cb6-2"><a href="1.4-numerical-approx.html#cb6-2" aria-hidden="true" tabindex="-1"></a>numerical_posterior <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">survival =</span> grid, </span>
<span id="cb6-3"><a href="1.4-numerical-approx.html#cb6-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">posterior =</span> <span class="fu">numerator</span>(grid)<span class="sc">/</span>denominator) <span class="co"># Bayes&#39; theorem</span></span>
<span id="cb6-4"><a href="1.4-numerical-approx.html#cb6-4" aria-hidden="true" tabindex="-1"></a>numerical_posterior <span class="sc">%&gt;%</span></span>
<span id="cb6-5"><a href="1.4-numerical-approx.html#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb6-6"><a href="1.4-numerical-approx.html#cb6-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> survival, <span class="at">y =</span> posterior) <span class="sc">+</span> </span>
<span id="cb6-7"><a href="1.4-numerical-approx.html#cb6-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:numapprox"></span>
<img src="banana-book_files/figure-html/numapprox-1.png" alt="Winter survival posterior distribution obtained by numerical integration." width="672" />
<p class="caption">
Figure 1.4: Winter survival posterior distribution obtained by numerical integration.
</p>
</div>
<p>How good is our numerical approximation of survival posterior distribution? Ideally, we would want to compare the approximation to the true posterior distribution. Although a closed-form expression for the posterior distribution is in general intractable, when you combine a binomial likelihood together with a beta distribution as a prior, then the posterior distribution is also a beta distribution, which makes it amenable to all sorts of exact calculations<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. The beta distribution is continuous between 0 and 1, and extends the uniform distribution to situations where not all outcomes are equally likely. It has two parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that control its shape (Figure <a href="1.4-numerical-approx.html#fig:betadistribution">1.5</a>).</p>

<div class="figure"><span style="display:block;" id="fig:betadistribution"></span>
<img src="banana-book_files/figure-html/betadistribution-1.png" alt="The distribution beta(\(a\),\(b\)) for different values of \(a\) and \(b\). Note that for \(a = b = 1\), we get the uniform distribution between 0 and 1 in the top left panel. When \(a\) and \(b\) are equal, the distribution is symmetric, and the bigger \(a\) and \(b\), the more peaked the distribution or the smaller the variance." width="672" />
<p class="caption">
Figure 1.5: The distribution beta(<span class="math inline">\(a\)</span>,<span class="math inline">\(b\)</span>) for different values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. Note that for <span class="math inline">\(a = b = 1\)</span>, we get the uniform distribution between 0 and 1 in the top left panel. When <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are equal, the distribution is symmetric, and the bigger <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the more peaked the distribution or the smaller the variance.
</p>
</div>
If the likelihood of the data <span class="math inline">\(y\)</span> is binomial with <span class="math inline">\(n\)</span> trials and probability of success <span class="math inline">\(\theta\)</span>, and the prior is a beta distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, then the posterior is a beta distribution with parameters <span class="math inline">\(a + y\)</span> and <span class="math inline">\(b + n - y\)</span><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. In our example, we have <span class="math inline">\(n = 57\)</span> trials and <span class="math inline">\(y = 19\)</span> animals that survived and a uniform prior between 0 and 1 or a beta distribution with parameters <span class="math inline">\(a = b = 1\)</span>, therefore survival has a beta posterior distribution with parameters 20 and 39. In Figure <a href="1.4-numerical-approx.html#fig:compar">1.6</a>, we superimpose the exact posterior and the numerical approximation. Clearly, the two distributions are indistinguishable, suggesting that the numerical approximation is more than fine.
<div class="figure"><span style="display:block;" id="fig:compar"></span>
<img src="banana-book_files/figure-html/compar-1.png" alt="Comparison of exact (dashed line) vs. numerical approximation (continuous line) of winter survival posterior distribution." width="672" />
<p class="caption">
Figure 1.6: Comparison of exact (dashed line) vs. numerical approximation (continuous line) of winter survival posterior distribution.
</p>
</div>
<!-- To finish up, let's add the prior.  -->
<!-- ```{r, echo = FALSE} -->
<!-- ggplot() +  -->
<!--   geom_line(data = numerical_posterior,  -->
<!--             aes(x = survival, y = posterior),  -->
<!--             size = 1.5,  -->
<!--             col = wesanderson::wes_palettes$Royal1[2],  -->
<!--             alpha = 0.5) +  -->
<!--   geom_line(data = dfexpposterior,  -->
<!--             aes(x = survival, y = explicit_posterior), -->
<!--             col = wesanderson::wes_palettes$Royal1[3],  -->
<!--             size = 1.5,  -->
<!--             linetype = "dashed") +  -->
<!--   geom_line(data = dfprior, -->
<!--             aes(x = survival, y = prior), -->
<!--             col = wesanderson::wes_palettes$Royal1[1], -->
<!--             size = 1.5) -->
<!-- ``` -->
<p>In our example, we have a single parameter to estimate, winter survival. This means dealing with a one-dimensional integral in the denominator which is pretty easy with quadrature techniques and the <code>R</code> function <code>integrate()</code>. Now what if we had multiple parameters? For example, imagine you’d like to fit a capture-recapture model with detection probability <span class="math inline">\(p\)</span> and regression parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> for the intercept and slope of a relationship between survival probability and a covariate, then Bayes’ theorem gives you the posterior distribution of all three parameters together:</p>
<p><span class="math display">\[ \Pr(\alpha, \beta, p \mid \text{data}) = \frac{ \Pr(\text{data} \mid \alpha, \beta, p) \times \Pr(\alpha, \beta, p)}{\iiint \, \Pr(\text{data} \mid \alpha, \beta, p) \Pr(\alpha, \beta, p) d\alpha d\beta dp} \]</span>
There are two computational challenges with this formula. First, do we really wish to calculate a three-dimensional integral? The answer is no, one-dimensional and two-dimensional integrals are so much further we can go with standard methods. Second, we’re more interested in a posterior distribution for each parameter separately than the joint posterior distribution. The so-called marginal distribution of <span class="math inline">\(p\)</span> for example is obtained by integrating over all the other parameters – a two-dimensional integral in this example. Now imagine with tens or hundreds of parameters to estimate, these integrals become highly multi-dimensional and simply intractable. In the next section, I introduce powerful simulation methods to circumvent this issue.</p>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-king_bayesian_2009" class="csl-entry">
King, Ruth, B. J. T. Morgan, O. Gimenez, and S. P. Brooks. 2009. <em>Bayesian <span>Analysis</span> for <span>Population</span> <span>Ecology</span></em>. Chapman; Hall/CRC.
</div>
<div id="ref-mcelreathbook" class="csl-entry">
McElreath, Richard. 2016. <em>Statistical <span>Rethinking</span>: <span>A</span> <span>Bayesian</span> <span>Course</span> with <span>Examples</span> in <span>R</span> and <span>Stan</span></em>. 1st edition. Chapman; Hall/CRC.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>We used a similar example in <span class="citation">King et al. (<a href="#ref-king_bayesian_2009" role="doc-biblioref">2009</a>)</span><a href="1.4-numerical-approx.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>I follow <span class="citation">McElreath (<a href="#ref-mcelreathbook" role="doc-biblioref">2016</a>)</span> and use labels on the right to help remember what each line is about.<a href="1.4-numerical-approx.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>We say that the beta distribution is the conjugate prior distribution for the binomial distribution.<a href="1.4-numerical-approx.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p><strong>provide a sketch of the proof</strong><a href="1.4-numerical-approx.html#fnref9" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="1.3-what-is-the-bayesian-approach.html"><button class="btn btn-default">Previous</button></a>
<a href="1.5-markov-chain-monte-carlo-mcmc.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
