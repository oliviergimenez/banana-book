<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.6 Assessing convergence | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models" />
<meta property="og:type" content="book" />

<meta property="og:description" content="This is a textbook on the analysis of capture-recapture data with hidden Markov models (HMM) implemented in the Bayesian framework with R." />
<meta name="github-repo" content="oliviergimenez/banana-book" />

<meta name="author" content="Olivier Gimenez" />

<meta name="date" content="2022-12-23" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This is a textbook on the analysis of capture-recapture data with hidden Markov models (HMM) implemented in the Bayesian framework with R.">

<title>1.6 Assessing convergence | Bayesian Analysis of Capture-Recapture Data with Hidden Markov Models</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
<link rel="stylesheet" href="css/toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#welcome" id="toc-welcome">Welcome</a>
<ul>
<li><a href="license.html#license" id="toc-license">License</a></li>
</ul></li>
<li class="has-sub"><a href="preface.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="why-this-book.html#why-this-book" id="toc-why-this-book">Why this book?</a></li>
<li><a href="who-should-read-this-book.html#who-should-read-this-book" id="toc-who-should-read-this-book">Who should read this book?</a></li>
<li><a href="what-will-you-learn.html#what-will-you-learn" id="toc-what-will-you-learn">What will you learn?</a></li>
<li><a href="what-wont-you-learn.html#what-wont-you-learn" id="toc-what-wont-you-learn">What won’t you learn?</a></li>
<li><a href="prerequisites.html#prerequisites" id="toc-prerequisites">Prerequisites</a></li>
<li><a href="acknowledgements.html#acknowledgements" id="toc-acknowledgements">Acknowledgements</a></li>
<li><a href="how-this-book-was-written.html#how-this-book-was-written" id="toc-how-this-book-was-written">How this book was written</a></li>
</ul></li>
<li><a href="about-the-author.html#about-the-author" id="toc-about-the-author">About the author</a></li>
<li class="part"><span><b>I I. Fundations</b></span></li>
<li><a href="introduction.html#introduction" id="toc-introduction">Introduction</a></li>
<li class="has-sub"><a href="1-crashcourse.html#crashcourse" id="toc-crashcourse"><span class="toc-section-number">1</span> Bayesian statistics &amp; MCMC</a>
<ul>
<li><a href="1.1-introduction-1.html#introduction-1" id="toc-introduction-1"><span class="toc-section-number">1.1</span> Introduction</a></li>
<li><a href="1.2-bayes-theorem.html#bayes-theorem" id="toc-bayes-theorem"><span class="toc-section-number">1.2</span> Bayes’ theorem</a></li>
<li><a href="1.3-what-is-the-bayesian-approach.html#what-is-the-bayesian-approach" id="toc-what-is-the-bayesian-approach"><span class="toc-section-number">1.3</span> What is the Bayesian approach?</a></li>
<li><a href="1.4-numerical-approx.html#numerical-approx" id="toc-numerical-approx"><span class="toc-section-number">1.4</span> Approximating posteriors via numerical integration</a></li>
<li class="has-sub"><a href="1.5-markov-chain-monte-carlo-mcmc.html#markov-chain-monte-carlo-mcmc" id="toc-markov-chain-monte-carlo-mcmc"><span class="toc-section-number">1.5</span> Markov chain Monte Carlo (MCMC)</a>
<ul>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#monte-carlo-integration" id="toc-monte-carlo-integration"><span class="toc-section-number">1.5.1</span> Monte Carlo integration</a></li>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#markovmodelmcmc" id="toc-markovmodelmcmc"><span class="toc-section-number">1.5.2</span> Markov chains</a></li>
<li><a href="1.5-markov-chain-monte-carlo-mcmc.html#metropolis-algorithm" id="toc-metropolis-algorithm"><span class="toc-section-number">1.5.3</span> Metropolis algorithm</a></li>
</ul></li>
<li class="has-sub"><a href="1.6-convergence-diag.html#convergence-diag" id="toc-convergence-diag"><span class="toc-section-number">1.6</span> Assessing convergence</a>
<ul>
<li><a href="1.6-convergence-diag.html#burn-in" id="toc-burn-in"><span class="toc-section-number">1.6.1</span> Burn-in</a></li>
<li><a href="1.6-convergence-diag.html#chain-length" id="toc-chain-length"><span class="toc-section-number">1.6.2</span> Chain length</a></li>
<li><a href="1.6-convergence-diag.html#what-if-you-have-issues-of-convergence" id="toc-what-if-you-have-issues-of-convergence"><span class="toc-section-number">1.6.3</span> What if you have issues of convergence?</a></li>
</ul></li>
<li><a href="1.7-summary.html#summary" id="toc-summary"><span class="toc-section-number">1.7</span> Summary</a></li>
<li><a href="1.8-suggested-reading.html#suggested-reading" id="toc-suggested-reading"><span class="toc-section-number">1.8</span> Suggested reading</a></li>
</ul></li>
<li><a href="faq.html#faq" id="toc-faq">FAQ</a></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="convergence-diag" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Assessing convergence</h2>

<div class="rmdnote">
When implementing MCMC, we need to determine how long it takes for our Markov chain to converge to the target distribution, and the number of iterations we need after achieving convergence to get reasonable Monte Carlo estimates of numerical summaries (posterior means and credible intervals).
</div>
<div id="burn-in" class="section level3" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Burn-in</h3>
<p>In practice, we discard observations from the start of the Markov chain and just use observations from the chain once it has converged. The initial observations that we discard are usually referred to as the <em>burn-in</em>.</p>
<p>The simplest method to determine the length of the burn-in period is to look at trace plots. Going back to our example, we see from the trace plot in Figure <a href="1.6-convergence-diag.html#fig:burnin">1.12</a> that we need at least 100 iterations to achieve convergence toward an average survival around 0.3. It is always better to be conservative when specifying the length of the burn-in period, and in this example, we would use 250 or even 500 iterations as a burn-in. The length of the burn-in period can be determined by performing preliminary MCMC short runs.</p>
<div class="figure"><span style="display:block;" id="fig:burnin"></span>
<img src="banana-book_files/figure-html/burnin-1.png" alt="Determining the length of the burn-in period. The chain starts at value 0.99 and rapidly stabilises, with values bouncing back and forth around 0.3 from the 100th iteration onwards. You may choose the shaded area as the burn-in, and discard the corresponding values." width="672" />
<p class="caption">
Figure 1.12: Determining the length of the burn-in period. The chain starts at value 0.99 and rapidly stabilises, with values bouncing back and forth around 0.3 from the 100th iteration onwards. You may choose the shaded area as the burn-in, and discard the corresponding values.
</p>
</div>
<p>Inspecting the trace plot for a single run of the Markov chain is useful. However, we usually run the Markov chain several times, starting from different over-dispersed points, to check that all runs achieve the same stationary distribution. This approach is formalised by using the Brooks-Gelman-Rubin (BGR) statistic <span class="math inline">\(\hat{R}\)</span> which measures the ratio of the total variability combining multiple chains (between-chain plus within-chain) to the within-chain variability. The BGR statistic asks whether there is a chain effect, and is very much alike the <span class="math inline">\(F\)</span> test in an analysis of variance. Values below 1.1 indicate likely convergence.</p>
<p>Back to our example, we run two Markov chains with starting values 0.2 and 0.8 using 100 up to 5000 iterations, and calculate the BGR statistic using half the number of iterations as the length of the burn-in. From Figure <a href="1.6-convergence-diag.html#fig:bgr">1.13</a>, we get a value of the BGR statistic near 1 by up to 2000 iterations, which suggests that with 2000 iterations as a burn-in, there is no evidence of a lack of convergence.</p>
<div class="figure"><span style="display:block;" id="fig:bgr"></span>
<img src="banana-book_files/figure-html/bgr-1.png" alt="Brooks-Gelman-Rubin statistic as a function of the number of iterations." width="672" />
<p class="caption">
Figure 1.13: Brooks-Gelman-Rubin statistic as a function of the number of iterations.
</p>
</div>
<p>It is important to bear in mind that a value near 1 for the BGR statistic is only a necessary <em>but not sufficient</em> condition for convergence. In other words, this diagnostic cannot tell you for sure that the Markov chain has achieved convergence, only that it has not.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
</div>
<div id="chain-length" class="section level3" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Chain length</h3>
<p>How long of a chain is needed to produce reliable parameter estimates? To answer this question, you need to keep in mind that successive steps in a Markov chain are not independent – this is usually referred to as <em>autocorrelation</em>. Ideally, we would like to keep autocorrelation as low as possible. Here again, trace plots are useful to diagnose issues with autocorrelation. Let’s get back to our survival example. Figure <a href="1.6-convergence-diag.html#fig:tracechainlength">1.14</a> shows trace plots for different values of the standard deviation (parameter <em>away</em>) of the (normal) proposal distribution we use to propose a candidate value (Section <a href="1.5-markov-chain-monte-carlo-mcmc.html#metropolis-algorithm">1.5.3</a>). Small and big moves provide high correlations between successive observations of the Markov chain, whereas a standard deviation of 1 allows efficient exploration of the parameter space. The movement around the parameter space is referred to as <em>mixing</em>. Mixing is bad when the chain makes small and big moves, and good otherwise.</p>
<div class="figure"><span style="display:block;" id="fig:tracechainlength"></span>
<img src="banana-book_files/figure-html/tracechainlength-1.png" alt="Trace plots for different values of the standard deviation (SD) of the proposal distribution. Left: The chain exhibits small moves and mixing is bad. Right: The chain exhibits big moves and mixing is bad. Middle: The chain exhibits adequate moves and mixing is good. Only the thousand last iterations are shown." width="672" />
<p class="caption">
Figure 1.14: Trace plots for different values of the standard deviation (SD) of the proposal distribution. Left: The chain exhibits small moves and mixing is bad. Right: The chain exhibits big moves and mixing is bad. Middle: The chain exhibits adequate moves and mixing is good. Only the thousand last iterations are shown.
</p>
</div>
<p>In addition to trace plots, autocorrelation function (ACF) plots are a convenient way of displaying the strength of autocorrelation in a given sample values. ACF plots provide the autocorrelation between successively sampled values separated by an increasing number of iterations, or <em>lag</em> (Figure <a href="1.6-convergence-diag.html#fig:acfchainlength">1.15</a>).</p>
<div class="figure"><span style="display:block;" id="fig:acfchainlength"></span>
<img src="banana-book_files/figure-html/acfchainlength-1.png" alt="Autocorrelation function plots for different values of the standard deviation (SD) of the proposal distribution. Left and right: Autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Middle: Autocorrelation is weak, decreases rapidly with increasing lag and mixing is good." width="672" />
<p class="caption">
Figure 1.15: Autocorrelation function plots for different values of the standard deviation (SD) of the proposal distribution. Left and right: Autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. Middle: Autocorrelation is weak, decreases rapidly with increasing lag and mixing is good.
</p>
</div>
<p>Autocorrelation is not necessarily a big issue. Strongly correlated observations just require large sample sizes and therefore longer simulations. But how many iterations exactly? The effective sample size (<code>n.eff</code>) measures chain length while taking into account chain autocorrelation. You should check the <code>n.eff</code> of every parameter of interest, and of any interesting parameter combinations. In general, we need <span class="math inline">\(\text{n.eff} \geq 1000\)</span> independent steps to get reasonable Monte Carlo estimates of model parameters. In the animal survival example, <code>n.eff</code> can be calculated with the R <code>coda::effectiveSize()</code> function.</p>
<table>
<thead>
<tr class="header">
<th align="right">Proposal SD</th>
<th align="right">n.eff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.1</td>
<td align="right">224</td>
</tr>
<tr class="even">
<td align="right">1.0</td>
<td align="right">1934</td>
</tr>
<tr class="odd">
<td align="right">10.0</td>
<td align="right">230</td>
</tr>
</tbody>
</table>
<p>As expected, <code>n.eff</code> is less than the number of MCMC iterations because of autocorrelation. Only when the standard deviation of the proposal distribution is 1 and mixing is good (Figures <a href="1.6-convergence-diag.html#fig:tracechainlength">1.14</a> and <a href="1.6-convergence-diag.html#fig:acfchainlength">1.15</a>) we get a satisfying effective sample size.</p>
</div>
<div id="what-if-you-have-issues-of-convergence" class="section level3" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> What if you have issues of convergence?</h3>
<p>When diagnosing MCMC convergence, you will (very) often run into troubles. In this section you will find some helpful tips I hope.</p>
<p>When mixing is bad and effective sample size is small, you may just need to increase burn-in and/or sample more. Using more informative priors might also make Markov chains converge faster by helping your MCMC sampler (e.g. the Metropolis algorithm) navigating more efficiently the parameter space. In the same spirit, picking better initial values for starting the chain does not harm. For doing that, a strategy consists in using estimates from a simpler model for which your MCMC chains do converge.</p>
<p>If convergence issues persist, often there is a problem with your model<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a>. A bug in the code? A typo somewhere? A mistake in your maths? As often when coding is involved, the issue can be identified by removing complexities, and start with a simpler model until you find what the problem is.</p>
<p>A general advice is to see your model as a data generating tool in the first place, simulate data from it using some realistic values for the parameters, and try to recover these parameter values by fitting the model to the simulated data. Simulating from a model will help you understanding how it works, what it does not do, and the data you need to get reasonable parameter estimates.</p>
<p>We will see other strategies to improve convergence in the next chapters.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="14">
<li id="fn14"><p>Cross-reference sections on local minima and parameter redundancy for pathological cases.<a href="1.6-convergence-diag.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>The quote ‘When you have computational problems, often there’s a problem with your model’ is the folk theorem of statistical computing stated by Andrew Gelman in 2008, see <a href="https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/" class="uri">https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/</a><a href="1.6-convergence-diag.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>Cross reference relevant chapters. Option 1. Change your sampler. Option 2. Reparameterize (standardize covariates, plus non-centering: <span class="math inline">\(\alpha \sim N(0,\sigma)\)</span> becomes <span class="math inline">\(\alpha = z \sigma\)</span> with <span class="math inline">\(z \sim N(0,1)\)</span>).<a href="1.6-convergence-diag.html#fnref16" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="1.5-markov-chain-monte-carlo-mcmc.html"><button class="btn btn-default">Previous</button></a>
<a href="1.7-summary.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
