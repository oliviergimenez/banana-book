<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.10 Decoding after marginalization | banana-book.knit" />
<meta property="og:type" content="book" />






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="1.10 Decoding after marginalization | banana-book.knit">

<title>1.10 Decoding after marginalization | banana-book.knit</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if bootstrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="1-hmmcapturerecapture.html#hmmcapturerecapture" id="toc-hmmcapturerecapture"><span class="toc-section-number">1</span> Hidden Markov models</a>
<ul>
<li><a href="1.1-introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1.1</span> Introduction</a></li>
<li><a href="1.2-longitudinal-data.html#longitudinal-data" id="toc-longitudinal-data"><span class="toc-section-number">1.2</span> Longitudinal data</a></li>
<li class="has-sub"><a href="1.3-a-markov-model-for-longitudinal-data.html#a-markov-model-for-longitudinal-data" id="toc-a-markov-model-for-longitudinal-data"><span class="toc-section-number">1.3</span> A Markov model for longitudinal data</a>
<ul>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#assumptions" id="toc-assumptions"><span class="toc-section-number">1.3.1</span> Assumptions</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#transition-matrix" id="toc-transition-matrix"><span class="toc-section-number">1.3.2</span> Transition matrix</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#initial-states" id="toc-initial-states"><span class="toc-section-number">1.3.3</span> Initial states</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#likelihood" id="toc-likelihood"><span class="toc-section-number">1.3.4</span> Likelihood</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#example" id="toc-example"><span class="toc-section-number">1.3.5</span> Example</a></li>
</ul></li>
<li><a href="1.4-bayesian-formulation.html#bayesian-formulation" id="toc-bayesian-formulation"><span class="toc-section-number">1.4</span> Bayesian formulation</a></li>
<li><a href="1.5-nimble-implementation.html#nimble-implementation" id="toc-nimble-implementation"><span class="toc-section-number">1.5</span> NIMBLE implementation</a></li>
<li class="has-sub"><a href="1.6-hidden-markov-models.html#hidden-markov-models" id="toc-hidden-markov-models"><span class="toc-section-number">1.6</span> Hidden Markov models</a>
<ul>
<li><a href="1.6-hidden-markov-models.html#capturerecapturedata" id="toc-capturerecapturedata"><span class="toc-section-number">1.6.1</span> Capture-recapture data</a></li>
<li><a href="1.6-hidden-markov-models.html#observation-matrix" id="toc-observation-matrix"><span class="toc-section-number">1.6.2</span> Observation matrix</a></li>
<li><a href="1.6-hidden-markov-models.html#hidden-markov-model" id="toc-hidden-markov-model"><span class="toc-section-number">1.6.3</span> Hidden Markov model</a></li>
<li><a href="1.6-hidden-markov-models.html#likelihoodhmm" id="toc-likelihoodhmm"><span class="toc-section-number">1.6.4</span> Likelihood</a></li>
</ul></li>
<li><a href="1.7-fittinghmmnimble.html#fittinghmmnimble" id="toc-fittinghmmnimble"><span class="toc-section-number">1.7</span> Fitting HMM with NIMBLE</a></li>
<li class="has-sub"><a href="1.8-marginalization.html#marginalization" id="toc-marginalization"><span class="toc-section-number">1.8</span> Marginalization</a>
<ul>
<li><a href="1.8-marginalization.html#brute-force-approach" id="toc-brute-force-approach"><span class="toc-section-number">1.8.1</span> Brute-force approach</a></li>
<li><a href="1.8-marginalization.html#forward-algorithm" id="toc-forward-algorithm"><span class="toc-section-number">1.8.2</span> Forward algorithm</a></li>
<li><a href="1.8-marginalization.html#nimblemarginalization" id="toc-nimblemarginalization"><span class="toc-section-number">1.8.3</span> NIMBLE implementation</a></li>
</ul></li>
<li><a href="1.9-pooled-likelihood.html#pooled-likelihood" id="toc-pooled-likelihood"><span class="toc-section-number">1.9</span> Pooled encounter histories</a></li>
<li class="has-sub"><a href="1.10-decoding.html#decoding" id="toc-decoding"><span class="toc-section-number">1.10</span> Decoding after marginalization</a>
<ul>
<li><a href="1.10-decoding.html#viterbi-theory" id="toc-viterbi-theory"><span class="toc-section-number">1.10.1</span> Theory</a></li>
<li><a href="1.10-decoding.html#implementation" id="toc-implementation"><span class="toc-section-number">1.10.2</span> Implementation</a></li>
<li><a href="1.10-decoding.html#compute-average" id="toc-compute-average"><span class="toc-section-number">1.10.3</span> Compute first, average after</a></li>
<li><a href="1.10-decoding.html#average-first-compute-after" id="toc-average-first-compute-after"><span class="toc-section-number">1.10.4</span> Average first, compute after</a></li>
</ul></li>
<li><a href="1.11-summary.html#summary" id="toc-summary"><span class="toc-section-number">1.11</span> Summary</a></li>
<li><a href="1.12-suggested-reading.html#suggested-reading" id="toc-suggested-reading"><span class="toc-section-number">1.12</span> Suggested reading</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="decoding" class="section level2" number="1.10">
<h2><span class="header-section-number">1.10</span> Decoding after marginalization</h2>
<p>If you need to infer the latent states, and you cannot afford the computation times of the complete likelihood of Section <a href="1.6-hidden-markov-models.html#likelihoodhmm">1.6.4</a>, you can still use the marginal likelihood with the forward algorithm of Section <a href="1.8-marginalization.html#forward-algorithm">1.8.2</a>. You will need an extra step to decode the latent states with the Viterbi algorithm. The Viterbi algorithm allows you to compute the sequence of states that is most likely to have generated the sequence of observations.</p>
<div id="viterbi-theory" class="section level3" number="1.10.1">
<h3><span class="header-section-number">1.10.1</span> Theory</h3>
<p>In our simulated dataset, animal #15 has the encounter history (2, 1, 1, 1, 1) which was generated from the sequence of states (1, 1, 2, 2, 2) with survival probability <span class="math inline">\(\phi = 0.8\)</span> and detection probability <span class="math inline">\(p = 0.6\)</span>.</p>
<p>Imagine you do not know the truth. What is the chance that animal #15 was alive throughout the study when observing the encounter history detected in first winter, then missed in each subsequent winter? The chance of being alive in the first winter and detected when alive is 1. The chance of being alive in the second winter and non-detected is <span class="math inline">\(0.8 \times (1-0.6) = 0.32\)</span>. The same goes for third, fourth and fifth winters. In total, the probability of being alive throughout the study for an animal with encounter history (2, 1, 1, 1, 1) is <span class="math inline">\(1 \times 0.32 \times 0.32 \times 0.32 \times 0.32 = 0.01048576\)</span>.</p>
<p>Now what is the chance that animal #15 was alive, then dead for the rest of the study, when observing the encounter history (2, 1, 1, 1, 1)? And the chance of being alive in first and second winters, then dead after when observing the same encounter history? And so on. You need to enumerate all possible sequences of states and compute the probability for each of them, and choose the most probable sequence, that is with maximum probability. In our example, we would need to compute <span class="math inline">\(2^5 = 32\)</span> of these probabilities, and <span class="math inline">\(N^T\)</span> in general. Needless to say, these calculations quickly become cumbersome, if not impossible, as the number of states and/or the number of sampling occasions increases.</p>
<p>This is where the Viterbi algorithm comes in. The idea is to decompose this overall complex problem in a sequence of smallers problems that are easier to solve. If dynamic programming rings a bell, the Viterbi algorithm should look familiar to you. The Viterbi algorithm is based on the fact that the optimal path to each winter and each state can be deduced from the optimal path to the previous winter and each state.</p>
<p>For first winter, the probability of being alive and detected is 1, while the probability of being dead and detected is 0. Now what is the probability of being alive in the second winter and non-detected? If the animal was alive in the first winter, it remains alive and is missed with probability <span class="math inline">\(1 \times \phi (1-p) = 0.32\)</span>. If it was dead in the first winter, then this probability is 0. The maximum probability is 0.32 obviously so the most probable scenario to being alive in the second winter is being alive in the first winter. What about being dead in the second winter? If the animal was alive in first winter, then the probability is <span class="math inline">\(1 \times (1-\phi) \times 1 = 0.2\)</span>. If dead, then this probability is <span class="math inline">\(0 \times 0 \times (1-p) = 0\)</span>. The maximum probability is 0.2 obviously so the most probable scenario to being dead in the second winter is being alive in the first winter. Doing these calculations for third, fourth and fifth winters, we get the probabilities:</p>
<table>
<colgroup>
<col width="9%" />
<col width="7%" />
<col width="16%" />
<col width="19%" />
<col width="22%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">winter 1</th>
<th align="center">winter2</th>
<th align="center">winter 3</th>
<th align="center">winter 4</th>
<th align="center">winter 5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>state alive</strong></td>
<td align="center">1</td>
<td align="center">0.32 = max(0, 0.32)</td>
<td align="center">0.2304 = max(0, 0.2304)</td>
<td align="center">0.110592 = max(0, 0.110592)</td>
<td align="center">0.053084416 = max(0, 0.05308416</td>
</tr>
<tr class="even">
<td align="center"><strong>state dead</strong></td>
<td align="center">0</td>
<td align="center">1 = max(1, 0.2)</td>
<td align="center">1 = max(0.096, 1)</td>
<td align="center">1 = max(0.04608, 1)</td>
<td align="center">1 = max(0.0221184, 1)</td>
</tr>
<tr class="odd">
<td align="center"><strong>observation</strong></td>
<td align="center">detected</td>
<td align="center">missed</td>
<td align="center">missed</td>
<td align="center">missed</td>
<td align="center">missed</td>
</tr>
</tbody>
</table>
<p>Finally, to get (or decode) the optimal path, you work backwards and trace back the previous value that yielded the maximum probability. The most probable state in the last winter is dead (1 &gt; 0.05308416), dead again in the fourth winter (1 &gt; 0.110592), dead in the third winter (1 &gt; 0.2304), alive in the second winter (1 &gt; 0.48) and alive in the firt winter (1 &gt; 0). According to the Viterbi algorithm, the sequence of states that most likelily generated the sequence of observations (2, 1, 1, 1, 1) is alive then dead, dead, dead and dead or (1 2 2 2 2). This differs slightly from the actual sequence of states (1, 1, 2, 2, 2) in that the state in second winter is decoded dead while animal #15 only dies in third winter.</p>
<p>In contrast to the brute force approach, calculations are not duplicated but stored and used again like in the forward algorithm. Briefly speaking, the Viterbi algorithm works like the forward algorithm where sums are replaced by calculating maximums.</p>
<p>In practice, the Viterbi algorithm works as illustrated in Figure <a href="1.10-decoding.html#fig:treillis-viterbi">1.1</a>. First you initialize the procedure by calculating at <span class="math inline">\(t=1\)</span> for all states <span class="math inline">\(j=1,\ldots,N\)</span> the values <span class="math inline">\(\nu_1(j) = Pr(z_1 = j) \omega_{j,y_1}\)</span>. Then you compute for all states <span class="math inline">\(j=1,\ldots,N\)</span> the values <span class="math inline">\(\nu_t(j) = \displaystyle{\max_{i=1,\ldots,N} \nu_{t-1}(i) \gamma_{i,j} \omega_{j,y_t}}\)</span> for <span class="math inline">\(t = 2, \ldots, T\)</span>. Here at each time <span class="math inline">\(t\)</span> we determine the probability of the best path ending at each of the states <span class="math inline">\(j=1,\ldots,N\)</span>. Finally, you compute the probability of the best global path <span class="math inline">\(\displaystyle{\max_{j=1,\ldots,N}\nu_T(j)}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:treillis-viterbi"></span>
<img src="images/treillis-viterbi.png" alt="Graphical representation of the Viterbi algorithm with $\phi = 0.8$ and $p = 0.6$. States are alive $z = 1$ or dead $z = 2$ and observations are non-detected $y = 1$ or detected $y = 2$. To be done properly w/ tikz." width="100%" />
<p class="caption">
Figure 1.1: Graphical representation of the Viterbi algorithm with <span class="math inline">\(\phi = 0.8\)</span> and <span class="math inline">\(p = 0.6\)</span>. States are alive <span class="math inline">\(z = 1\)</span> or dead <span class="math inline">\(z = 2\)</span> and observations are non-detected <span class="math inline">\(y = 1\)</span> or detected <span class="math inline">\(y = 2\)</span>. To be done properly w/ tikz.
</p>
</div>
</div>
<div id="implementation" class="section level3" number="1.10.2">
<h3><span class="header-section-number">1.10.2</span> Implementation</h3>
<p>Let’s write a R function to implement the Viterbi algorithm. As parameters, our function will take the transition and observation matrices, the vector of initial state probabilities and the observed sequence of detections and non-detections for which you aim to compute the sequence of states from which it was most likely generated:</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="1.10-decoding.html#cb50-1" tabindex="-1"></a><span class="co"># getViterbi() returns sequence of states that most likely generated sequence of observations</span></span>
<span id="cb50-2"><a href="1.10-decoding.html#cb50-2" tabindex="-1"></a><span class="co"># adapted from https://github.com/vbehnam/viterbi</span></span>
<span id="cb50-3"><a href="1.10-decoding.html#cb50-3" tabindex="-1"></a>getViterbi <span class="ot">&lt;-</span> <span class="cf">function</span>(Omega, Gamma, delta, y) {</span>
<span id="cb50-4"><a href="1.10-decoding.html#cb50-4" tabindex="-1"></a><span class="co"># Omega: transition matrix</span></span>
<span id="cb50-5"><a href="1.10-decoding.html#cb50-5" tabindex="-1"></a><span class="co"># Gamma: observation matrix</span></span>
<span id="cb50-6"><a href="1.10-decoding.html#cb50-6" tabindex="-1"></a><span class="co"># delta: vector of initial state probabilities</span></span>
<span id="cb50-7"><a href="1.10-decoding.html#cb50-7" tabindex="-1"></a><span class="co"># y: observed sequence of detections and non-detections</span></span>
<span id="cb50-8"><a href="1.10-decoding.html#cb50-8" tabindex="-1"></a>  </span>
<span id="cb50-9"><a href="1.10-decoding.html#cb50-9" tabindex="-1"></a><span class="co"># get number of states and sampling occasions</span></span>
<span id="cb50-10"><a href="1.10-decoding.html#cb50-10" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Gamma)</span>
<span id="cb50-11"><a href="1.10-decoding.html#cb50-11" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb50-12"><a href="1.10-decoding.html#cb50-12" tabindex="-1"></a>  </span>
<span id="cb50-13"><a href="1.10-decoding.html#cb50-13" tabindex="-1"></a><span class="co"># nu is the corresponding likelihood</span></span>
<span id="cb50-14"><a href="1.10-decoding.html#cb50-14" tabindex="-1"></a>nu <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> N, <span class="at">ncol =</span> T)</span>
<span id="cb50-15"><a href="1.10-decoding.html#cb50-15" tabindex="-1"></a><span class="co"># zz contains the most likely states up until this point</span></span>
<span id="cb50-16"><a href="1.10-decoding.html#cb50-16" tabindex="-1"></a>zz <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> N, <span class="at">ncol =</span> T)</span>
<span id="cb50-17"><a href="1.10-decoding.html#cb50-17" tabindex="-1"></a>firstObs <span class="ot">&lt;-</span> y[<span class="dv">1</span>]</span>
<span id="cb50-18"><a href="1.10-decoding.html#cb50-18" tabindex="-1"></a>  </span>
<span id="cb50-19"><a href="1.10-decoding.html#cb50-19" tabindex="-1"></a><span class="co"># fill in first columns of both matrices</span></span>
<span id="cb50-20"><a href="1.10-decoding.html#cb50-20" tabindex="-1"></a><span class="co">#nu[,1] &lt;- initial * emission[,firstObs]</span></span>
<span id="cb50-21"><a href="1.10-decoding.html#cb50-21" tabindex="-1"></a><span class="co">#zz[,1] &lt;- 0</span></span>
<span id="cb50-22"><a href="1.10-decoding.html#cb50-22" tabindex="-1"></a>nu[,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>) <span class="co"># initial = (1, 0) * emission[,firstObs] = (1, 0)</span></span>
<span id="cb50-23"><a href="1.10-decoding.html#cb50-23" tabindex="-1"></a>zz[,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># alive at first occasion</span></span>
<span id="cb50-24"><a href="1.10-decoding.html#cb50-24" tabindex="-1"></a></span>
<span id="cb50-25"><a href="1.10-decoding.html#cb50-25" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>T) {</span>
<span id="cb50-26"><a href="1.10-decoding.html#cb50-26" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb50-27"><a href="1.10-decoding.html#cb50-27" tabindex="-1"></a>      obs <span class="ot">&lt;-</span> y[i]</span>
<span id="cb50-28"><a href="1.10-decoding.html#cb50-28" tabindex="-1"></a>      <span class="co"># initialize to -1, then overwritten by for loop coz all possible values are &gt;= 0</span></span>
<span id="cb50-29"><a href="1.10-decoding.html#cb50-29" tabindex="-1"></a>      nu[j,i] <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb50-30"><a href="1.10-decoding.html#cb50-30" tabindex="-1"></a>      <span class="co"># loop to find max and argmax for k</span></span>
<span id="cb50-31"><a href="1.10-decoding.html#cb50-31" tabindex="-1"></a>      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb50-32"><a href="1.10-decoding.html#cb50-32" tabindex="-1"></a>        value <span class="ot">&lt;-</span> nu[k,i<span class="dv">-1</span>] <span class="sc">*</span> Gamma[k,j] <span class="sc">*</span> Omega[j,obs]</span>
<span id="cb50-33"><a href="1.10-decoding.html#cb50-33" tabindex="-1"></a>        <span class="cf">if</span> (value <span class="sc">&gt;</span> nu[j,i]) {</span>
<span id="cb50-34"><a href="1.10-decoding.html#cb50-34" tabindex="-1"></a>          <span class="co"># maximizing for k</span></span>
<span id="cb50-35"><a href="1.10-decoding.html#cb50-35" tabindex="-1"></a>          nu[j,i] <span class="ot">&lt;-</span> value</span>
<span id="cb50-36"><a href="1.10-decoding.html#cb50-36" tabindex="-1"></a>          <span class="co"># argmaximizing for k</span></span>
<span id="cb50-37"><a href="1.10-decoding.html#cb50-37" tabindex="-1"></a>          zz[j,i] <span class="ot">&lt;-</span> k</span>
<span id="cb50-38"><a href="1.10-decoding.html#cb50-38" tabindex="-1"></a>        }</span>
<span id="cb50-39"><a href="1.10-decoding.html#cb50-39" tabindex="-1"></a>      }</span>
<span id="cb50-40"><a href="1.10-decoding.html#cb50-40" tabindex="-1"></a>    }</span>
<span id="cb50-41"><a href="1.10-decoding.html#cb50-41" tabindex="-1"></a>  }</span>
<span id="cb50-42"><a href="1.10-decoding.html#cb50-42" tabindex="-1"></a>  <span class="co"># mlp = most likely path</span></span>
<span id="cb50-43"><a href="1.10-decoding.html#cb50-43" tabindex="-1"></a>  mlp <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb50-44"><a href="1.10-decoding.html#cb50-44" tabindex="-1"></a>  <span class="co"># argmax for stateSeq[,T]</span></span>
<span id="cb50-45"><a href="1.10-decoding.html#cb50-45" tabindex="-1"></a>  am <span class="ot">&lt;-</span> <span class="fu">which.max</span>(nu[,T])</span>
<span id="cb50-46"><a href="1.10-decoding.html#cb50-46" tabindex="-1"></a>  mlp[T] <span class="ot">&lt;-</span> zz[am,T]</span>
<span id="cb50-47"><a href="1.10-decoding.html#cb50-47" tabindex="-1"></a>  </span>
<span id="cb50-48"><a href="1.10-decoding.html#cb50-48" tabindex="-1"></a>  <span class="co"># backtrace using backpointers</span></span>
<span id="cb50-49"><a href="1.10-decoding.html#cb50-49" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> T<span class="sc">:</span><span class="dv">2</span>) {</span>
<span id="cb50-50"><a href="1.10-decoding.html#cb50-50" tabindex="-1"></a>    zm <span class="ot">&lt;-</span> <span class="fu">which.max</span>(nu[,i])</span>
<span id="cb50-51"><a href="1.10-decoding.html#cb50-51" tabindex="-1"></a>    mlp[i<span class="dv">-1</span>] <span class="ot">&lt;-</span> zz[zm,i]</span>
<span id="cb50-52"><a href="1.10-decoding.html#cb50-52" tabindex="-1"></a>  }</span>
<span id="cb50-53"><a href="1.10-decoding.html#cb50-53" tabindex="-1"></a>  <span class="fu">return</span>(mlp)</span>
<span id="cb50-54"><a href="1.10-decoding.html#cb50-54" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that instead of writing your own R function, you could use a built-in function from an existing R package to implement the Viterbi algorithm (for example, the <code>viterbi()</code> function from the <code>HMM</code> and <code>depmixS4</code> packages), and call it from NIMBLE as we have seen in Section <a href="#callrfninnimble"><strong>??</strong></a>. The difficulty is that HMM for capture-recapture data have specific features that make standard functions not adapted and requires coding your own Viterbi function. In particular, we have to deal with detection at first encounter, which is not estimated but is always one because an individual has to be captured to be marked and released for the first time. Also, our transition and observation matrices are not always homogeneous and may depend on time.</p>
<p>Let’s test our <code>getViterbi()</code> function with our previous example. Remember animal #15 has the encounter history (2, 1, 1, 1, 1) which was generated from the sequence of states (1, 1, 2, 2, 2). Applying our function to that animal encounter history, we get:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="1.10-decoding.html#cb51-1" tabindex="-1"></a>delta <span class="co"># Vector of initial states probabilities</span></span>
<span id="cb51-2"><a href="1.10-decoding.html#cb51-2" tabindex="-1"></a><span class="do">## [1] 1 0</span></span>
<span id="cb51-3"><a href="1.10-decoding.html#cb51-3" tabindex="-1"></a>Gamma <span class="co"># Transition matrix</span></span>
<span id="cb51-4"><a href="1.10-decoding.html#cb51-4" tabindex="-1"></a><span class="do">##      [,1] [,2]</span></span>
<span id="cb51-5"><a href="1.10-decoding.html#cb51-5" tabindex="-1"></a><span class="do">## [1,]  0.8  0.2</span></span>
<span id="cb51-6"><a href="1.10-decoding.html#cb51-6" tabindex="-1"></a><span class="do">## [2,]  0.0  1.0</span></span>
<span id="cb51-7"><a href="1.10-decoding.html#cb51-7" tabindex="-1"></a>Omega <span class="co"># Observation matrix</span></span>
<span id="cb51-8"><a href="1.10-decoding.html#cb51-8" tabindex="-1"></a><span class="do">##      [,1] [,2]</span></span>
<span id="cb51-9"><a href="1.10-decoding.html#cb51-9" tabindex="-1"></a><span class="do">## [1,]  0.4  0.6</span></span>
<span id="cb51-10"><a href="1.10-decoding.html#cb51-10" tabindex="-1"></a><span class="do">## [2,]  1.0  0.0</span></span>
<span id="cb51-11"><a href="1.10-decoding.html#cb51-11" tabindex="-1"></a><span class="fu">getViterbi</span>(<span class="at">Omega =</span> Omega, </span>
<span id="cb51-12"><a href="1.10-decoding.html#cb51-12" tabindex="-1"></a>           <span class="at">Gamma =</span> Gamma, </span>
<span id="cb51-13"><a href="1.10-decoding.html#cb51-13" tabindex="-1"></a>           <span class="at">delta =</span> delta, </span>
<span id="cb51-14"><a href="1.10-decoding.html#cb51-14" tabindex="-1"></a>           <span class="at">y =</span> y[<span class="dv">15</span>,] <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb51-15"><a href="1.10-decoding.html#cb51-15" tabindex="-1"></a><span class="do">## [1] 1 2 2 2 2</span></span></code></pre></div>
<p>The Viterbi algorithm does pretty well at recovering the latent states, despite incorrectly decoding a death in the second winter while individual #15 only dies in the third winter. We obtained the same results by implementing the Viterbi algorithm by hand in Section <a href="1.10-decoding.html#viterbi-theory">1.10.1</a>.</p>
<p>Now that we have a function that implements the Viterbi algorithm, we can use it with our MCMC outputs. You have two options, either you apply Viterbi to each MCMC iteration then you compute the posterior median or mode path for each individual, or you compute the posterior mean or median of the transition and observation matrices then you apply Viterbi to each individual encounter history.</p>
<p>For both options, we will need the values from the posterior distributions of survival and detection probabilities:</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="1.10-decoding.html#cb52-1" tabindex="-1"></a>phi <span class="ot">&lt;-</span> <span class="fu">c</span>(mcmc.output<span class="sc">$</span>chain1[,<span class="st">&#39;phi&#39;</span>], mcmc.output<span class="sc">$</span>chain2[,<span class="st">&#39;phi&#39;</span>])</span>
<span id="cb52-2"><a href="1.10-decoding.html#cb52-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">c</span>(mcmc.output<span class="sc">$</span>chain1[,<span class="st">&#39;p&#39;</span>], mcmc.output<span class="sc">$</span>chain2[,<span class="st">&#39;p&#39;</span>])</span></code></pre></div>
</div>
<div id="compute-average" class="section level3" number="1.10.3">
<h3><span class="header-section-number">1.10.3</span> Compute first, average after</h3>
<p>First option is to apply Viterbi to each MCMC sample, then to compute median of the MCMC Viterbi paths for each observed sequence:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="1.10-decoding.html#cb53-1" tabindex="-1"></a>niter <span class="ot">&lt;-</span> <span class="fu">length</span>(p)</span>
<span id="cb53-2"><a href="1.10-decoding.html#cb53-2" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb53-3"><a href="1.10-decoding.html#cb53-3" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(y), <span class="at">ncol =</span> T)</span>
<span id="cb53-4"><a href="1.10-decoding.html#cb53-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(y)){</span>
<span id="cb53-5"><a href="1.10-decoding.html#cb53-5" tabindex="-1"></a>  res_mcmc <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> niter, <span class="at">ncol =</span> T)</span>
<span id="cb53-6"><a href="1.10-decoding.html#cb53-6" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>niter){</span>
<span id="cb53-7"><a href="1.10-decoding.html#cb53-7" tabindex="-1"></a>    <span class="co"># Initial states</span></span>
<span id="cb53-8"><a href="1.10-decoding.html#cb53-8" tabindex="-1"></a>    delta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb53-9"><a href="1.10-decoding.html#cb53-9" tabindex="-1"></a>    <span class="co"># Transition matrix</span></span>
<span id="cb53-10"><a href="1.10-decoding.html#cb53-10" tabindex="-1"></a>    transition <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb53-11"><a href="1.10-decoding.html#cb53-11" tabindex="-1"></a>    transition[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> phi[j]      <span class="co"># Pr(alive t -&gt; alive t+1)</span></span>
<span id="cb53-12"><a href="1.10-decoding.html#cb53-12" tabindex="-1"></a>    transition[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> phi[j]  <span class="co"># Pr(alive t -&gt; dead t+1)</span></span>
<span id="cb53-13"><a href="1.10-decoding.html#cb53-13" tabindex="-1"></a>    transition[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span>        <span class="co"># Pr(dead t -&gt; alive t+1)</span></span>
<span id="cb53-14"><a href="1.10-decoding.html#cb53-14" tabindex="-1"></a>    transition[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span>        <span class="co"># Pr(dead t -&gt; dead t+1)</span></span>
<span id="cb53-15"><a href="1.10-decoding.html#cb53-15" tabindex="-1"></a>    <span class="co"># Observation matrix </span></span>
<span id="cb53-16"><a href="1.10-decoding.html#cb53-16" tabindex="-1"></a>    emission <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb53-17"><a href="1.10-decoding.html#cb53-17" tabindex="-1"></a>    emission[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> p[j]      <span class="co"># Pr(alive t -&gt; non-detected t)</span></span>
<span id="cb53-18"><a href="1.10-decoding.html#cb53-18" tabindex="-1"></a>    emission[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> p[j]          <span class="co"># Pr(alive t -&gt; detected t)</span></span>
<span id="cb53-19"><a href="1.10-decoding.html#cb53-19" tabindex="-1"></a>    emission[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span>          <span class="co"># Pr(dead t -&gt; non-detected t)</span></span>
<span id="cb53-20"><a href="1.10-decoding.html#cb53-20" tabindex="-1"></a>    emission[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">0</span>          <span class="co"># Pr(dead t -&gt; detected t)</span></span>
<span id="cb53-21"><a href="1.10-decoding.html#cb53-21" tabindex="-1"></a>    res_mcmc[j,<span class="dv">1</span><span class="sc">:</span>T] <span class="ot">&lt;-</span> <span class="fu">getViterbi</span>(emission, transition, delta, y[i,] <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb53-22"><a href="1.10-decoding.html#cb53-22" tabindex="-1"></a>  }</span>
<span id="cb53-23"><a href="1.10-decoding.html#cb53-23" tabindex="-1"></a>  res[i, <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y[<span class="dv">1</span>,])] <span class="ot">&lt;-</span> <span class="fu">apply</span>(res_mcmc, <span class="dv">2</span>, median)</span>
<span id="cb53-24"><a href="1.10-decoding.html#cb53-24" tabindex="-1"></a>}</span></code></pre></div>
You can compare the Viterbi decoding to the actual states <span class="math inline">\(z\)</span>:
<div class="figure"><span style="display:block;" id="fig:viterbiaveragecompute"></span>
<img src="banana-book_files/figure-html/viterbiaveragecompute-1.png" alt="Comparison of the actual sequences of states to the sequences of states decoded with Viterbi and the *average first, compute after* method." width="672" />
<p class="caption">
Figure 1.2: Comparison of the actual sequences of states to the sequences of states decoded with Viterbi and the <em>average first, compute after</em> method.
</p>
</div>
<p>Decoding is correct except that the alive actual state is often decoded as the dead state by the Viterbi algorithm. Note that here we compute the Viterbi paths after we run NIMBLE. You could turn the R function <code>getViterbi()</code> into a NIMBLE function and plug it in your model code to apply Viterbi. This would not make any difference except perhaps to increase MCMC computation times.</p>
</div>
<div id="average-first-compute-after" class="section level3" number="1.10.4">
<h3><span class="header-section-number">1.10.4</span> Average first, compute after</h3>
<p>Second option is to compute the posterior mean of the observation and transition matrices, then to apply Viterbi:</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="1.10-decoding.html#cb54-1" tabindex="-1"></a><span class="co"># Initial states</span></span>
<span id="cb54-2"><a href="1.10-decoding.html#cb54-2" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb54-3"><a href="1.10-decoding.html#cb54-3" tabindex="-1"></a><span class="co"># Transition matrix</span></span>
<span id="cb54-4"><a href="1.10-decoding.html#cb54-4" tabindex="-1"></a>transition <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb54-5"><a href="1.10-decoding.html#cb54-5" tabindex="-1"></a>transition[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(phi)      <span class="co"># Pr(alive t -&gt; alive t+1)</span></span>
<span id="cb54-6"><a href="1.10-decoding.html#cb54-6" tabindex="-1"></a>transition[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(phi)  <span class="co"># Pr(alive t -&gt; dead t+1)</span></span>
<span id="cb54-7"><a href="1.10-decoding.html#cb54-7" tabindex="-1"></a>transition[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">0</span>              <span class="co"># Pr(dead t -&gt; alive t+1)</span></span>
<span id="cb54-8"><a href="1.10-decoding.html#cb54-8" tabindex="-1"></a>transition[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">1</span>              <span class="co"># Pr(dead t -&gt; dead t+1)</span></span>
<span id="cb54-9"><a href="1.10-decoding.html#cb54-9" tabindex="-1"></a><span class="co"># Observation matrix </span></span>
<span id="cb54-10"><a href="1.10-decoding.html#cb54-10" tabindex="-1"></a>emission <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb54-11"><a href="1.10-decoding.html#cb54-11" tabindex="-1"></a>emission[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(p)      <span class="co"># Pr(alive t -&gt; non-detected t)</span></span>
<span id="cb54-12"><a href="1.10-decoding.html#cb54-12" tabindex="-1"></a>emission[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(p)          <span class="co"># Pr(alive t -&gt; detected t)</span></span>
<span id="cb54-13"><a href="1.10-decoding.html#cb54-13" tabindex="-1"></a>emission[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span>                <span class="co"># Pr(dead t -&gt; non-detected t)</span></span>
<span id="cb54-14"><a href="1.10-decoding.html#cb54-14" tabindex="-1"></a>emission[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="dv">0</span>                <span class="co"># Pr(dead t -&gt; detected t)</span></span>
<span id="cb54-15"><a href="1.10-decoding.html#cb54-15" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(y), <span class="at">ncol =</span> T)</span>
<span id="cb54-16"><a href="1.10-decoding.html#cb54-16" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(y)){</span>
<span id="cb54-17"><a href="1.10-decoding.html#cb54-17" tabindex="-1"></a>  res[i, <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(y[<span class="dv">1</span>,]) ] <span class="ot">&lt;-</span> <span class="fu">getViterbi</span>(emission, transition, delta, y[i,] <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb54-18"><a href="1.10-decoding.html#cb54-18" tabindex="-1"></a>}</span></code></pre></div>
Again, you can compare the result of the Viterbi decoding to the actual states we simulated and used to generate the observations:
<div class="figure"><span style="display:block;" id="fig:viterbicomputeaverage"></span>
<img src="banana-book_files/figure-html/viterbicomputeaverage-1.png" alt="Comparison of the actual sequences of states to the sequences of states decoded with Viterbi and the *compute first, average after* approach." width="672" />
<p class="caption">
Figure 1.3: Comparison of the actual sequences of states to the sequences of states decoded with Viterbi and the <em>compute first, average after</em> approach.
</p>
</div>
<p>The results are very similar to those we obtained in Section <a href="1.10-decoding.html#compute-average">1.10.3</a>, and Figure <a href="1.10-decoding.html#fig:viterbicomputeaverage">1.3</a> is indisguishable from Figure <a href="1.10-decoding.html#fig:viterbiaveragecompute">1.2</a>.</p>
</div>
</div>
<p style="text-align: center;">
<a href="1.9-pooled-likelihood.html"><button class="btn btn-default">Previous</button></a>
<a href="1.11-summary.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
