[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome online version book Bayesian Analysis Capture-Recapture Data Hidden Markov Models – Theory Case Studies R. HMM framework gained much attention ecological literature last decade, suggested general modelling framework demography plant animal populations. particular, HMMs increasingly used analyse capture-recapture data estimate key population parameters (e.g., survival, dispersal, recruitment abundance) applications fields ecology.parallel, Bayesian statistics well established fast growing ecology related disciplines, resonates scientific reasoning allows accommodating uncertainty smoothly. popularity Bayesian statistics also comes availability free pieces software (WinBUGS, OpenBUGS, JAGS, Stan, NIMBLE) allow practitioners code analyses.book offers Bayesian treatment HMMs applied capture-recapture data. learn use R package NIMBLE seen many future Bayesian statistical ecology deal complex models /big data. important part book consists case studies presented tutorial style abide “learning ” philosophy.’m currently writing book, welcome feedback. may raise issue , amend directly R Markdown file generated page ’re reading clicking ‘Edit page’ icon right panel, email . Many thanks!Olivier Gimenez, Montpellier, France\nLast updated: August 10, 2023","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"online version book licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.code public domain, licensed Creative Commons CC0 1.0 Universal (CC0 1.0).","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"why-this-book","chapter":"Preface","heading":"Why this book?","text":"completed. capture-recapture data models, fields application.1 Brief history capture-recapture, switch state-space/hidden Markov model (HMM) formulation. Flexibility HMM decompose complex problems smaller pieces easier understand, model analyse. satellite guidance conservation endangered species. Bayes? Also three fav research topics – capture-recapture, HMM Bayes statistics – let’s enjoy great cocktail together.","code":""},{"path":"preface.html","id":"who-should-read-this-book","chapter":"Preface","heading":"Who should read this book?","text":"book aimed beginners ’re comfortable using R write basic code (including loops), well connoisseurs capture-recapture ’d like tap power Bayesian side statistics. audiences, thinking HMM framework help confidently building models make capture-recapture data.","code":""},{"path":"preface.html","id":"what-will-you-learn","chapter":"Preface","heading":"What will you learn?","text":"book divided five parts. first part aimed getting --speed Bayesian statistics, NIMBLE, hidden Markov models. second part teach capture-recapture models open populations, reproducible R code ease learning process. third part, focus issues inferring states (dealing uncertainty assignment, modelling waiting time distribution). fourth part provides real-world case studies scientific literature can reproduce using material covered previous chapters. problems can either ) used cement deepen understanding methods models, ii) adapted purpose, iii) serve teaching projects. fifth last chapter closes book take-home messages recommendations, list frequently asked questions references cited book. Likely amended feedbacks.","code":""},{"path":"preface.html","id":"what-wont-you-learn","chapter":"Preface","heading":"What won’t you learn?","text":"hardly maths book. equations use either simple enough understood without background maths, can skipped without prejudice. cover Bayesian statistics even hidden Markov models fully, provide just need work capture-recapture data. interested knowing topics, hopefully section Suggested reading end chapter put right direction. also number important topics specific capture-recapture cover, including closed-population capture-recapture models (Williams, Nichols, Conroy 2002), spatial capture-recapture models (Royle et al. 2013). models can treated HMMs, now usual formulation just fine. spatial considerations Covariates chapter w/ splines CAR. ’m sure yet SCR models (R. Glennie’s Biometrics paper HMMs open pop SCR easy Bayes transform implement NIMBLE).","code":""},{"path":"preface.html","id":"prerequisites","chapter":"Preface","heading":"Prerequisites","text":"book uses primarily R package NIMBLE, need install least R NIMBLE. bunch R packages used. can install running:","code":"\ninstall.packages(c(\n  \"magick\", \"MCMCvis\", \"nimble\", \"pdftools\", \n  \"tidyverse\", \"wesanderson\" \n))"},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"completed.","code":""},{"path":"preface.html","id":"how-this-book-was-written","chapter":"Preface","heading":"How this book was written","text":"writing book RStudio using bookdown. book website hosted GitHub Pages, automatically updated every push Github Actions. source available GitHub.version book ’re reading built R version 4.2.3 (2023-03-15) following packages:","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"name Olivier Gimenez (https://oliviergimenez.github.io/). senior (euphemism young anymore) scientist National Centre Scientific Research (CNRS) beautiful city Montpellier, France.struggled studying maths, obtained PhD applied statistics long time ago galaxy wine cheese. awarded habilitation (https://en.wikipedia.org/wiki/Habilitation) ecology evolution stop pretending understand colleagues talking . recently embarked sociology studies hey, .Lost somewhere interface animal ecology, statistical modeling social sciences, -called expertise lies population dynamics species distribution modeling address questions ecology conservation biology impact human activities management large carnivores. nothing without students colleagues kind enough bear .may find Twitter (https://twitter.com/oaggimenez), GitHub (https://github.com/oliviergimenez), get touch email.","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"introduction-1.html","id":"introduction-1","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"survival.html","id":"survival","chapter":"1 Survival","heading":"1 Survival","text":"WORK PROGRESS.","code":""},{"path":"survival.html","id":"introduction-2","chapter":"1 Survival","heading":"1.1 Introduction","text":"fourth chapter, learn Cormack-Jolly-Seber model allows estimating survival based capture-recapture data. also see deal covariates try explain temporal /individual variation survival.","code":""},{"path":"survival.html","id":"the-cormack-jolly-seber-cjs-model","chapter":"1 Survival","heading":"1.2 The Cormack-Jolly-Seber (CJS) model","text":"previous chapter, introduced capture-recapture model constant survival detection probabilities formulated HMM fitted data NIMBLE. Historically, however, slightly complicated model first proposed – -called Cormack-Jolly-Seber (CJS) model – survival recapture probabilities time-varying. feature CJS model useful account variation due environmental conditions survival sampling effort detection. Schematically CJS model can represented way:Note states (gray) observations (white) change. still \\(z = 1\\) alive, \\(z = 2\\) dead, \\(y = 1\\) non-detected, \\(y = 2\\) detected.Parameters now indexed time. survival probability defined probability staying alive (“ah, ha, ha, ha, stayin’ alive” like Bee Gees say) interval \\(t\\) \\(t+1\\), \\(\\phi_t = \\Pr(z_{t+1} = 1 | z_t = 1)\\). detection probability defined probability observed \\(t\\) given ’re alive \\(t\\), \\(p_t = \\Pr(y_{t} = 1 | z_t = 1)\\). important bear mind survival operates interval detection occurs specific time. Fast forward covariates section?CJS model named three statisticians published independent sole-author papers introducing less approach, year apart ! fact, Richard Cormack George Jolly working corridor Scotland back 1960’s. meet every day coffee play game together, never mention work aware ’s work.","code":""},{"path":"survival.html","id":"capture-recapture-data","chapter":"1 Survival","heading":"1.3 Capture-recapture data","text":"turn fitting CJS model actual data, let’s talk capture-recapture minute. said previous chapter (section 1.6.1) individuals individual marked. can accomplished two ways, either artificial marks like rings birds ear tags mammals, (non-invasive) natural marks like coat patterns feces DNA sequencing (Figure 1.1).\nFigure 1.1: Animal marks\nThroughout chapter, use data White-throated Dipper (Cinclus cinclus; dipper hereafter) kindly provided Gilbert Marzolin (Figure 1.2). total, 294 dippers known sex wing length captured recaptured 1981 1987 March-June period. Birds least 1 year old initially banded.\nFigure 1.2: White-throated Dipper (Cinclus cinclus)\nmay scroll data :seven first columns years Gilbert went field captured birds. 0 stands non-detection, 1 detection. eighth column informs sex bird, F female M male. last column gives measure wing length first time bird captured.","code":""},{"path":"survival.html","id":"fitting-the-cjs-model-to-the-dipper-data-with-nimble","chapter":"1 Survival","heading":"1.4 Fitting the CJS model to the dipper data with NIMBLE","text":"write NIMBLE code corresponding CJS model, need make adjustments NIMBLE code model constant parameters previous chapter. main modification concerns observation transitions matrices need time-varying therefore become arrays inherit third dimension besides rows columns. Also need priors time-varying survival detection probabilities. insist first several cohort vs single cohort previous chapter get:likelihood change, except time-varying observation transition matrices need used appropriately:Overall, code looks like:read data:Get occasion first capture individuals: Say several cohorts, different prevcious chapter exampleNow specify constants:Now data list. Note add 1 data 1 non-detections 2 detections. may use coding prefer course, just need adjust \\(\\Omega\\) \\(\\Gamma\\) matrices model .Now let’s write function initial values. latent states, go easy way, say individuals alive study period.specify parameters ’d like monitor:provide MCMC details:last, ’re ready run NIMBLE:may look numerical summaries. Comment variation survival, constancy detection. write example interpret estimates. Note small effective sample size last survival recapture probabilities, get back minute.Let’s produce caterpillar plot estimates.Let’s focus minute last survival probability. See mixing bad overlap prior big. parameter redundant, can shown product \\(\\phi_6\\) \\(p_7\\) can estimated.two potential issues, either intrinsic extrinsic parameter redundacy. Intrinsic redundancy means model likelihood can expressed smaller number parameters; feature model. Extrinsic redundancymeans model structure fine, lack data makes parameter non-estimable; feature data. Recommendation overlap, et refer book Diana Cole.","code":"\n...\n# parameters\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  for (t in 1:(T-1)){\n    phi[t] ~ dunif(0, 1) # prior survival\n    gamma[1,1,t] <- phi[t]      # Pr(alive t -> alive t+1)\n    gamma[1,2,t] <- 1 - phi[t]  # Pr(alive t -> dead t+1)\n    gamma[2,1,t] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,t] <- 1        # Pr(dead t -> dead t+1)\n    p[t] ~ dunif(0, 1) # prior detection\n    omega[1,1,t] <- 1 - p[t]    # Pr(alive t -> non-detected t)\n    omega[1,2,t] <- p[t]        # Pr(alive t -> detected t)\n    omega[2,1,t] <- 1        # Pr(dead t -> non-detected t)\n    omega[2,2,t] <- 0        # Pr(dead t -> detected t)\n  }\n...\n...\n# likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, j-1])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2, j-1])\n    }\n  }\n...\nhmm.phitpt <- nimbleCode({\n  # parameters\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  for (t in 1:(T-1)){\n    phi[t] ~ dunif(0, 1) # prior survival\n    gamma[1,1,t] <- phi[t]      # Pr(alive t -> alive t+1)\n    gamma[1,2,t] <- 1 - phi[t]  # Pr(alive t -> dead t+1)\n    gamma[2,1,t] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,t] <- 1        # Pr(dead t -> dead t+1)\n    p[t] ~ dunif(0, 1) # prior detection\n    omega[1,1,t] <- 1 - p[t]    # Pr(alive t -> non-detected t)\n    omega[1,2,t] <- p[t]        # Pr(alive t -> detected t)\n    omega[2,1,t] <- 1        # Pr(dead t -> non-detected t)\n    omega[2,2,t] <- 0        # Pr(dead t -> detected t)\n  }\n  # likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, j-1])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2, j-1])\n    }\n  }\n})\ndipper <- read_csv(here::here(\"dat\", \"dipper.csv\"), show_col_types = FALSE)\ny <- dipper %>%\n  select(year_1981:year_1987) %>%\n  as.matrix()\nfirst <- apply(y, 1, function(x) min(which(x !=0)))\nmy.constants <- list(N = nrow(y), T = ncol(y), first = first)\nmy.constants\n## $N\n## [1] 294\n## \n## $T\n## [1] 7\n## \n## $first\n##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n##  [28] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n##  [55] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\n##  [82] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n## [109] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4\n## [136] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n## [163] 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n## [190] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6\n## [217] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n## [244] 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n## [271] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\nmy.data <- list(y = y + 1)\nzinits <- y + 1 # non-detection -> alive\nzinits[zinits == 2] <- 1 # dead -> alive\ninitial.values <- function() list(phi = runif(my.constants$T-1,0,1),\n                                  p = runif(my.constants$T-1,0,1),\n                                  z = zinits)\nparameters.to.save <- c(\"phi\", \"p\")\nparameters.to.save\n## [1] \"phi\" \"p\"\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 2\nmcmc.output <- nimbleMCMC(code = hmm.phitpt,\n                          constants = my.constants,\n                          data = my.data,\n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter,\n                          nburnin = n.burnin,\n                          nchains = n.chains)\nMCMCsummary(mcmc.output, round = 2)##        mean   sd 2.5%  50% 97.5% Rhat n.eff\n## phi[1] 0.73 0.14 0.46 0.72  0.99 1.02   199\n## phi[2] 0.45 0.07 0.32 0.44  0.59 1.02   410\n## phi[3] 0.48 0.06 0.35 0.48  0.59 1.01   506\n## phi[4] 0.63 0.06 0.52 0.63  0.75 1.03   415\n## phi[5] 0.60 0.06 0.49 0.60  0.72 1.01   365\n## phi[6] 0.74 0.13 0.51 0.74  0.97 1.10    38\n## p[1]   0.66 0.14 0.38 0.67  0.89 1.01   344\n## p[2]   0.87 0.08 0.68 0.89  0.98 1.02   249\n## p[3]   0.88 0.07 0.73 0.89  0.97 1.02   307\n## p[4]   0.87 0.06 0.74 0.88  0.96 1.05   333\n## p[5]   0.90 0.05 0.77 0.91  0.98 1.01   224\n## p[6]   0.72 0.13 0.50 0.72  0.97 1.08    37\nMCMCplot(object = mcmc.phitpt, params = c(\"phi\",\"p\"))\npriors <- runif(3000, 0, 1)\nMCMCtrace(object = mcmc.phitpt,\n          ISB = FALSE,\n          exact = TRUE, \n          params = c(\"phi[6]\"),\n          pdf = FALSE, \n          priors = priors)"},{"path":"survival.html","id":"cjs-model-derivatives","chapter":"1 Survival","heading":"1.5 CJS model derivatives","text":"Besides model fitted previous chapter constant parameters CJS model time-varying parameters, might want fit -models time variation either detection survival.Let’s start model time-varying survival constant detection. NIMBLE, code :obtain following estimates: commentNow model time-varying detection constant survival, NIMBLE code :Parameter estimates : commentWe’re left four models. Different ecological hypotheses. ? Model selection. raffiner","code":"\nhmm.phitp <- nimbleCode({\n  for (t in 1:(T-1)){\n    phi[t] ~ dunif(0, 1) # prior survival\n    gamma[1,1,t] <- phi[t]      # Pr(alive t -> alive t+1)\n    gamma[1,2,t] <- 1 - phi[t]  # Pr(alive t -> dead t+1)\n    gamma[2,1,t] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,t] <- 1        # Pr(dead t -> dead t+1)\n  }\n  p ~ dunif(0, 1) # prior detection\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)\n  omega[1,2] <- p        # Pr(alive t -> detected t)\n  omega[2,1] <- 1        # Pr(dead t -> non-detected t)\n  omega[2,2] <- 0        # Pr(dead t -> detected t)\n  # likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, j-1])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2])\n    }\n  }\n})##        mean   sd 2.5%  50% 97.5% Rhat n.eff\n## phi[1] 0.63 0.10 0.42 0.63  0.82 1.04   564\n## phi[2] 0.46 0.06 0.35 0.46  0.59 1.01   629\n## phi[3] 0.48 0.05 0.37 0.48  0.59 1.00   610\n## phi[4] 0.62 0.06 0.51 0.62  0.73 1.00   553\n## phi[5] 0.61 0.05 0.50 0.61  0.72 1.00   568\n## phi[6] 0.59 0.05 0.48 0.59  0.69 1.03   463\n## p      0.89 0.03 0.82 0.89  0.95 1.04   211\nhmm.phipt <- nimbleCode({\n  phi ~ dunif(0, 1) # prior survival\n  gamma[1,1] <- phi      # Pr(alive t -> alive t+1)\n  gamma[1,2] <- 1 - phi  # Pr(alive t -> dead t+1)\n  gamma[2,1] <- 0        # Pr(dead t -> alive t+1)\n  gamma[2,2] <- 1        # Pr(dead t -> dead t+1)\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  for (t in 1:(T-1)){\n    p[t] ~ dunif(0, 1) # prior detection\n    omega[1,1,t] <- 1 - p[t]    # Pr(alive t -> non-detected t)\n    omega[1,2,t] <- p[t]        # Pr(alive t -> detected t)\n    omega[2,1,t] <- 1        # Pr(dead t -> non-detected t)\n    omega[2,2,t] <- 0        # Pr(dead t -> detected t)\n  }\n  # likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2, j-1])\n    }\n  }\n})##      mean   sd 2.5%  50% 97.5% Rhat n.eff\n## phi  0.56 0.03 0.52 0.56  0.61 1.02   381\n## p[1] 0.75 0.12 0.48 0.77  0.93 1.03   452\n## p[2] 0.85 0.08 0.68 0.86  0.97 1.02   359\n## p[3] 0.85 0.07 0.69 0.85  0.96 1.00   316\n## p[4] 0.89 0.05 0.77 0.89  0.97 1.00   412\n## p[5] 0.91 0.04 0.82 0.92  0.98 1.00   376\n## p[6] 0.90 0.07 0.73 0.91  1.00 1.07   111"},{"path":"survival.html","id":"model-selection-via-waic","chapter":"1 Survival","heading":"1.6 Model selection via WAIC","text":"four models best supported data? answer question, need bear mind used observed data fit models, close truth models perform predicting future data – predictive accuracy – assessed. natural candidate measure predictive accuracy likelihood referred context predictive density. However, know neither true process, future data, can estimate predictive density bias.may heard Akaike Information Criterion (AIC) Feequentist framework, Deviance Information Criterion (DIC) Bayesian framework. consider Widely Applicable Information Criterion Watanabe Information Criterion (WAIC). AIC, DIC WAIC aim provide approximation predictive accuracy.AIC predictive measure choice Frequentist framework ecologists, DIC around time Bayesian applications due availability population BUGS pieces software. However, methods utilize point estimate unknown parameters. Also, various difficulties noted DIC may give nonsensical results posterior distribution well summarized mean. fully Bayesian approach like use entire posterior distribution evaluate predictive performance, exaclty WAIC .Conveniently, NIMBLE calculates WAIC . E.g. model previous chapter. explainDipper example - continuedLower values WAIC imply higher predictive accuracy. favor model constant parameters.","code":"\nparameters.to.save <- c(\"phi\", \"p\", \"z\") \nmcmc.phitpt <- nimbleMCMC(code = hmm.phitpt,\n                          constants = my.constants,\n                          data = my.data,\n                          inits = initial.values,\n                          monitors = parameters.to.save,\n                          niter = n.iter,\n                          nburnin = n.burnin,\n                          nchains = n.chains,\n                          WAIC = TRUE) ##       model  WAIC\n## 1   (phi,p) 265.9\n## 2  (phit,p) 277.6\n## 3  (phi,pt) 270.2\n## 4 (phit,pt) 308.8"},{"path":"survival.html","id":"why-bayes-incorporate-prior-information","chapter":"1 Survival","heading":"1.7 Why Bayes? Incorporate prior information","text":"far, assumed non-informative prior survival \\(\\text{Beta}(1,1) = \\text{Uniform}(0,1)\\). prior, mean posterior survival \\(\\phi = 0.56\\) credible interval \\([0.52,0.62]\\). Graphically may represent posterior distribution survival obtained two chains different colors, prior gray dashed line:\n","code":""},{"path":"survival.html","id":"prior-elicitation","chapter":"1 Survival","heading":"1.7.1 Prior elicitation","text":"thing know lot passerines shame able use information act start scratch know nothing. illustrate incorporate prior information acknowledging species similar body masses similar survival. gathering information several European passerines dipper, let’s assume built regression survival vs. body mass – allometric relationship. Knowing dippers weigh average 59.8g, ’re now position build prior dipper survival probability predicting value using regression. obtain predicted survival 0.57 standard deviation 0.075. Using informative prior \\(\\text{Normal}(0.57, sd = 0.073)\\) NIMBLE, get mean posterior \\(0.56\\) credible interval \\([0.52, 0.61]\\). ’s barely difference non-informative prior, quite disappointment.Now let’s assume three first years data, happened? fit model constant parameters non-informative informative priors dataset delete final 4 years data. Now benefit using prior information becomes clear credible interval prior information ignored width 0.53, twice much prior information used (0.24), illustrating increased precision provided prior. may assess visually gain precision comparing survival posterior distributions without informative prior:brief, aim get estimate survival, Gilbert conduct data collection 3 years, reached precision 7 years data using prior information derived body mass. brief, prior information worth 4 years field data. course, assuming ecological question remains whether 3 7 years data, unlikely case, long-term data, much can ask, “just” annual survival probability .","code":""},{"path":"survival.html","id":"moment-matching","chapter":"1 Survival","heading":"1.7.2 Moment matching","text":"prior \\(\\text{Normal}(0.57, sd = 0.073)\\) entirely satisfying constrained positive less one, minimum probability (survival) well defined. specific example, prior distribution centered positive values far 0, sandard deviation small enough chances get values smaller 0 higher 1 null (convince , hist(rnorm(1000, mean = 0.57, sd = 0.073))). Can better? answer yes.Remember Beta distribution? Recall Beta distribution continuous distribution values 0 1. therefore convenient specify priors survival detection probabilities. Plus know everything Beta distribution, particular moments. \\(X \\sim Beta(\\alpha,\\beta)\\), first (mean) second moments (variance) \\(X\\) \\(\\mu = \\text{E}(X) = \\frac{\\alpha}{\\alpha + \\beta}\\) \\(\\sigma^2 = \\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\).capture-recapture example, know priori mean probability ’re interested \\(\\mu = 0.57\\) variance \\(\\sigma^2 = 0.073^2\\). Parameters \\(\\mu\\) \\(\\sigma^2\\) seen moments \\(Beta(\\alpha,\\beta)\\) distribution. Now look values \\(\\alpha\\) \\(\\beta\\) match observed moments Beta distribution \\(\\mu\\) \\(\\sigma^2\\). need another set equations:\n\\[\\alpha = \\bigg(\\frac{1-\\mu}{\\sigma^2}- \\frac{1}{\\mu} \\bigg)\\mu^2\\]\n\\[\\beta = \\alpha \\bigg(\\frac{1}{\\mu}-1\\bigg)\\]\nmodel, means:Now simply use \\(\\text{Beta}(\\alpha = 25.6,\\beta = 19.3)\\) prior instead \\(\\text{Normal}(0.57, sd = 0.073)\\).","code":"\n(alpha <- ( (1 - 0.57)/(0.073*0.073) - (1/0.57) )*0.57^2)\n## [1] 25.65\n(beta <- alpha * ( (1/0.57) - 1))\n## [1] 19.35"},{"path":"survival.html","id":"model-assumptions","chapter":"1 Survival","heading":"1.8 Model assumptions","text":"Statistical models rely assumptions, CJS model makes exception.Bayesian framework, posterior predictive checks. Explain principle. However omnibus, test directions departure good fit CJS data. Also need figure distance, summary statistics, easy w/ HMM.Good think tests.Design: mark lost, Identity individuals recorded without error (false positives), Captured individuals random sample.Model: Homogeneity survival recapture probabilities, Independence individuals (overdispersion).Test validity assumptions: assumptions valid, whatever inferential framework, Use goodness--fit tests — Pradel et al. (2005), R implementation package R2ucare. Posterior predictive checks can also used (Gelman et al. 2020). Make distinction omnibus vs. directional tests (two-sided vs one-sided).Example CJS w/ Dipper. see ways account transience trap-dependence. Gof model obtained easily sustracting. aware ’m mixing Bayesian Frequentist frameworks, information theoretic null hypothesis testing, colleagues might like , best two worlds, just dismiss amount research done gof testing?!close section model assumptions, stress survival refers study area, need think carefully survival actually mean capture-recapture. precisely, mortality permanent emigration confounded. Therefore estimate apparent survival, true survival. Apparent survival probability product true survival study area fidelity. Consequently, apparent survival always lower true survival unless study area fidelity exactly 1. Use caution interpretation.Get encounter histories, counts groups:Consider females ::Overall test:perform Test.3Sr transience Test3.Sm:Test2.Ct trap-depWhat tests significant? detect transient effect, 2 age classes considered survival probability account issue (fast forward covariate section ). trap dependence significant, Cooch White (2017) illustrate use time-varying individual covariate account effect (fast forward section covariate ), Pradel Sanz (2012) recommend multievent models (fast forward case study).","code":"\nlibrary(R2ucare)\ndipper <- read_csv(here::here(\"dat\", \"dipper.csv\"))\ndip.hist <- dipper %>%\n  select(year_1981:year_1987) %>%\n  as.matrix()\ndip.freq <- rep(1, nrow(dip.hist))\ndip.group <- dipper$sex\nmask <- (dip.group == 'F')\ndip.fem.hist <- dip.hist[mask,]\ndip.fem.freq <- dip.freq[mask]\noverall_CJS(dip.fem.hist, dip.fem.freq)\n##                          chi2 degree_of_freedom p_value\n## Gof test for CJS model: 10.28                12   0.592\ntest3sr <- test3sr(dip.fem.hist, dip.fem.freq)\ntest3sr\n## $test3sr\n##      stat        df     p_val sign_test \n##     4.985     5.000     0.418     1.428 \n## \n## $details\n##   component  stat p_val signed_test  test_perf\n## 1         2 0.858 0.354       0.926 Chi-square\n## 2         3 3.586 0.058       1.894 Chi-square\n## 3         4 0.437 0.509       0.661 Chi-square\n## 4         5 0.103 0.748      -0.321 Chi-square\n## 5         6 0.001 0.982       0.032 Chi-square\ntest2ct <- test2ct(dip.fem.hist, dip.fem.freq)\ntest2ct\n## $test2ct\n##      stat        df     p_val sign_test \n##     3.250     4.000     0.517    -0.901 \n## \n## $details\n##   component dof stat p_val signed_test test_perf\n## 1         2   1    0     1           0    Fisher\n## 2         3   1    0     1           0    Fisher\n## 3         4   1    0     1           0    Fisher\n## 4         5   1 3.25 0.071      -1.803    Fisher"},{"path":"survival.html","id":"covariates","chapter":"1 Survival","heading":"1.9 Covariates","text":"Proportion variance explained. Path analyses, structural equation models. Splines (spatial stats? CAR model?). Imputation multistate models account missing data. Explain basics parametric statistical modeling (linear models, GLMs random effects).Talk ANODEV?","code":""},{"path":"survival.html","id":"can-we-explain-time-variation-embrace-heterogeneity","chapter":"1 Survival","heading":"1.9.1 Can we explain time variation? Embrace heterogeneity","text":"Include temporal covariates, say \\(x_t\\).Include temporal covariates, say \\(x_t\\).\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t\\).\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t\\).Let’s investigate effect water flow dipper survival (Marzolin 2002).Let’s investigate effect water flow dipper survival (Marzolin 2002).]Regression intercept slopeTime-dependent (covariate constrained) survival probability estimates","code":"\nhmm.phiflowp <- nimbleCode({\n  delta[1] <- 1          # Pr(alive t = 1) = 1\n  delta[2] <- 0          # Pr(dead t = 1) = 0\n  for (t in 1:(T-1)){\n    logit(phi[t]) <- beta[1] + beta[2] * flow[t] #<<\n    gamma[1,1,t] <- phi[t]      # Pr(alive t -> alive t+1)\n    gamma[1,2,t] <- 1 - phi[t]  # Pr(alive t -> dead t+1)\n    gamma[2,1,t] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,t] <- 1        # Pr(dead t -> dead t+1)\n  }\n  p ~ dunif(0, 1) # prior detection\n  omega[1,1] <- 1 - p    # Pr(alive t -> non-detected t)\n  omega[1,2] <- p        # Pr(alive t -> detected t)\n  omega[2,1] <- 1        # Pr(dead t -> non-detected t)\n  omega[2,2] <- 0        # Pr(dead t -> detected t)\n  beta[1] ~ dnorm(0, 1.5) # prior intercept #<<\n  beta[2] ~ dnorm(0, 1.5) # prior slope #<<\n  # likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, j-1])\n      y[i,j] ~ dcat(omega[z[i,j], 1:2])\n    }\n  }\n})\n# water flow in L/s\nwater_flow <- c(443, 1114, 529, 434, 627, 466) # 1981, 1982, ..., 1987\nwater_flow_st <- (water_flow - mean(water_flow))/sd(water_flow) #<<\nmy.constants <- list(N = nrow(y),\n                     T = ncol(y),\n                     first = first,\n                     flow = water_flow_st) #<<\n\ninitial.values <- function() list(beta = rnorm(2,0,1),\n                                  p = runif(1,0,1),\n                                  z = zinits)\n\nparameters.to.save <- c(\"beta\", \"p\", \"phi\")"},{"path":"survival.html","id":"embrace-heterogeneity","chapter":"1 Survival","heading":"1.9.2 Embrace heterogeneity","text":"Include temporal covariates, say \\(x_t\\)Include temporal covariates, say \\(x_t\\)\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t\\)\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t\\)temporal variation fully explained covariates, add random effectsIf temporal variation fully explained covariates, add random effects\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t + \\varepsilon_t, \\; \\varepsilon_t \\sim N(0,\\sigma^2)\\)\\(\\text{logit}(\\phi_t) = \\beta_1 + \\beta_2 x_t + \\varepsilon_t, \\; \\varepsilon_t \\sim N(0,\\sigma^2)\\)","code":"hmm.phiflowREp <- nimbleCode({\n  for (t in 1:(T-1)){\n    logit(phi[t]) <- beta[1] + beta[2] * flow[t] + eps[t]\n    eps[t] ~ dnorm(0, sd = sdeps)\n    ...\n  }\n  sdeps ~ dunif(0,10)\n  ..."},{"path":"survival.html","id":"what-about-individual-heterogeneity","chapter":"1 Survival","heading":"1.9.3 What about individual heterogeneity?","text":"Discrete covariate like, e.g., sexDiscrete covariate like, e.g., sexContinuous covariate like, e.g., mass sizeContinuous covariate like, e.g., mass sizeSex wing length DipperSex effectLet’s use covariate \\(\\text{sex}\\) takes value 0 male, 1 femaleLet’s use covariate \\(\\text{sex}\\) takes value 0 male, 1 femaleAnd write \\(\\text{logit}(\\phi_i) = \\beta_1 + \\beta_2 \\; \\text{sex}_i\\) bird \\(\\)write \\(\\text{logit}(\\phi_i) = \\beta_1 + \\beta_2 \\; \\text{sex}_i\\) bird \\(\\)male survival isThen male survival \\[\\text{logit}(\\phi_i) = \\beta_1\\]female survival \\[\\text{logit}(\\phi_i) = \\beta_1 + \\beta_2\\]Nimble implementation sex covariateNimble implementation nested indexingLet’s use covariate \\(\\text{sex}\\) contains 1s 2s, indicating sex individual: 1 male, 2 femaleE.g. individual \\(= 2\\), beta[sex[]] gives beta[sex[2]] beta[1] beta[2] depending whether sex[2] 1 2.wing length?Wing lengthYou may test effect sex wing length, see exercise Worksheets.","code":"\nhmm.phisexp <- nimbleCode({\n...\n  for (i in 1:N){ #<<\n    logit(phi[i]) <- beta[1] + beta[2] * sex[i] #<<\n    gamma[1,1,i] <- phi[i]      # Pr(alive t -> alive t+1)\n    gamma[1,2,i] <- 1 - phi[i]  # Pr(alive t -> dead t+1)\n    gamma[2,1,i] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,i] <- 1        # Pr(dead t -> dead t+1)\n  } #<<\n  beta[1] ~ dnorm(mean = 0, sd = 1.5) #<<\n  beta[2] ~ dnorm(mean = 0, sd = 1.5) #<<\n  phi_male <- 1/(1+exp(-beta[1])) #<<\n  phi_female <- 1/(1+exp(-(beta[1]+beta[2]))) #<<\n...\n  # likelihood\n  for (i in 1:N){\n    z[i,first[i]] ~ dcat(delta[1:2])\n    for (j in (first[i]+1):T){\n      z[i,j] ~ dcat(gamma[z[i,j-1], 1:2, i]) #<<\n      y[i,j] ~ dcat(omega[z[i,j], 1:2])\n    }\n  }\n})##             mean   sd  2.5%   50% 97.5% Rhat n.eff\n## beta[1]     0.29 0.14  0.01  0.29  0.57 1.01   237\n## beta[2]    -0.09 0.19 -0.47 -0.10  0.29 1.01   241\n## p           0.90 0.03  0.83  0.90  0.95 1.02   253\n## phi_female  0.55 0.04  0.48  0.55  0.62 1.02   698\n## phi_male    0.57 0.03  0.50  0.57  0.64 1.01   237\n...\nfor (i in 1:N){\n  phi[i] <- beta[sex[i]] #<<\n  gamma[1,1,i] <- phi[i]      # Pr(alive t -> alive t+1)\n  gamma[1,2,i] <- 1 - phi[i]  # Pr(alive t -> dead t+1)\n  gamma[2,1,i] <- 0           # Pr(dead t -> alive t+1)\n  gamma[2,2,i] <- 1           # Pr(dead t -> dead t+1)\n}\nbeta[1] ~ dunif(0,1) # male survival #<<\nbeta[2] ~ dunif(0,1) # female survival #<<\n...##         mean   sd 2.5%  50% 97.5% Rhat n.eff\n## beta[1] 0.57 0.03 0.50 0.57  0.63 1.00   616\n## beta[2] 0.55 0.03 0.48 0.55  0.62 1.02   657\n## p       0.90 0.03 0.83 0.90  0.95 1.10   229\n...\n  for (i in 1:N){ #<<\n    logit(phi[i]) <- beta[1] + beta[2] * winglength[i] #<<\n    gamma[1,1,i] <- phi[i]      # Pr(alive t -> alive t+1)\n    gamma[1,2,i] <- 1 - phi[i]  # Pr(alive t -> dead t+1)\n    gamma[2,1,i] <- 0        # Pr(dead t -> alive t+1)\n    gamma[2,2,i] <- 1        # Pr(dead t -> dead t+1)\n  }\n  beta[1] ~ dnorm(mean = 0, sd = 1.5) # intercept #<<\n  beta[2] ~ dnorm(mean = 0, sd = 1.5) # slope #<<\n..."},{"path":"survival.html","id":"what-if-covariates-vary-with-individual-and-time","chapter":"1 Survival","heading":"1.9.4 What if covariates vary with individual and time?","text":"Think age example (see exercises Worksheets); covariate nested indexing works fine.Think age example (see exercises Worksheets); covariate nested indexing works fine.Now, think body size across life.Now, think body size across life.Problem record size animal non-detected.Problem record size animal non-detected.Discretize small, medium large treat state — later.Discretize small, medium large treat state — later.Assume model covariate fill missing values (imputation).Assume model covariate fill missing values (imputation).","code":""},{"path":"survival.html","id":"summary","chapter":"1 Survival","heading":"1.10 Summary","text":"Blabla.Blabla.Blabla.Blabla.","code":""},{"path":"survival.html","id":"suggested-reading","chapter":"1 Survival","heading":"1.11 Suggested reading","text":"bit history CJS model people involved developements S.T. Buckland (2016).bit history CJS model people involved developements S.T. Buckland (2016).CJS state-space formulation Gimenez et al. (2007) Royle (2008).CJS state-space formulation Gimenez et al. (2007) Royle (2008).Also read Lebreton et al. 1992, long monography, gem, w/ introduction AIC, model assumptions, etc.Also read Lebreton et al. 1992, long monography, gem, w/ introduction AIC, model assumptions, etc.WAIC, video https://www.youtube.com/watch?v=vSjL2Zc-gEQ R. McElreath. Cite relevant papers particular paper Gelman et al. 2014 Understanding predictive information criteria Bayesian models.WAIC, video https://www.youtube.com/watch?v=vSjL2Zc-gEQ R. McElreath. Cite relevant papers particular paper Gelman et al. 2014 Understanding predictive information criteria Bayesian models.Work missing values Bonner et al. (2006) Langrock King (2013) Worthington et al. (2015).Work missing values Bonner et al. (2006) Langrock King (2013) Worthington et al. (2015).example incorporate prior information McCarthy Masters (2005).example incorporate prior information McCarthy Masters (2005).Combine live recapture w/ dead recoveries Lebreton et al. (1999) go spatial account emigration Gilroy et al. (2012) Schaub & Royle (2014).Combine live recapture w/ dead recoveries Lebreton et al. (1999) go spatial account emigration Gilroy et al. (2012) Schaub & Royle (2014).Non-identifiability Bayesian framework, see Gimenez et al. (2009) book Cole (2020).Non-identifiability Bayesian framework, see Gimenez et al. (2009) book Cole (2020).","code":""},{"path":"dispersal.html","id":"dispersal","chapter":"2 Dispersal","heading":"2 Dispersal","text":"WORK PROGRESS.","code":""},{"path":"uncertainty.html","id":"uncertainty","chapter":"3 State uncertainty","heading":"3 State uncertainty","text":"WORK PROGRESS.","code":""},{"path":"introduction-3.html","id":"introduction-3","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"introduction-4.html","id":"introduction-4","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"take-home-messages.html","id":"take-home-messages","chapter":"Take-home messages","heading":"Take-home messages","text":"–>\n –>–>–>–>","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
