<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.6 Hidden Markov models | banana-book.knit" />
<meta property="og:type" content="book" />






<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="1.6 Hidden Markov models | banana-book.knit">

<title>1.6 Hidden Markov models | banana-book.knit</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if bootstrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="1-hmmcapturerecapture.html#hmmcapturerecapture" id="toc-hmmcapturerecapture"><span class="toc-section-number">1</span> Hidden Markov models</a>
<ul>
<li><a href="1.1-introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1.1</span> Introduction</a></li>
<li><a href="1.2-longitudinal-data.html#longitudinal-data" id="toc-longitudinal-data"><span class="toc-section-number">1.2</span> Longitudinal data</a></li>
<li class="has-sub"><a href="1.3-a-markov-model-for-longitudinal-data.html#a-markov-model-for-longitudinal-data" id="toc-a-markov-model-for-longitudinal-data"><span class="toc-section-number">1.3</span> A Markov model for longitudinal data</a>
<ul>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#assumptions" id="toc-assumptions"><span class="toc-section-number">1.3.1</span> Assumptions</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#transition-matrix" id="toc-transition-matrix"><span class="toc-section-number">1.3.2</span> Transition matrix</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#initial-states" id="toc-initial-states"><span class="toc-section-number">1.3.3</span> Initial states</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#likelihood" id="toc-likelihood"><span class="toc-section-number">1.3.4</span> Likelihood</a></li>
<li><a href="1.3-a-markov-model-for-longitudinal-data.html#example" id="toc-example"><span class="toc-section-number">1.3.5</span> Example</a></li>
</ul></li>
<li><a href="1.4-bayesian-formulation.html#bayesian-formulation" id="toc-bayesian-formulation"><span class="toc-section-number">1.4</span> Bayesian formulation</a></li>
<li><a href="1.5-nimble-implementation.html#nimble-implementation" id="toc-nimble-implementation"><span class="toc-section-number">1.5</span> NIMBLE implementation</a></li>
<li class="has-sub"><a href="1.6-hidden-markov-models.html#hidden-markov-models" id="toc-hidden-markov-models"><span class="toc-section-number">1.6</span> Hidden Markov models</a>
<ul>
<li><a href="1.6-hidden-markov-models.html#capturerecapturedata" id="toc-capturerecapturedata"><span class="toc-section-number">1.6.1</span> Capture-recapture data</a></li>
<li><a href="1.6-hidden-markov-models.html#observation-matrix" id="toc-observation-matrix"><span class="toc-section-number">1.6.2</span> Observation matrix</a></li>
<li><a href="1.6-hidden-markov-models.html#hidden-markov-model" id="toc-hidden-markov-model"><span class="toc-section-number">1.6.3</span> Hidden Markov model</a></li>
<li><a href="1.6-hidden-markov-models.html#likelihoodhmm" id="toc-likelihoodhmm"><span class="toc-section-number">1.6.4</span> Likelihood</a></li>
</ul></li>
<li><a href="1.7-fittinghmmnimble.html#fittinghmmnimble" id="toc-fittinghmmnimble"><span class="toc-section-number">1.7</span> Fitting HMM with NIMBLE</a></li>
<li class="has-sub"><a href="1.8-marginalization.html#marginalization" id="toc-marginalization"><span class="toc-section-number">1.8</span> Marginalization</a>
<ul>
<li><a href="1.8-marginalization.html#brute-force-approach" id="toc-brute-force-approach"><span class="toc-section-number">1.8.1</span> Brute-force approach</a></li>
<li><a href="1.8-marginalization.html#forward-algorithm" id="toc-forward-algorithm"><span class="toc-section-number">1.8.2</span> Forward algorithm</a></li>
<li><a href="1.8-marginalization.html#nimblemarginalization" id="toc-nimblemarginalization"><span class="toc-section-number">1.8.3</span> NIMBLE implementation</a></li>
</ul></li>
<li><a href="1.9-pooled-likelihood.html#pooled-likelihood" id="toc-pooled-likelihood"><span class="toc-section-number">1.9</span> Pooled encounter histories</a></li>
<li class="has-sub"><a href="1.10-decoding.html#decoding" id="toc-decoding"><span class="toc-section-number">1.10</span> Decoding after marginalization</a>
<ul>
<li><a href="1.10-decoding.html#viterbi-theory" id="toc-viterbi-theory"><span class="toc-section-number">1.10.1</span> Theory</a></li>
<li><a href="1.10-decoding.html#implementation" id="toc-implementation"><span class="toc-section-number">1.10.2</span> Implementation</a></li>
<li><a href="1.10-decoding.html#compute-average" id="toc-compute-average"><span class="toc-section-number">1.10.3</span> Compute first, average after</a></li>
<li><a href="1.10-decoding.html#average-first-compute-after" id="toc-average-first-compute-after"><span class="toc-section-number">1.10.4</span> Average first, compute after</a></li>
</ul></li>
<li><a href="1.11-summary.html#summary" id="toc-summary"><span class="toc-section-number">1.11</span> Summary</a></li>
<li><a href="1.12-suggested-reading.html#suggested-reading" id="toc-suggested-reading"><span class="toc-section-number">1.12</span> Suggested reading</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="hidden-markov-models" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Hidden Markov models</h2>
<div id="capturerecapturedata" class="section level3" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Capture-recapture data</h3>
<div class="blackbox">
<p>Unfortunately, the data with alive and dead states is the data we wish we had. In real life, animals cannot be monitored exhaustively, like humans in a medical trial. This is why we use capture-recapture protocols, in which animals are captured, individually marked, and released alive. Then, these animals may be detected again, or go undetected. Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected. This issue is usually referred to as that of imperfect detection. As a consequence of imperfect detection, the Markov process for survival is only partially observed: You know an animal is alive when you detect it, but when an animal goes undetected, whether it is alive or dead is unknown to you. This is where hidden Markov models (HMMs) come in.</p>
</div>
<p>Let’s get back to the data we had in the previous section. The truth is in <span class="math inline">\(z\)</span> which contains the fate of all individuals with <span class="math inline">\(z = 1\)</span> for alive, and <span class="math inline">\(z = 2\)</span> for dead:</p>
<pre><code>## # A tibble: 57 × 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    &lt;int&gt;      &lt;int&gt;      &lt;int&gt;      &lt;int&gt;      &lt;int&gt;
##  1     1          1          1          1          1
##  2     2          1          1          1          1
##  3     3          1          1          1          1
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          1          2          2
##  7     7          1          1          1          1
##  8     8          1          2          2          2
##  9     9          1          1          1          1
## 10    10          1          2          2          2
## # ℹ 47 more rows
## # ℹ 1 more variable: `winter 5` &lt;int&gt;</code></pre>
<p>Unfortunately, we have only partial access to <span class="math inline">\(z\)</span>. What we do observe is <span class="math inline">\(y\)</span> the detections and non-detections. How are <span class="math inline">\(z\)</span> and <span class="math inline">\(y\)</span> connected?</p>
<p>The easiest connection is with dead animals which go undetected for sure. Therefore when an animal is dead i.e. <span class="math inline">\(z = 2\)</span>, it cannot be detected, therefore <span class="math inline">\(y = 0\)</span>:</p>
<pre><code>## # A tibble: 57 × 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    &lt;int&gt;      &lt;int&gt;      &lt;int&gt;      &lt;int&gt;      &lt;int&gt;
##  1     1          1          1          1          1
##  2     2          1          1          1          1
##  3     3          1          1          1          1
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          1          0          0
##  7     7          1          1          1          1
##  8     8          1          0          0          0
##  9     9          1          1          1          1
## 10    10          1          0          0          0
## # ℹ 47 more rows
## # ℹ 1 more variable: `winter 5` &lt;int&gt;</code></pre>
<p>Now alive animals may be detected or not. If an animal is alive <span class="math inline">\(z = 1\)</span>, it is detected <span class="math inline">\(y = 1\)</span> with probability <span class="math inline">\(p\)</span> or not <span class="math inline">\(y = 0\)</span> with probability <span class="math inline">\(1-p\)</span>. In our example, first detection coincides with first winter for all individuals.</p>
<pre><code>## # A tibble: 57 × 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
##  1     1          1          0          0          0
##  2     2          1          0          1          0
##  3     3          1          0          0          0
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          0          0          0
##  7     7          1          0          1          1
##  8     8          1          1          1          1
##  9     9          1          1          1          1
## 10    10          1          1          0          0
## # ℹ 47 more rows
## # ℹ 1 more variable: `winter 5` &lt;dbl&gt;</code></pre>
<p>Compare with the previous <span class="math inline">\(z\)</span> table. Some 1’s for alive have become 0’s for non-detection, other 1’s for alive have remained 1’s for detection. This <span class="math inline">\(y\)</span> table is what we observe in real life. I hope I have convinced you that to make the connection between observations, the <span class="math inline">\(y\)</span>, and true states, the <span class="math inline">\(z\)</span>, we need to describe how observations are made (or emitted in the HMM terminology) from the states.</p>
</div>
<div id="observation-matrix" class="section level3" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Observation matrix</h3>
<p>The novelty in HMMs is the link between observations and states. This link is made through observation probabilities. For example, the probability of detecting an animal <span class="math inline">\(i\)</span> at <span class="math inline">\(t\)</span> given it is alive at <span class="math inline">\(t\)</span> is <span class="math inline">\(\Pr(y_{i,t}=2|z_{i,t}=1)=\omega_{1,2}\)</span>. It is the detection probability <span class="math inline">\(p\)</span>. If individual <span class="math inline">\(i\)</span> is dead at <span class="math inline">\(t\)</span>, then it is missed for sure, and <span class="math inline">\(\Pr(y_{i,t}=1|z_{i,t}=2)=\omega_{2,1}=1\)</span>.</p>
<p>We can gather these observation probabilities into an observation matrix <span class="math inline">\(\mathbf{\Omega}\)</span>. In rows we have the states alive <span class="math inline">\(z = 1\)</span> and dead <span class="math inline">\(z = 2\)</span>, while in columns we have the observations non-detected <span class="math inline">\(y = 1\)</span> and detected <span class="math inline">\(y = 2\)</span> (previously coded 0 and 1 respectively):</p>
<p><span class="math display">\[\begin{align*}
\mathbf{\Omega} =
\left(\begin{array}{cc}
\omega_{1,1} &amp; \omega_{1,2}\\
\omega_{2,1} &amp; \omega_{2,2}
\end{array}\right) =
\left(\begin{array}{cc}
1 - p &amp; p\\
1 &amp; 0
\end{array}\right)
\end{align*}\]</span></p>
<p>In survival models we will use throughout this book, we condition the fate of individuals on first detection, which boils down to set the corresponding detection probability to 1.</p>
<p>The observation matrix is:</p>
<p><span class="math display">\[\begin{matrix}
&amp; \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 &amp; y_t=2 \\
    \mbox{(non-detected)} &amp; \mbox{(detected)} \\ \hdashline
1 - p &amp; p\\
1 &amp; 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
&amp; \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t}=1 \; \mbox{(alive)}\\ z_{t}=2 \; \mbox{(dead)}
    \end{matrix}
\end{matrix}\]</span></p>
</div>
<div id="hidden-markov-model" class="section level3" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> Hidden Markov model</h3>
<p>Our hidden Markov model can be represented this way:</p>
<p><img src="banana-book_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>States <span class="math inline">\(z\)</span> are in gray. Observations <span class="math inline">\(y\)</span> are in white. All individuals are first captured in the first winter <span class="math inline">\(t = 1\)</span>, and are therefore all alive <span class="math inline">\(z_1 = 1\)</span> and detected <span class="math inline">\(y_1 = 2\)</span>.</p>
<div class="blackbox">
<p>A hidden Markov model is just two time series running in parallel. One for the states with the Markovian property, and the other of for the observations generated from the states. HMM are a special case of state-space models in which latent states are discrete.</p>
</div>
<p>Have a look to the example below, in which an individual is detected at first sampling occasion, detected again, then missed for the rest of the study. While on occasion <span class="math inline">\(t=3\)</span> that individual was alive <span class="math inline">\(z_3=1\)</span> and went undetected <span class="math inline">\(y_3=1\)</span>, on occasions <span class="math inline">\(t=4\)</span> and <span class="math inline">\(t=5\)</span> it went undetected <span class="math inline">\(y_4=y_5=1\)</span> because it was dead <span class="math inline">\(z_4=z_5=2\)</span>. Because we condition on first detection, the link between state and observation at <span class="math inline">\(t=1\)</span> is deterministic and <span class="math inline">\(p = 1\)</span>.</p>
<p><img src="banana-book_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<div id="likelihoodhmm" class="section level3" number="1.6.4">
<h3><span class="header-section-number">1.6.4</span> Likelihood</h3>
<p>In the Bayesian framework, we usually work with the so-called complete likelihood, that is the probability of the observed data <span class="math inline">\(y\)</span> and the latent states <span class="math inline">\(z\)</span> given the parameters of our model, here the survival and detection probabilities <span class="math inline">\(\phi\)</span> and <span class="math inline">\(p\)</span>. The complete likelihood for individual <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
\end{align*}\]</span></p>
<p>Using the definition of a conditional probability, we have:</p>
<p><span class="math display">\[\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &amp;= \color{blue}{\Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})}\\
\end{align*}\]</span></p>
<p>Then by using the independence of the <span class="math inline">\(y\)</span> conditional on the <span class="math inline">\(z\)</span>, and the likelihood of a Markov chain, we get that:</p>
<p><span class="math display">\[\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                &amp;= \color{blue}{\left(\prod_{t=1}^T{\Pr{(y_{i,t} | z_{i,t})}}\right) \left(\Pr(z_{i,1}) \prod_{t=2}^T{\Pr{(z_{i,t} | z_{i,t-1})}}\right)}\\
\end{align*}\]</span></p>
<p>Finally, by recognizing the observation and transition probabilities, we have that the complete likelihood for individual <span class="math inline">\(i\)</span> is:</p>
<p><span class="math display">\[\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &amp;= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                &amp;= \color{blue}{\left(\prod_{t=1}^T{\omega_{z_{i,t}, y_{i,t}}}\right) \left(\Pr(z_{i,1}) \prod_{t=2}^T{\gamma_{z_{i,t-1},z_{i,t}}}\right)}\\
\end{align*}\]</span></p>
<p>To obtain the complete likelihood of the whole dataset, we need to multiply this individual likelihood for each animal <span class="math inline">\(\displaystyle{\prod_{i=1}^N{\Pr(\mathbf{y}_i,\mathbf{z}_i)}}\)</span>. When several individuals have the same contribution, calculating their individual contribution only once can greatly reduce the computational burden, as illustrated in Section <a href="1.9-pooled-likelihood.html#pooled-likelihood">1.9</a>.</p>
<p>The Bayesian approach with MCMC methods allows treating the latent states <span class="math inline">\(z_{i,t}\)</span> as if they were parameters, and to be estimated as such. However, the likelihood is rather complex with a large number of latent states <span class="math inline">\(z_{i,t}\)</span>, which comes with computational costs and slow mixing. There are situations where the latent states are the focus of ecological inference and need to be estimated (see Suggested reading below). However, if not needed, you might want to get rid of the latent states and rely on the so-called marginal likelihood. By doing so, you can avoid sampling the latent states, focus on the ecological parameters, and often speeds up computations and improves mixing as shown in Section <a href="1.8-marginalization.html#marginalization">1.8</a>. Actually, you can even estimate the latent states afterwards, as illustrated in Section <a href="1.10-decoding.html#decoding">1.10</a>.</p>
<!-- It has a matrix formulation: -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y}) &= \mathbf{\delta} \; \mathbf{\Omega} \; \mathbf{\Gamma} \cdots \mathbf{\Omega} \; \mathbf{\Gamma} \; \mathbf{\Omega} \; \mathbb{1} -->
<!-- \end{align*} -->
<!-- ### Example -->
<!-- Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood? -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\ -->
<!-- \end{align*} -->
<!-- Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood? -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\ -->
<!-- &= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\ -->
<!-- \end{align*} -->
<!-- Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood? -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\ -->
<!-- &= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\ -->
<!-- &= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1}\delta_1 \gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 2} -->
<!-- \end{align*} -->
<!-- Note: $\Pr(z_1 = 1) = \delta_1 = 1$ and $\Pr(z_1 = 2) = 0$. -->
<!-- Let assume an animal is detected, then missed. We have $\mathbf{y} = (2, 1)$. What is the contribution of this animal to the likelihood? -->
<!-- \begin{align*} -->
<!-- \Pr(\mathbf{y} = (2, 1)) &= \sum_{z_1 = 1}^2 \; \sum_{z_2 = 1}^2 w_{z_1, y_1 = 2} w_{z_2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2} \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1, z_1, z_1, z_1)}\\ -->
<!-- &= \sum_{z_1 = 1}^2 \left( w_{z_1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 1} + w_{z_1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \Pr(z_1) \gamma_{z_1, z_2 = 2} \right) \\ -->
<!-- &= w_{z_1 = 1, y_1 = 2} w_{z_2 = 1, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 1} + w_{z_1 = 1, y_1 = 2} w_{z_2 = 2, y_2 = 1} \delta_1 \gamma_{z_1 = 1, z_2 = 2}\\ -->
<!-- &= (1 - p) \phi + (1-\phi) -->
<!-- \end{align*} -->
<!-- Note: $w_{z_1 = 1, y_1 = 2} = \Pr(y_1 = 2 | z_1 = 1) = 1$ because we condition on first capture. -->
</div>
</div>
<p style="text-align: center;">
<a href="1.5-nimble-implementation.html"><button class="btn btn-default">Previous</button></a>
<a href="1.7-fittinghmmnimble.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
