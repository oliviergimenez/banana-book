% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
]{krantz}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{Alegreya}
\usepackage[scale=.7]{sourcecodepro}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{blkarray}
\pgfplotsset{compat=1.18}
\usepackage{tcolorbox}
\usepackage{arydshln}
\newtcolorbox{blackbox}{ colback=white, colframe=purple, coltext=black, boxsep=5pt, arc=4pt}
\usepackage{subfig}
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Bayesian analysis of capture-recapture data with hidden Markov models},
  pdfauthor={Olivier Gimenez},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Bayesian analysis of capture-recapture data with hidden Markov models}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Theory and case studies in R and NIMBLE}
\author{Olivier Gimenez}
\date{2025-08-12}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables
\chapter*{Welcome}\label{welcome}


Welcome to the online version of the book \emph{Bayesian analysis of capture-recapture data with hidden Markov models: Theory and case studies in R and NIMBLE}. Here, I write about three of my favorite research topics -- capture-recapture, hidden Markov models and Bayesian statistics -- let's enjoy this great cocktail together ðŸ¹

I'm currently writing this book, and I welcome any feedback. You may raise an issue \href{https://github.com/oliviergimenez/banana-book/issues}{here}, amend directly the R Markdown file that generated the page you're reading by clicking on the `Edit this page' icon in the right panel, or \href{mailto:olivier.gimenez@cefe.cnrs.fr}{email me}. Many thanks!

Olivier Gimenez. Written in Montpellier, France and Athens, Greece.
Last updated: August 12, 2025

\section*{License}\label{license}


The online version of this book is licensed under the \href{http://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License}.

The code is public domain, licensed under \href{https://creativecommons.org/publicdomain/zero/1.0/}{Creative Commons CC0 1.0 Universal (CC0 1.0)}.

\chapter*{Preface}\label{preface}


\section*{Why this book?}\label{why-this-book}


The HMM framework has gained much attention in the ecological literature over the last decade, and has been suggested as a general modelling framework for the demography of plant and animal populations. HMMs are increasingly used to analyse capture-recapture data and estimate key population parameters (e.g., survival, dispersal, or recruitment) with applications in all fields of ecology. I have assembled a searchable list at \url{https://oliviergimenez.github.io/curated-list-HMM-apps/} of HMM analyses of capture-recapture data to get inspiration. This list is not exhaustive, please get in touch with us if you'd like to add a reference. The first objective of this book is to illustrate the flexibility of HMM to decompose complex problems in smaller pieces that are easier to understand, model and analyse.

In parallel, Bayesian statistics is well established and fast growing in ecology and related disciplines, because it resonates with the scientific reasoning and allows accommodating uncertainty smoothly. The popularity of Bayesian statistics also comes from the availability of free pieces of software (WinBUGS, OpenBUGS, JAGS, Stan) that allow practitioners to code their own analyses. The second objective of this book is to illustrate the use of the R package NIMBLE (\citet{deValpine2017}) to analyse capture-recapture data with HMM in a Bayesian framework. NIMBLE is seen by many as the future of Bayesian statistical ecology to deal with complex models and/or big data.

An important part of the book consists in case studies from published papers presented in a tutorial style to abide by the ``learning by doing'' philosophy. The third objective of this book is to provide reproducible analyses with code and data to teach yourself by example.

\section*{Who should read this book?}\label{who-should-read-this-book}


This book is aimed at beginners who're comfortable using R and write basic code, as well as connoisseurs of capture-recapture who'd like to tap into the power of the Bayesian side of statistics. For both audiences, thinking in the HMM framework will help you in confidently building models and make the most of your capture-recapture data.

\section*{What will you learn?}\label{what-will-you-learn}


The book is divided into three parts. The first part \texttt{Foundations} is aimed at getting you up-to-speed with Bayesian statistics, NIMBLE, and hidden Markov models. The second part \texttt{Transitions} will teach you all about capture-recapture models for open populations, with reproducible R code to ease the learning process. The third part \texttt{Case\ studies} provides real-world case studies from the scientific literature that you can reproduce using material covered in previous chapters. These problems can either i) be used to cement and deepen your understanding of methods and models, ii) be adapted for your own purpose, or iii) serve as teaching projects. \textbf{Here say that the code is available on GitHub, and the data through a package \texttt{crdata}.}

\section*{What won't you learn?}\label{what-wont-you-learn}


I won't lie, there is some maths in this book. However, the equations I use are either simple enough to be understood without a background in maths, or can be skipped without prejudice. I do not cover Bayesian statistics or even hidden Markov models exhaustively, I provide just what you need to work with capture-recapture data. If you are interested in knowing more about these topics, hopefully the section \texttt{Suggested\ reading} at the end of each chapter will put you in the right direction. There are also a number of important topics specific to capture-recapture that I do not cover, including closed-population capture-recapture models \citep{WilliamsEtAl2002}, spatial capture-recapture models \citep{RoyleEtAl2013book} and continuous models \citep{rushing2023continuouscr} (\textbf{I might end up writing a chapter on continuous models.}). These models can be treated as HMMs, but for now the usual formulation is just fine. These developments will be the subject of new chapters in a second edition, hopefully.

\section*{Prerequisites}\label{prerequisites}


This book uses primarily the R package NIMBLE, so you need to install at least R and NIMBLE. A bunch of other R packages are used. You can install them all at once by running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
  \StringTok{"bookdown"}\NormalTok{, }\StringTok{"coda"}\NormalTok{, }\StringTok{"forecast"}\NormalTok{, }\StringTok{"ggtern"}\NormalTok{, }\StringTok{"gtools"}\NormalTok{, }
  \StringTok{"here"}\NormalTok{, }\StringTok{"janitor"}\NormalTok{, }\StringTok{"magick"}\NormalTok{, }\StringTok{"MCMCvis"}\NormalTok{, }\StringTok{"nimble"}\NormalTok{, }
  \StringTok{"nimbleEcology"}\NormalTok{, }\StringTok{"patchwork"}\NormalTok{, }\StringTok{"pdftools"}\NormalTok{, }
  \StringTok{"RColorBrewer"}\NormalTok{, }\StringTok{"sessioninfo"}\NormalTok{, }\StringTok{"tidyverse"}\NormalTok{, }
  \StringTok{"wesanderson"} 
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\section*{How this book was written}\label{how-this-book-was-written}


I wrote this book in RStudio \url{http://www.rstudio.com/ide/} using bookdown \url{http://bookdown.org/}. The book website \url{https://oliviergimenez.github.io/banana-book} is hosted with GitHub Pages \url{https://pages.github.com/}, and automatically updated after every push by Github Actions \url{https://github.com/features/actions}. The source is available from GitHub \url{https://github.com/oliviergimenez/banana-book}.

The version of the book you're reading was built with R version 4.5.0 (2025-04-11) and the following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
package & version & source \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
bookdown & 0.43 & CRAN (R 4.5.0) \\
coda & 0.19-4.1 & CRAN (R 4.5.0) \\
forecast & 8.24.0 & CRAN (R 4.5.0) \\
ggtern & 3.5.0 & CRAN (R 4.5.0) \\
gtools & 3.9.5 & CRAN (R 4.5.0) \\
here & 1.0.1 & CRAN (R 4.5.0) \\
janitor & NA & NA \\
magick & 2.8.7 & CRAN (R 4.5.0) \\
MCMCvis & 0.16.3 & CRAN (R 4.5.0) \\
nimble & 1.3.0 & CRAN (R 4.5.0) \\
nimbleEcology & 0.5.0 & CRAN (R 4.5.0) \\
patchwork & 1.3.0 & CRAN (R 4.5.0) \\
pdftools & 3.5.0 & CRAN (R 4.5.0) \\
RColorBrewer & 1.1-3 & CRAN (R 4.5.0) \\
sessioninfo & 1.2.3 & CRAN (R 4.5.0) \\
tidyverse & 2.0.0 & CRAN (R 4.5.0) \\
wesanderson & 0.3.7 & CRAN (R 4.5.0) \\
\end{longtable}

\section*{About the author}\label{about-the-author}


My name is Olivier Gimenez (\url{https://oliviergimenez.github.io/}). I am a senior (euphemism for not so young anymore) scientist at the National Centre for Scientific Research (CNRS; \url{https://www.cnrs.fr/en}) in the beautiful city of Montpellier, France.

I struggled studying maths, obtained a PhD in applied statistics a long time ago in a galaxy of wine and cheese. I was awarded my habilitation (\url{https://en.wikipedia.org/wiki/Habilitation}) in ecology and evolution so that I could stop pretending to understand what my colleagues were talking about. More recently I embarked in sociology studies because hey, why not.

Lost somewhere at the interface of animal ecology, statistical modeling and social sciences, my so-called expertise lies in population dynamics and species distribution modeling to address questions in ecology and conservation biology about the impact of human activities and the management of carnivores. I would be nothing without the students and colleagues who are kind enough to bear with me.

You may find me on Twitter/X (\url{https://twitter.com/oaggimenez}), GitHub (\url{https://github.com/oliviergimenez}), or get in touch by email at olivier\textbar dot\textbar gimenez\textbar at\textbar cefe\textbar dot\textbar cnrs\textbar dot\textbar fr.

\section*{Acknowledgements}\label{acknowledgements}


Writing a book is quite an adventure, and a lot of people contributed to make this book a reality. I wish to thank:

\begin{itemize}
\tightlist
\item
  Rob Calver, Sherry Thomas, Vaishali Singh and Kumar Shashi at Chapman and Hall/CRC.\\
\item
  Marc KÃ©ry, Rachel McCrea, Byron Morgan and Etienne PrÃ©vost for their positive reviews of the book proposal I sent to Chapman and Hall/CRC, and their constructive comments and suggestions.\\
\item
  Marc KÃ©ry for his precious pieces of advice on the process of writing.\\
\item
  Perry de Valpine, Daniel Turek, Chris Paciorek and Ben Goldstein for the \texttt{NIMBLE} and \texttt{nimbleEcology} R packages.\\
\item
  Colleagues who shared their data; See list at \url{https://github.com/oliviergimenez/banana-book\#readme}
\item
  People who commented, corrected, offered pieces of advice; See list at \url{https://github.com/oliviergimenez/banana-book\#readme}.
\item
  Yihui Xie for the \texttt{bookdown} R package.
\item
  Attendees of the workshops we run in relation to the content of this book (latest edition was in 2023, see \url{https://oliviergimenez.github.io/bayesian-hmm-cr-workshop-valencia/})
\item
  Perry de Valpine, Sarah Cubaynes, ChloÃ© Nater, Maud QuÃ©rouÃ© and Daniel Turek for their help with running workshops in relation to the content of this book (2021 edition: \url{https://oliviergimenez.github.io/bayesian-cr-workshop/}; 2022 edition: \url{https://oliviergimenez.github.io/hmm-cr-nimble-isec2022-workshop/}).\\
\item
  Ruth King, Steve Brooks and Byron Morgan for the workshop on Bayesian statistics for ecologists we taught in Cambridge, the book we wrote together \citep{king_bayesian_2009}, and their contribution to statistical ecology.\\
\item
  Jean-Dominique Lebreton, Roger Pradel and RÃ©mi Choquet for the workshops on modelling individual histories with state uncertainty we taught over the years, and sharing their science of capture-recapture with me.\\
\item
  My family, including my mother, my parents-in-law for their kindness and hospitality, my amazing kids and wonderful wife for putting up with me while I was writing this book.
\end{itemize}

\mainmatter

\part{Foundations}\label{part-foundations}

\chapter*{Introduction}\label{introduction}


WORK IN PROGRESS

This first part \texttt{Foundations} is aimed at getting you up-to-speed with Bayesian statistics, NIMBLE, and hidden Markov models.

\chapter{Bayesian statistics \& MCMC}\label{crashcourse}

\section{Introduction}\label{introduction-1}

In this first chapter, you will learn what the Bayesian theory is, and how you may use it with a simple example. You will also see how to implement simulation algorithms to implement the Bayesian method for more complex analyses. This is not an exhaustive treatment of Bayesian statistics, but you should get what you need to navigate through the rest of the book.

\section{Bayes' theorem}\label{bayes-theorem}

Let's not wait any longer and jump into it. Bayesian statistics relies on the Bayes' theorem (or law, or rule, whatever you prefer) named after Reverend Thomas Bayes (Figure \ref{fig:revbayes}). This theorem was published in 1763 two years after Bayes' death thanks to his friend's efforts Richard Price, and was independently discovered by Pierre-Simon Laplace \citep{mcgrayne2011}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/amazing-thomas-bayes-illustration} 

}

\caption{Cartoon of Thomas Bayes with Bayes' theorem in background. Source: James Kulich at <https://www.elmhurst.edu/blog/thomas-bayes/>}\label{fig:revbayes}
\end{figure}

As we will see in a minute, Bayes' theorem is all about conditional probabilities, which are somehow tricky to understand. Conditional probability of outcome or event A given event B, which we denote \(\Pr(A \mid B)\), is the probability that A occurs, revised by considering the additional information that event B has occurred. For example, a friend of yours rolls a fair dice and asks you the probability that the outcome was a six (event A). Your answer is 1/6 because each side of the dice is equally likely to come up. Now imagine that you're told the number rolled was even (event B) before you answer your friend's question. Because there are only three even numbers, one of which is six, you may revise your answer for the probability that a six was rolled from 1/6 to \(\Pr(A \mid B) = 1/3\). The order in which A and B appear is important, make sure you do not confuse \(\Pr(A \mid B)\) and \(\Pr(B \mid A)\).

Bayes' theorem gives you \(\Pr(A \mid B)\) using marginal probabilities \(\Pr(A)\) and \(\Pr(B)\) and \(\Pr(B \mid A)\):

\[\Pr(A \mid B) = \displaystyle{\frac{ \Pr(B \mid A) \; \Pr(A)}{\Pr(B)}}.\]

Originally, Bayes' theorem was seen as a way to infer an unkown cause A of a particular effect B, knowing the probability of effect B given cause A. Think for example of a situation where a medical diagnosis is needed, with A an unknown disease and B symptoms, the doctor knows Pr(symptoms\textbar disease) and wants to derive Pr(disease\textbar symptoms). This way of reversing \(\Pr(B \mid A)\) into \(\Pr(A \mid B)\) explains why Bayesian thinking used to be referred to as `inverse probability'.

I don't know about you, but I need to think twice for not messing the letters around. I find it easier to remember Bayes' theorem written like this:

\[\Pr(\text{hypothesis} \mid \text{data}) = \frac{ \Pr(\text{data} \mid \text{hypothesis}) \; \Pr(\text{hypothesis})}{\Pr(\text{data})}\]

\begin{blackbox}
The \emph{hypothesis} is a working assumption about which you want to learn using \emph{data}. In capture--recapture analyses, the hypothesis might be a parameter like detection probability, or regression parameters in a relationship between survival probability and a covariate (see Chapter \ref{survival}). Bayes' theorem tells us how to obtain the probability of a hypothesis given the data we have.

\end{blackbox}

This is great because think about it, this is exactly what the scientific method is! We'd like to know how plausible some hypothesis is based on some data we collected, and possibly compare several hypotheses among them. In that respect, the Bayesian reasoning matches the scientific reasoning, which probably explains why the Bayesian framework is so natural for doing and understanding statistics.

You might ask then, why is Bayesian statistics not the default in statistics? Until recently, there were practical problems to implement Bayes' theorem. Recent advances in computational power coupled with the development of new algorithms have led to a great increase in the application of Bayesian methods within the last three decades.

\section{What is the Bayesian approach?}\label{what-is-the-bayesian-approach}

Typical statistical problems involve estimating a parameter (or several parameters) \(\theta\) with available data. To do so, you might be more used to the frequentist rather than the Bayesian method. The frequentist approach, and in particular maximum likelihood estimation (MLE), assumes that the parameters are fixed, and have unknown values to be estimated. Therefore classical estimates are generally point estimates of the parameters of interest. In contrast, the Bayesian approach assumes that the parameters are not fixed, and have some unknown distribution. A probability distribution is a mathematical expression that gives the probability for a random variable to take particular values. It may be either discrete (e.g., the Bernoulli, Binomial or Poisson distribution) or continuous (e.g., the Gaussian distribution also known as the normal distribution).

The Bayesian approach is based upon the idea that you, as an experimenter, begin with some prior beliefs about the system. Then you collect data and update your prior beliefs on the basis of observations. These observations might arise from field work, lab work or from expertise of your esteemed colleagues. This updating process is based upon Bayes' theorem. Loosely, let's say \(A = \theta\) and \(B = \text{data}\), then Bayes' theorem gives you a way to estimate parameter \(\theta\) given the data you have:

\[{\color{red}{\Pr(\theta \mid \text{data})}} = \frac{\color{blue}{\Pr(\text{data} \mid \theta)} \times \color{green}{\Pr(\theta)}}{\color{orange}{\Pr(\text{data})}}.\]
Let's spend some time going through each quantity in this formula.

On the left-hand side is the \(\color{red}{\text{posterior distribution}}\). It represents what you know after having seen the data. This is the basis for inference and clearly what you're after, a distribution, possibly multivariate if you have more than one parameter.

On the right-hand side, there is the \(\color{blue}{\text{likelihood}}\). This quantity is the same as in the MLE approach. Yes, the Bayesian and frequentist approaches have the same likelihood at their core, which mostly explains why results often do not differ much. The likelihood captures the information you have in your data, given a model parameterized with \(\theta\).

Then we have the \(\color{green}{\text{prior distribution}}\). This quantity represents what you know before seeing the data. This is the source of much discussion about the Bayesian approach. It may be vague if you don't know anything about \(\theta\). Usually however, you never start from scratch, and you'd like your prior to reflect the information you have (see Section \ref{elicitprior} for how to accomplish that).

Last, we have \(\color{orange}{\Pr(\text{data})}\) which is sometimes called the average likelihood because it is obtained by integrating the likelihood with respect to the prior \(\color{orange}{\Pr(\text{data}) = \int{L(\text{data} \mid \theta)\Pr(\theta) d\theta}}\) so that the posterior is standardized, that is it integrates to one for the posterior to be a distribution. The average likelihood is an integral with dimension the number of parameters \(\theta\) you need to estimate. This quantity is difficult, if not impossible, to calculate in general. This is one of the reasons why the Bayesian method wasn't used until recently, and why we need algorithms to estimate posterior distributions as I illustrate in the next section.

\section{Approximating posteriors via numerical integration}\label{numerical-approx}

Let's take an example to illustrate Bayes' theorem. Say we capture, mark and release \(n = 57\) animals at the beginning of a winter, out of which we recapture \(y = 19\) animals alive (we used a similar example in \citet{king_bayesian_2009}). We'd like to estimate winter survival \(\theta\). The data are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{19} \CommentTok{\# nb of success}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{57} \CommentTok{\# nb of attempts}
\end{Highlighting}
\end{Shaded}

We build our model first. Assuming all animals are independent of each other and have the same survival probability, then \(y\) the number of alive animals at the end of the winter is a binomial distribution with \(n\) trials and \(\theta\) the probability of success:

\begin{align*}
y &\sim \text{Binomial}(n, \theta) &\text{[likelihood]}
\end{align*}

Note that I follow \citet{mcelreathbook} and use labels on the right to help remember what each line is about. This likelihood can be visualised in \texttt{R}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{) }\CommentTok{\# grid of values for survival}
\NormalTok{likelihood }\OtherTok{\textless{}{-}} \FunctionTok{dbinom}\NormalTok{(y, n, grid) }\CommentTok{\# compute binomial likelihood}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{survival =}\NormalTok{ grid, }\AttributeTok{likelihood =}\NormalTok{ likelihood) }
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival, }\AttributeTok{y =}\NormalTok{ likelihood) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-29-1.pdf}}

This is the binomial likelihood with \(n = 57\) released animals and \(y = 19\) survivors after winter. The value of survival (on the x-axis) that corresponds to the maximum of the likelihood function (on the y-axis) is the MLE, or the proportion of success in this example, close to 0.33.

Besides the likelihood, priors are another component of the model in the Bayesian approach. For a parameter that is a probability, the one thing we know is that the prior should be a continuous random variable that lies between 0 and 1. To reflect that, we often go for the uniform distribution \(U(0,1)\) to imply \emph{vague} priors. Here vague means that survival has, before we see the data, the same probability of falling between 0.1 and 0.2 and between 0.8 and 0.9, for example.

\begin{align*}
\theta &\sim \text{Uniform}(0, 1) &\text{[prior for }\theta \text{]}
\end{align*}

Now we apply Bayes' theorem. We write a \texttt{R} function that computes the product of the likelihood times the prior, or the numerator in Bayes' theorem: \(\Pr(\text{data} \mid \theta) \times \Pr(\theta)\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{numerator }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(theta) }\FunctionTok{dbinom}\NormalTok{(y, n, theta) }\SpecialCharTok{*} \FunctionTok{dunif}\NormalTok{(theta, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We write another function that calculates the denominator, the average likelihood: \(\Pr(\text{data}) = \int{L(\theta \mid \text{data}) \Pr(\theta) d\theta}\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{denominator }\OtherTok{\textless{}{-}} \FunctionTok{integrate}\NormalTok{(numerator,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{$}\NormalTok{value}
\end{Highlighting}
\end{Shaded}

We use the \texttt{R} function \texttt{integrate} to calculate the integral in the denominator, which implements quadrature techniques to divide in little squares the area underneath the curve delimited by the function to integrate (here the numerator), and count them.

Then we get a numerical approximation of the posterior of winter survival by applying Bayes' theorem:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.01}\NormalTok{) }\CommentTok{\# grid of values for theta}
\NormalTok{numerical\_posterior }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{survival =}\NormalTok{ grid, }
                                  \AttributeTok{posterior =} \FunctionTok{numerator}\NormalTok{(grid)}\SpecialCharTok{/}\NormalTok{denominator) }\CommentTok{\# Bayes\textquotesingle{} theorem}
\NormalTok{numerical\_posterior }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival, }\AttributeTok{y =}\NormalTok{ posterior) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{1.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-33-1.pdf}}

How good is our numerical approximation of survival posterior distribution? Ideally, we would want to compare the approximation to the true posterior distribution. Although a closed-form expression for the posterior distribution is in general intractable, when you combine a binomial likelihood together with a beta distribution as a prior, then the posterior distribution is also a beta distribution, which makes it amenable to all sorts of exact calculations. We say that the beta distribution is the conjugate prior distribution for the binomial distribution. The beta distribution is continuous between 0 and 1, and extends the uniform distribution to situations where not all outcomes are equally likely. It has two parameters \(a\) and \(b\) that control its shape (Figure \ref{fig:betadistribution}).



\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/betadistribution-1.pdf}}
\caption{\label{fig:betadistribution}The distribution beta(\(a\),\(b\)) for different values of \(a\) and \(b\). Note that for \(a = b = 1\), we get the uniform distribution between 0 and 1 in the top left panel. When \(a\) and \(b\) are equal, the distribution is symmetric, and the bigger \(a\) and \(b\), the more peaked the distribution around the mean (the smaller the variance). The expectation (or mean) of a beta(\(a\),\(b\)) is \(\displaystyle{\frac{a}{a + b}}\).}
\end{figure}

If the likelihood of the data \(y\) is binomial with \(n\) trials and probability of success \(\theta\), and the prior is a beta distribution with parameters \(a\) and \(b\), then the posterior is a beta distribution with parameters \(a + y\) and \(b + n - y\). In our example, we have \(n = 57\) trials and \(y = 19\) animals that survived and a uniform prior between 0 and 1 or a beta distribution with parameters \(a = b = 1\), therefore survival has a beta posterior distribution with parameters 20 and 39. Let's superimpose the exact posterior and the numerical approximation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{explicit\_posterior }\OtherTok{\textless{}{-}} \FunctionTok{dbeta}\NormalTok{(grid, y }\SpecialCharTok{+}\NormalTok{ a, n }\SpecialCharTok{{-}}\NormalTok{ y }\SpecialCharTok{+}\NormalTok{ b)}
\NormalTok{dfexpposterior }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{survival =}\NormalTok{ grid, }\AttributeTok{explicit\_posterior =}\NormalTok{ explicit\_posterior)}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ numerical\_posterior, }
            \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival, }\AttributeTok{y =}\NormalTok{ posterior), }
            \AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }
            \AttributeTok{col =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Royal1[}\DecValTok{2}\NormalTok{],}
            \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dfexpposterior, }
            \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival, }\AttributeTok{y =}\NormalTok{ explicit\_posterior),}
            \AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }
            \AttributeTok{col =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Royal1[}\DecValTok{3}\NormalTok{], }
            \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-34-1.pdf}}

Clearly, the exact (dashed line) vs.~numerical approximation (continuous line) of winter survival posterior distribution are indistinguishable, suggesting that the numerical approximation is more than fine.

In our example, we have a single parameter to estimate, winter survival. This means dealing with a one-dimensional integral in the denominator which is pretty easy with quadrature techniques and the \texttt{R} function \texttt{integrate()}. Now what if we had multiple parameters? For example, imagine you'd like to fit a capture-recapture model with detection probability \(p\) and regression parameters \(\alpha\) and \(\beta\) for the intercept and slope of a relationship between survival probability and a covariate, then Bayes' theorem gives you the posterior distribution of all three parameters together:

\[ \Pr(\alpha, \beta, p \mid \text{data}) = \frac{ \Pr(\text{data} \mid \alpha, \beta, p) \times \Pr(\alpha, \beta, p)}{\iiint \, \Pr(\text{data} \mid \alpha, \beta, p) \Pr(\alpha, \beta, p) d\alpha d\beta dp} \]
There are two computational challenges with this formula. First, do we really wish to calculate a three-dimensional integral? The answer is no, one-dimensional and two-dimensional integrals are so much further we can go with standard methods. Second, we're more interested in a posterior distribution for each parameter separately than the joint posterior distribution. The so-called marginal distribution of \(p\) for example is obtained by integrating over all the other parameters -- a two-dimensional integral in this example. Now imagine with tens or hundreds of parameters to estimate, these integrals become highly multi-dimensional and simply intractable. In the next section, I introduce powerful simulation methods to circumvent this issue.

\section{Markov chain Monte Carlo (MCMC)}\label{markov-chain-monte-carlo-mcmc}

In the early 1990s, statisticians rediscovered work from the 1950's in physics. In a famous paper that would lay the fundations of modern Bayesian statistics (Figure \ref{fig:mcmcpaper}), the authors use simulations to approximate posterior distributions with some precision by drawing large samples. This is a neat trick to avoid explicit calculation of the multi-dimensional integrals we struggle with when using Bayes' theorem.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/metropolis} 

}

\caption{MCMC article cover. Source: The Journal of Chemical Physics -- https://aip.scitation.org/doi/10.1063/1.1699114}\label{fig:mcmcpaper}
\end{figure}

These simulation algorithms are called Markov chain Monte Carlo (MCMC), and they definitely gave a boost to Bayesian statistics. There are two parts in MCMC, Markov chain and Monte Carlo, let's try and make sense of these terms.

\subsection{Monte Carlo integration}\label{monte-carlo-integration}

What does Monte Carlo stand for? Monte Carlo integration is a simulation technique to calculate integrals of any function \(f\) of random variable \(X\) with distribution \(\Pr(X)\) say \(\int f(X) \Pr(X)dX\). You draw values \(X_1,\ldots,X_k\) from \(\Pr(X)\) the distribution of \(X\), apply function \(f\) to these values, then calculate the mean of these new values \(\displaystyle{\frac{1}{k}}\sum_{i=1}^k{f(X_i)}\) to approximate the integral. How is Monte Carlo integration used in a Bayesian context? The posterior distribution contains all the information we need about the parameter to be estimated. When dealing with many parameters however, you may want to summarise posterior results by calculating numerical summaries. The simplest numerical summary is the mean of the posterior distribution, \(E(\theta) = \int \theta \Pr(\theta|\text{data})\), where \(X\) is \(\theta\) now and \(f\) is the identity function. Posterior mean can be calculated with Monte Carlo integration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_from\_posterior }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{39}\NormalTok{) }\CommentTok{\# draw 1000 values from posterior survival beta(20,39)}
\FunctionTok{mean}\NormalTok{(sample\_from\_posterior) }\CommentTok{\# compute mean with Monte Carlo integration}
\DocumentationTok{\#\# [1] 0.3421}
\end{Highlighting}
\end{Shaded}

You may check that the mean we have just calculated matches closely the expectation of a beta distribution:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{20}\SpecialCharTok{/}\NormalTok{(}\DecValTok{20}\SpecialCharTok{+}\DecValTok{39}\NormalTok{) }\CommentTok{\# expectation of beta(20,39)}
\DocumentationTok{\#\# [1] 0.339}
\end{Highlighting}
\end{Shaded}

Another useful numerical summary is the credible interval within which our parameter falls with some probability, usually 0.95 hence a 95\(\%\) credible interval. Finding the bounds of a credible interval requires calculating quantiles, which in turn involves integrals and the use of Monte Carlo integration. A 95\(\%\) credible interval for winter survival can be obtained in \texttt{R} with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(sample\_from\_posterior, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{, }\FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{))}
\DocumentationTok{\#\#   2.5\%  97.5\% }
\DocumentationTok{\#\# 0.2258 0.4671}
\end{Highlighting}
\end{Shaded}

\subsection{Markov chains}\label{markovmodelmcmc}

What is a Markov chain? A Markov chain is a random sequence of numbers, in which each number depends only on the previous number. An example is the weather in my home town in Southern France, Montpellier, in which a sunny day is most likely to be followed by another sunny day, say with probability 0.8, and a rainy day is rarely followed by another rainy day, say with probability 0.1. The dynamic of this Markov chain is captured by the transition matrix \(\mathbf{\Gamma}\):
\[
\begin{matrix}
& \\
\mathbf{\Gamma} = 
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    \text{sunny tomorrow} & \text{rainy tomorrow} \\ 
0.8 & 0.2 \\ 
0.9 & 0.1 \\ 
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    \text{sunny today} \\ \text{rainy today}
    \end{matrix}
\end{matrix}
\]
In rows the weather today, and in columns the weather tomorrow. The cells give the probability of a sunny or rainy day tomorrow, given the day is sunny or rainy today. Under certain conditions, a Markov chain will converge to a unique stationary distribution. In our weather example, let's run the Markov chain for 20 steps:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weather }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.9}\NormalTok{, }\FloatTok{0.1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{, }\AttributeTok{byrow =}\NormalTok{ T) }\CommentTok{\# transition matrix}
\NormalTok{steps }\OtherTok{\textless{}{-}} \DecValTok{20}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{steps)\{}
\NormalTok{  weather }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%*\%}\NormalTok{ weather }\CommentTok{\# matrix multiplication}
\NormalTok{\}}
\FunctionTok{round}\NormalTok{(weather, }\DecValTok{2}\NormalTok{) }\CommentTok{\# matrix product after 20 steps}
\DocumentationTok{\#\#      [,1] [,2]}
\DocumentationTok{\#\# [1,] 0.82 0.18}
\DocumentationTok{\#\# [2,] 0.82 0.18}
\end{Highlighting}
\end{Shaded}

Each row of the transition matrix converges to the same distribution \((0.82, 0.18)\) as the number of steps increases. Convergence happens no matter which state you start in, and you always have probability 0.82 of the day being sunny and 0.18 of the day being rainy.

Back to MCMC, the core idea is that you can build a Markov chain with a given stationary distribution set to be the desired posterior distribution.

\begin{blackbox}
Putting Monte Carlo and Markov chains together, MCMC allows us to generate a sample of values (Markov chain) whose distribution converges to the posterior distribution, and we can use this sample of values to calculate any posterior summaries (Monte Carlo), such as posterior means and credible intervals.

\end{blackbox}

\subsection{Metropolis algorithm}\label{metropolis-algorithm}

There are several ways of constructing Markov chains for Bayesian inference. You might have heard about the Metropolis-Hastings or the Gibbs sampler. Have a look to \url{https://chi-feng.github.io/mcmc-demo/} for an interactive gallery of MCMC algorithms. Here I illustrate the Metropolis algorithm and how to implement it in practice.

Let's go back to our example on animal survival estimation. We illustrate sampling from survival posterior distribution. We write functions for likelihood, prior and posterior:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 19 animals recaptured alive out of 57 captured, marked and released}
\NormalTok{survived }\OtherTok{\textless{}{-}} \DecValTok{19}
\NormalTok{released }\OtherTok{\textless{}{-}} \DecValTok{57}

\CommentTok{\# binomial log{-}likelihood function}
\NormalTok{loglikelihood }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, p)\{}
  \FunctionTok{dbinom}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{size =}\NormalTok{ released, }\AttributeTok{prob =}\NormalTok{ p, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# uniform prior density}
\NormalTok{logprior }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p)\{}
  \FunctionTok{dunif}\NormalTok{(}\AttributeTok{x =}\NormalTok{ p, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{, }\AttributeTok{log =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{\}}

\CommentTok{\# posterior density function (log scale)}
\NormalTok{posterior }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, p)\{}
  \FunctionTok{loglikelihood}\NormalTok{(x, p) }\SpecialCharTok{+} \FunctionTok{logprior}\NormalTok{(p) }\CommentTok{\# {-} log(Pr(data))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The Metropolis algorithm works as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We pick a value of the parameter to be estimated. This is where we start our Markov chain -- this is a \emph{starting} value, or a starting location.
\item
  To decide where to go next, we propose to move away from the current value of the parameter -- this is a \emph{candidate} value. To do so, we add to the current value some random value from e.g.~a normal distribution with some variance -- this is a \emph{proposal} distribution. The Metropolis algorithm is a particular case of the Metropolis-Hastings algorithm with symmetric proposals.
\item
  We compute the ratio of the probabilities at the candidate and current locations \(R=\displaystyle{\frac{{\Pr(\text{candidate}|\text{data})}}{{\Pr(\text{current}|\text{data})}}}\). This is where the magic of MCMC happens, in that \(\Pr(\text{data})\), the denominator in the Bayes' theorem, appears in both the numerator and the denominator in \(R\) therefore cancels out and does not need to be calculated.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  If the posterior at the candidate location \(\Pr(\text{candidate}|\text{data})\) is higher than at the current location \(\Pr(\text{current}|\text{data})\), in other words when the candidate value is more plausible than the current value, we definitely accept the candidate value. If not, then we accept the candidate value with probability \(R\) and reject with probability \(1-R\). For example, if the candidate value is ten times less plausible than the current value, then we accept with probability 0.1 and reject with probability 0.9. How does it work in practice? We use a continuous spinner that lands somewhere between 0 and 1 -- call the random spin \(X\). If \(X\) is smaller than \(R\), we move to the candidate location, otherwise we remain at the current location. We do not want to accept or reject too often. In practice, the Metropolis algorithm should have an acceptance probability between 0.2 and 0.4, which can be achieved by \emph{tuning} the variance of the normal proposal distribution.
\item
  We repeat 2-4 a number of times -- or \emph{steps}.
\end{enumerate}

Enough of the theory, let's implement the Metropolis algorithm in \texttt{R}. Let's start by setting the scene:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{steps }\OtherTok{\textless{}{-}} \DecValTok{100} \CommentTok{\# number of steps}
\NormalTok{theta.post }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, steps) }\CommentTok{\# vector to store samples}
\NormalTok{accept }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, steps) }\CommentTok{\# keep track of accept/reject}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{) }\CommentTok{\# for reproducibility}
\end{Highlighting}
\end{Shaded}

Now follow the 5 steps we've just described. First, we pick a starting value, and store it (step 1):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inits }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\NormalTok{theta.post[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ inits}
\NormalTok{accept[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

Then, we need a function to propose a candidate value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{move }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{away =} \DecValTok{1}\NormalTok{)\{ }\CommentTok{\# by default, standard deviation of the proposal distribution is 1}
\NormalTok{  logitx }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(x }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x)) }\CommentTok{\# apply logit transform ({-}infinity,+infinity)}
\NormalTok{  logit\_candidate }\OtherTok{\textless{}{-}}\NormalTok{ logitx }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, away) }\CommentTok{\# add a value taken from N(0,sd=away) to current value}
\NormalTok{  candidate }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(logit\_candidate) }\CommentTok{\# back{-}transform (0,1)}
  \FunctionTok{return}\NormalTok{(candidate)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We add a value taken from a normal distribution with mean zero and standard deviation we call \emph{away}. We work on the logit scale to make sure the candidate value for survival lies between 0 and 1.

Now we're ready for steps 2, 3 and 4. We write a loop to take care of step 5. We start at initial value 0.5 and run the algorithm for 100 steps or iterations:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{steps)\{ }\CommentTok{\# repeat steps 2{-}4 (step 5)}
  
  \CommentTok{\# propose candidate value for survival (step 2)}
\NormalTok{  theta\_star }\OtherTok{\textless{}{-}} \FunctionTok{move}\NormalTok{(theta.post[t}\DecValTok{{-}1}\NormalTok{])}
  
  \CommentTok{\# calculate ratio R (step 3)}
\NormalTok{  pstar }\OtherTok{\textless{}{-}} \FunctionTok{posterior}\NormalTok{(survived, }\AttributeTok{p =}\NormalTok{ theta\_star)  }
\NormalTok{  pprev }\OtherTok{\textless{}{-}} \FunctionTok{posterior}\NormalTok{(survived, }\AttributeTok{p =}\NormalTok{ theta.post[t}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{  logR }\OtherTok{\textless{}{-}}\NormalTok{ pstar }\SpecialCharTok{{-}}\NormalTok{ pprev }\CommentTok{\# likelihood and prior are on the log scale}
\NormalTok{  R }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(logR)}
  
  \CommentTok{\# accept candidate value or keep current value (step 4)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# spin continuous spinner}
  \ControlFlowTok{if}\NormalTok{ (X }\SpecialCharTok{\textless{}}\NormalTok{ R)\{}
\NormalTok{    theta.post[t] }\OtherTok{\textless{}{-}}\NormalTok{ theta\_star }\CommentTok{\# accept candidate value}
\NormalTok{    accept[t] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# accept}
\NormalTok{  \}}
  \ControlFlowTok{else}\NormalTok{\{}
\NormalTok{    theta.post[t] }\OtherTok{\textless{}{-}}\NormalTok{ theta.post[t}\DecValTok{{-}1}\NormalTok{] }\CommentTok{\# keep current value}
\NormalTok{    accept[t] }\OtherTok{\textless{}{-}} \DecValTok{0} \CommentTok{\# reject}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We get the following values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(theta.post) }\CommentTok{\# first values}
\DocumentationTok{\#\# [1] 0.5000 0.2302 0.2906 0.2906 0.2980 0.2980}
\FunctionTok{tail}\NormalTok{(theta.post) }\CommentTok{\# last values}
\DocumentationTok{\#\# [1] 0.2622 0.2622 0.2622 0.3727 0.3232 0.3862}
\end{Highlighting}
\end{Shaded}

Visually, you may look at the chain:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{steps, }\AttributeTok{y =}\NormalTok{ theta.post)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{color =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Zissou1[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{y =} \StringTok{"samples"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/chain-1.pdf}}

In this visualisation, remember that our Markov chain starts at value 0.5. The steps or iterations are on the x-axis, and samples on the y-axis. This graphical representation is called a trace plot.

The acceptance probability is the average number of times we accepted a candidated value, which is 0.44 and almost satisfying.

To make our life easier and avoid repeating the same lines of code again and again, let's make a function out of the code we have written so far:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{metropolis }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{steps =} \DecValTok{100}\NormalTok{, }\AttributeTok{inits =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{away =} \DecValTok{1}\NormalTok{)\{}
  
  \CommentTok{\# pre{-}alloc memory}
\NormalTok{  theta.post }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, steps)}
  
  \CommentTok{\# start}
\NormalTok{  theta.post[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ inits}
  
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{steps)\{}
    
    \CommentTok{\# propose candidate value for prob of success}
\NormalTok{    theta\_star }\OtherTok{\textless{}{-}} \FunctionTok{move}\NormalTok{(theta.post[t}\DecValTok{{-}1}\NormalTok{], }\AttributeTok{away =}\NormalTok{ away)}
    
    \CommentTok{\# calculate ratio R}
\NormalTok{    pstar }\OtherTok{\textless{}{-}} \FunctionTok{posterior}\NormalTok{(survived, }\AttributeTok{p =}\NormalTok{ theta\_star)  }
\NormalTok{    pprev }\OtherTok{\textless{}{-}} \FunctionTok{posterior}\NormalTok{(survived, }\AttributeTok{p =}\NormalTok{ theta.post[t}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{    logR }\OtherTok{\textless{}{-}}\NormalTok{ pstar }\SpecialCharTok{{-}}\NormalTok{ pprev}
\NormalTok{    R }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(logR)}
    
    \CommentTok{\# accept candidate value or keep current value (step 4)}
\NormalTok{    X }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# spin continuous spinner}
    \ControlFlowTok{if}\NormalTok{ (X }\SpecialCharTok{\textless{}}\NormalTok{ R)\{}
\NormalTok{      theta.post[t] }\OtherTok{\textless{}{-}}\NormalTok{ theta\_star}
\NormalTok{    \}}
    \ControlFlowTok{else}\NormalTok{\{}
\NormalTok{      theta.post[t] }\OtherTok{\textless{}{-}}\NormalTok{ theta.post[t}\DecValTok{{-}1}\NormalTok{]}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  theta.post}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Can we run another chain and start at initial value 0.2 this time? Yes, just go through the same algorithm again, and visualise the results with trace plot of survival for two chains starting at 0.2 (yellow) and 0.5 (blue) run for 100 steps:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta.post2 }\OtherTok{\textless{}{-}} \FunctionTok{metropolis}\NormalTok{(}\AttributeTok{steps =} \DecValTok{100}\NormalTok{, }\AttributeTok{inits =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{df2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{steps, }\AttributeTok{y =}\NormalTok{ theta.post2)}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{color =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Zissou1[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ df2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{size =} \FloatTok{1.5}\NormalTok{, }\AttributeTok{color =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Zissou1[}\DecValTok{3}\NormalTok{]) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{y =} \StringTok{"values from posterior distribution"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/twochains-1.pdf}}

Notice that we do not get the exact same results because the algorithm is stochastic. The question is to know whether we have reached the stationary distribution. Let's increase the number of steps, start at 0.5 and run a chain with 5000 iterations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{steps }\OtherTok{\textless{}{-}} \DecValTok{5000}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{theta.post }\OtherTok{\textless{}{-}} \FunctionTok{metropolis}\NormalTok{(}\AttributeTok{steps =}\NormalTok{ steps, }\AttributeTok{inits =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{steps, }\AttributeTok{y =}\NormalTok{ theta.post)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{color =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Zissou1[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{y =} \StringTok{"values from posterior distribution"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(theta.post), }\AttributeTok{linetype =} \StringTok{"posterior mean"}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_linetype\_manual}\NormalTok{(}\AttributeTok{name =} \StringTok{""}\NormalTok{, }\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/longchain-1.pdf}}

This is what we're after, a trace plot that looks like a beautiful lawn, see Section \ref{convergence-diag}.

Once the stationary distribution is reached, you may regard the realisations of the Markov chain as a sample from the posterior distribution, and obtain numerical summaries. In the next section, we consider several important implementation issues.

\section{Assessing convergence}\label{convergence-diag}

\begin{blackbox}
When implementing MCMC, we need to determine how long it takes for our Markov chain to converge to the target distribution, and the number of iterations we need after achieving convergence to get reasonable Monte Carlo estimates of numerical summaries (posterior means and credible intervals).

\end{blackbox}

\subsection{Burn-in}\label{burn-in}

In practice, we discard observations from the start of the Markov chain and just use observations from the chain once it has converged. The initial observations that we discard are usually referred to as the \emph{burn-in}.

The simplest method to determine the length of the burn-in period is to look at trace plots. Going back to our example, let's have a look to a trace plot of a chain that starts at value 0.99.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up the scene}
\NormalTok{steps }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{theta.post }\OtherTok{\textless{}{-}} \FunctionTok{metropolis}\NormalTok{(}\AttributeTok{steps =}\NormalTok{ steps, }\AttributeTok{inits =} \FloatTok{0.99}\NormalTok{)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{steps, }\AttributeTok{y =}\NormalTok{ theta.post)}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{, }\AttributeTok{color =}\NormalTok{ wesanderson}\SpecialCharTok{::}\NormalTok{wes\_palettes}\SpecialCharTok{$}\NormalTok{Zissou1[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{y =} \StringTok{"survival"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_light}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{annotate}\NormalTok{(}\StringTok{"rect"}\NormalTok{, }
           \AttributeTok{xmin =} \DecValTok{0}\NormalTok{, }
           \AttributeTok{xmax =} \DecValTok{100}\NormalTok{, }
           \AttributeTok{ymin =} \FloatTok{0.1}\NormalTok{, }
           \AttributeTok{ymax =} \DecValTok{1}\NormalTok{, }
           \AttributeTok{alpha =}\NormalTok{ .}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/burnin-1.pdf}}

The chain starts at value 0.99 and rapidly stabilises, with values bouncing back and forth around 0.3 from the 100th iteration onwards. You may choose the shaded area as the burn-in, and discard the first 100th values.

We see from the trace plot below that we need at least 100 iterations to achieve convergence toward an average survival around 0.3. It is always better to be conservative when specifying the length of the burn-in period, and in this example, we would use 250 or even 500 iterations as a burn-in. The length of the burn-in period can be determined by performing preliminary MCMC short runs.

Inspecting the trace plot for a single run of the Markov chain is useful. However, we usually run the Markov chain several times, starting from different over-dispersed points, to check that all runs achieve the same stationary distribution. This approach is formalised by using the Brooks-Gelman-Rubin (BGR) statistic \(\hat{R}\) which measures the ratio of the total variability combining multiple chains (between-chain plus within-chain) to the within-chain variability. The BGR statistic asks whether there is a chain effect, and is very much alike the \(F\) test in an analysis of variance. Values below 1.1 indicate likely convergence.

\textbf{Should be below 1.01, see Vehtari et al.~2021.}

Back to our example, we run two Markov chains with starting values 0.2 and 0.8 using 100 up to 5000 iterations, and calculate the BGR statistic using half the number of iterations as the length of the burn-in (code not shown):

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/bgr-1.pdf}}

We get a value of the BGR statistic near 1 by up to 2000 iterations, which suggests that with 2000 iterations as a burn-in, there is no evidence of a lack of convergence.

It is important to bear in mind that a value near 1 for the BGR statistic is only a necessary \emph{but not sufficient} condition for convergence. In other words, this diagnostic cannot tell you for sure that the Markov chain has achieved convergence, only that it has not.

\subsection{Chain length}\label{chain-length}

How long of a chain is needed to produce reliable parameter estimates? To answer this question, you need to keep in mind that successive steps in a Markov chain are not independent -- this is usually referred to as \emph{autocorrelation}. Ideally, we would like to keep autocorrelation as low as possible. Here again, trace plots are useful to diagnose issues with autocorrelation. Let's get back to our survival example. The figure below shows trace plots for different values of the standard deviation (parameter \emph{away}) of the normal proposal distribution we use to propose a candidate value (Section \ref{metropolis-algorithm}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# inspired from https://bookdown.org/content/3686/markov{-}chain{-}monte{-}carlo.html}

\NormalTok{n\_steps }\OtherTok{\textless{}{-}} \DecValTok{10000}

\NormalTok{d }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{away =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{accepted\_traj =} \FunctionTok{map}\NormalTok{(away, metropolis, }\AttributeTok{steps =}\NormalTok{ n\_steps, }\AttributeTok{inits =} \FloatTok{0.1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest}\NormalTok{(accepted\_traj)}

\NormalTok{d }\OtherTok{\textless{}{-}}
\NormalTok{  d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{proposal\_sd =} \FunctionTok{str\_c}\NormalTok{(}\StringTok{"Proposal SD = "}\NormalTok{, away),}
         \AttributeTok{iter        =} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_steps, }\AttributeTok{times =} \DecValTok{3}\NormalTok{))}

\NormalTok{trace }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ accepted\_traj, }\AttributeTok{x =}\NormalTok{ iter)) }\SpecialCharTok{+}
  \FunctionTok{geom\_path}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{4}\NormalTok{, }\AttributeTok{color =} \StringTok{"steelblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{1}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{color =} \StringTok{"steelblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\StringTok{"survival"}\NormalTok{, }\AttributeTok{breaks =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{5} \SpecialCharTok{*} \FloatTok{0.1}\NormalTok{, }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.15}\NormalTok{, }\FloatTok{0.5}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{"iterations"}\NormalTok{, }
                     \AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(n\_steps}\SpecialCharTok{{-}}\NormalTok{n\_steps}\SpecialCharTok{*}\DecValTok{10}\SpecialCharTok{/}\DecValTok{100}\NormalTok{,n\_steps,}\AttributeTok{by =} \DecValTok{600}\NormalTok{), }
                     \AttributeTok{limits =} \FunctionTok{c}\NormalTok{(n\_steps}\SpecialCharTok{{-}}\NormalTok{n\_steps}\SpecialCharTok{*}\DecValTok{10}\SpecialCharTok{/}\DecValTok{100}\NormalTok{, n\_steps)) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{proposal\_sd, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_light}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{14}\NormalTok{)}

\NormalTok{trace}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-48-1.pdf}}

Small and big moves in the left and right panels provide high correlations between successive observations of the Markov chain, whereas a standard deviation of 1 in the center panel allows efficient exploration of the parameter space. The movement around the parameter space is referred to as \emph{mixing}. Mixing is bad when the chain makes small and big moves, and good otherwise.

In addition to trace plots, autocorrelation function (ACF) plots are a convenient way of displaying the strength of autocorrelation in a given sample values. ACF plots provide the autocorrelation between successively sampled values separated by an increasing number of iterations, or \emph{lag}. We obtain the autocorrelation function plots for different values of the standard deviation of the proposal distribution with the R \texttt{forecast::ggAcf()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(forecast)}
\NormalTok{plot1 }\OtherTok{\textless{}{-}} \FunctionTok{ggAcf}\NormalTok{(}\AttributeTok{x =}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 0.1"}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Proposal SD = 0.1"}\NormalTok{)}
\NormalTok{plot2 }\OtherTok{\textless{}{-}} \FunctionTok{ggAcf}\NormalTok{(}\AttributeTok{x =}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 1"}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Proposal SD = 1"}\NormalTok{)}
\NormalTok{plot3 }\OtherTok{\textless{}{-}} \FunctionTok{ggAcf}\NormalTok{(}\AttributeTok{x =}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 10"}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Proposal SD = 10"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{(plot1 }\SpecialCharTok{+}\NormalTok{ plot2 }\SpecialCharTok{+}\NormalTok{ plot3)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-49-1.pdf}}

In the left and right panels, autocorrelation is strong, decreases slowly with increasing lag and mixing is bad. In the center panel, autocorrelation is weak, decreases rapidly with increasing lag and mixing is good.

Autocorrelation is not necessarily a big issue. Strongly correlated observations just require large sample sizes and therefore longer simulations. But how many iterations exactly? The effective sample size (\texttt{n.eff}) measures chain length while taking into account chain autocorrelation. You should check the \texttt{n.eff} of every parameter of interest, and of any interesting parameter combinations. In general, we need \(\text{n.eff} \geq 1000\) independent steps to get reasonable Monte Carlo estimates of model parameters. In the animal survival example, \texttt{n.eff} can be calculated with the R \texttt{coda::effectiveSize()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neff1 }\OtherTok{\textless{}{-}}\NormalTok{ coda}\SpecialCharTok{::}\FunctionTok{effectiveSize}\NormalTok{(d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 0.1"}\NormalTok{])}
\NormalTok{neff2 }\OtherTok{\textless{}{-}}\NormalTok{ coda}\SpecialCharTok{::}\FunctionTok{effectiveSize}\NormalTok{(d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 1"}\NormalTok{])}
\NormalTok{neff3 }\OtherTok{\textless{}{-}}\NormalTok{ coda}\SpecialCharTok{::}\FunctionTok{effectiveSize}\NormalTok{(d}\SpecialCharTok{$}\NormalTok{accepted\_traj[d}\SpecialCharTok{$}\NormalTok{proposal\_sd}\SpecialCharTok{==}\StringTok{"Proposal SD = 10"}\NormalTok{])}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\StringTok{"Proposal SD"} \OtherTok{=} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{),}
             \StringTok{"n.eff"} \OtherTok{=} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(neff1, neff2, neff3)))}
\NormalTok{df}
\DocumentationTok{\#\# \# A tibble: 3 x 2}
\DocumentationTok{\#\#   \textasciigrave{}Proposal SD\textasciigrave{} n.eff}
\DocumentationTok{\#\#           \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\DocumentationTok{\#\# 1           0.1   224}
\DocumentationTok{\#\# 2           1    1934}
\DocumentationTok{\#\# 3          10     230}
\end{Highlighting}
\end{Shaded}

As expected, \texttt{n.eff} is less than the number of MCMC iterations because of autocorrelation. Only when the standard deviation of the proposal distribution is 1 is the mixing good and we get a satisfying effective sample size.

\textbf{Should be greater than 400. Vehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian BuÌˆrkner (June 2021). ``Rank-Normalization, Folding, and Localization: An Improved RË† for Assessing Convergence of MCMC (with Discussion)''. In: Bayesian Analysis 16.2, pp.~667--718.issn: 1936-0975, 1931-6690.doi:10.1214/20-BA1221.}

\subsection{What if you have issues of convergence?}\label{what-if-you-have-issues-of-convergence}

When diagnosing MCMC convergence, you will (very) often run into troubles. In this section you will find some helpful tips I hope.

When mixing is bad and effective sample size is small, you may just need to increase burn-in and/or sample more. Using more informative priors might also make Markov chains converge faster by helping your MCMC sampler (e.g.~the Metropolis algorithm) navigating more efficiently the parameter space. In the same spirit, picking better initial values for starting the chain does not harm. For doing that, a strategy consists in using estimates from a simpler model for which your MCMC chains do converge.

If convergence issues persist, often there is a problem with your model (also known as the folk theorem of statistical computing as stated by Andrew Gelman in 2008, see \url{https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/}). A bug in the code? A typo somewhere? A mistake in your maths? As often when coding is involved, the issue can be identified by removing complexities, and start with a simpler model until you find what the problem is.

A general advice is to see your model as a data generating tool in the first place, simulate data from it using some realistic values for the parameters, and try to recover these parameter values by fitting the model to the simulated data. Simulating from a model will help you understanding how it works, what it does not do, and the data you need to get reasonable parameter estimates.

\section{Summary}\label{summary}

\begin{itemize}
\item
  With the Bayes' theorem, you update your beliefs (prior) with new data (likelihood) to get posterior beliefs (posterior): posterior \(\propto\) likelihood \(\times\) prior.
\item
  The idea of Markov chain Monte Carlo (MCMC) is to simulate values from a Markov chain which has a stationary distribution equal to the posterior distribution you're after.
\item
  In practice, you run a Markov chain multiple times starting from over-dispersed initial values.
\item
  You discard iterations in an initial burn-in phase and achieve convergence when all chains reach the same regime.
\item
  From there, you run the chains long enough and proceed with calculating Monte Carlo estimates of numerical summaries (e.g.~posterior means and credible intervals) for parameters.
\end{itemize}

\section{Suggested reading}\label{suggested-reading}

\begin{itemize}
\item
  \citet{mccarthy2007} is an excellent introduction to Bayesian statistics for ecologists.
\item
  For deeper insights, I recommend \citet{gelmanhill2006} which analyse data using the frequentist and Bayesian approaches side-by-side. The book by \citet{mcelreathbook} is also an excellent read. The presentation of the Metropolis algorithm in Section \ref{metropolis-algorithm} was inspired by \citet{alberthu2019}. If you'd like to know more about Monte Carlo methods, the book \citet{robert2004montecarlo} is a must (see also its R counterpart \citet{robert2004montecarloinr}).
\item
  I also recommend \citet{gelman2020workflow} in which the authors offer a workflow for Bayesian analyses. They discuss model building, model comparison, model checking, model validation, model understanding and troubleshooting of computational problems.
\end{itemize}

\chapter{NIMBLE tutorial}\label{intronimble}

\section{Introduction}\label{introduction-2}

In this second chapter, you will get familiar with NIMBLE, an R package that implements up-to-date MCMC algorithms for fitting complex models. NIMBLE spares you from coding the MCMC algorithms by hand, and requires only the specification of a likelihood and priors for model parameters. Should you wish to dive deeper into the mechanics, NIMBLE also got you covered and allows you to write samples, use custom functions, etc. We will illustrate NIMBLE's main features with a simple example, but the ideas hold for more complex problems.

\section{What is NIMBLE?}\label{what-is-nimble}

NIMBLE stands for \textbf{N}umerical \textbf{I}nference for statistical \textbf{M}odels using \textbf{B}ayesian and \textbf{L}ikelihood \textbf{E}stimation. Briefly speaking, NIMBLE is an R package that implements for you MCMC algorithms to generate samples from the posterior distribution of model parameters. Freed from the burden of coding your own MCMC algorithms, you only have to specify a likelihood and priors to apply the Bayes theorem. To do so, NIMBLE makes this easy by using a syntax very similar to the R syntax, which should make your life easier. It is also a direct extension of the BUGS language is also used by other programs like WinBUGS, OpenBUGS, and JAGS.

So why use NIMBLE you may ask? The short answer is that NIMBLE is capable of so much more than just running MCMC algorithms! First, you will work from within R, but in the background NIMBLE will translate your code in C++ for (in general) faster computation. Second, NIMBLE extends the BUGS language for writing new functions and distributions of your own, or borrow those written by others. Third, NIMBLE gives you full control of the MCMC samplers, and you may pick other algorithms than the defaults. Fourth, NIMBLE comes with a library of numerical methods other than MCMC algorithms, including sequential Monte Carlo (for particle filtering), Monte Carlo Expectation Maximization (for maximum likelihood), Hamiltonian Monte Carlo (like in program Stan), and Laplace approximation (like in program TMB). Last but not least, the development team is friendly and helpful, and based on users' feedbacks, NIMBLE folks work constantly at improving the package capabilities. The NIMBLE users google group is an open and inclusive space where everyone can receive help from the community: \url{https://groups.google.com/g/nimble-users}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/nimble-icon} 

}

\caption{Logo of the NIMBLE R package designed by Luke Larson.}\label{fig:nimblelogo}
\end{figure}

\section{Getting started}\label{start-nimble}

\begin{blackbox}
To run NIMBLE, you will need to:\\
1. Build a model consisting of a likelihood and priors.\\
2. Read in some data.\\
3. Specify parameters you want to make inference about.\\
4. Pick initial values for parameters to be estimated (for each chain).\\
5. Provide MCMC details namely the number of chains, the length of the burn-in period and the number of iterations following burn-in.

\end{blackbox}

First things first, let's not forget to load the \texttt{nimble} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nimble)}
\end{Highlighting}
\end{Shaded}

Note that before you can install \texttt{nimble} like any other R package, Windows users will need to install \texttt{Rtools}, and Mac users will need to install \texttt{Xcode}. More info and help trouble-shooting installation issues can be found here: \url{https://r-nimble.org/download}.

Now let's go back to our example on animal survival from the previous chapter. First step is to build our model by specifying the binomial likelihood and a uniform prior on survival probability \texttt{theta}. We use the \texttt{nimbleCode()} function and wrap code within curly brackets:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \CommentTok{\# derived quantity}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

You can check that the \texttt{model} R object contains your code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model}
\DocumentationTok{\#\# \{}
\DocumentationTok{\#\#     survived \textasciitilde{} dbinom(theta, released)}
\DocumentationTok{\#\#     theta \textasciitilde{} dunif(0, 1)}
\DocumentationTok{\#\#     lifespan \textless{}{-} {-}1/log(theta)}
\DocumentationTok{\#\# \}}
\end{Highlighting}
\end{Shaded}

In the code above, \texttt{survived} and \texttt{released} are known, only \texttt{theta} needs to be estimated. The line \texttt{survived\ \textasciitilde{}\ dbinom(theta,\ released)} states that the number of successes or animals that have survived over winter, \texttt{survived}, is distributed as (that's the \texttt{\textasciitilde{}}) a binomial with \texttt{released} trials and probability of success or survival \texttt{theta}. Then the line \texttt{theta\ \textasciitilde{}\ dunif(0,\ 1)} assigns a uniform distribution between 0 and 1 as a prior to the survival probability. This is all you need, a likelihood and priors for model parameters, NIMBLE knows the Bayes theorem. The last line \texttt{lifespan\ \textless{}-\ -\ 1/log(theta)} calculates a quantity derived from \texttt{theta}, which is the expected lifespan assuming constant survival. If you'd like to know more about the calculation of life expectancy, check out \citet{cook1967expectancy}.

A few comments:

\begin{itemize}
\item
  The most common distributions are readily available in NIMBLE. Among others, we will use later in the book \texttt{dbeta}, \texttt{dmultinom} and \texttt{dnorm}. If you cannot find what you need in NIMBLE, you can write your own distributions as illustrated in Section \ref{functions-in-nimble}.
\item
  It does not matter in what order you write each line of code, NIMBLE uses what is called a declarative language for building models. In brief, you write code that tells NIMBLE what you want to achieve, and not how to get there. In contrast, an imperative language requires that you write what you want your program to do step by step.
\item
  You can think of models in NIMBLE as graphs as in Figure \ref{fig:dag-survival}. A graph is made of relations (or edges) that can be of two types. A stochastic relation is signaled by a \texttt{\textasciitilde{}} sign and defines a random variable in the model, such as \texttt{survived} or \texttt{theta}. A deterministic relation is signaled by a \texttt{\textless{}-} sign, like \texttt{lifespan}. Relations define nodes on the left - the children - in terms of other nodes on the right - the parents, and relations are directed arrows from parents to children. Such graphs are called directed acyclic graph or DAG.
  \pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/dag-survival-1.pdf}}
\end{itemize}

Second step in our workflow is to read in some data. We use a list in which each component corresponds to a known quantity in the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{released =} \DecValTok{57}\NormalTok{, }\AttributeTok{survived =} \DecValTok{19}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can proceed with data passed this way, but you should know a little more about how NIMBLE sees data. NIMBLE distinguishes data and constants. Constants are values that do not change, e.g.~vectors of known index values or the indices used to define for loops. Data are values that you might want to change, basically anything that only appears on the left of a \texttt{\textasciitilde{}}. Declaring relevant values as constants is better for computational efficiency, but it is easy to forget, and fortunately NIMBLE will by itself distinguish data and constants. It will suggest you to move some data into constants to improve efficiency. I will not use the distinction between data and constants in this chapter, but in the next chapters it will become important.

Third step is to tell NIMBLE which nodes in your model you would like to keep track of, in other words the quantities you'd like to do inference about. In our model we want survival \texttt{theta} and \texttt{lifespan}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"theta"}\NormalTok{, }\StringTok{"lifespan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In general you have many quantities in your model, including some of little interest that are not worth monitoring, and having full control on verbosity will prove handy.

Fourth step is to specify initial values for all model parameters. As a bare minimum, you need initial values for all nodes that only appear on the left side of a \texttt{\textasciitilde{}} in your code and are not given as data. To make sure that the MCMC algorithm explores the posterior distribution, we start different chains with different parameter values. You can specify initial values for each chain (here we specify for three chains) in a list and put them in yet another list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{init1 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.1}\NormalTok{)}
\NormalTok{init2 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{init3 }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.9}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(init1, init2, init3)}
\NormalTok{initial.values}
\DocumentationTok{\#\# [[1]]}
\DocumentationTok{\#\# [[1]]$theta}
\DocumentationTok{\#\# [1] 0.1}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[2]]}
\DocumentationTok{\#\# [[2]]$theta}
\DocumentationTok{\#\# [1] 0.5}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# [[3]]}
\DocumentationTok{\#\# [[3]]$theta}
\DocumentationTok{\#\# [1] 0.9}
\end{Highlighting}
\end{Shaded}

Alternatively, you can write an R function that generates random initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{initial.values}\NormalTok{()}
\DocumentationTok{\#\# $theta}
\DocumentationTok{\#\# [1] 0.8356}
\end{Highlighting}
\end{Shaded}

If you are using a function to generate random initial values, it's always a good idea to set the seed in your code before you draw the initial values. For example like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.seed }\OtherTok{\textless{}{-}} \DecValTok{666}
\FunctionTok{set.seed}\NormalTok{(my.seed)}
\end{Highlighting}
\end{Shaded}

Setting the seed makes your code reproducible, which really helps if you need to trouble-shoot it later. Initialization problems are not uncommon when working with NIMBLE, and being able to reproduce the same initial values again is very useful for solving them.

Fifth and last step, you need to tell NIMBLE the number of chains to run, say \texttt{n.chain}, how long the burn-in period should be, say \texttt{n.burnin}, and the number of iterations following the burn-in period to be used for posterior inference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{3}
\end{Highlighting}
\end{Shaded}

In NIMBLE, you specify the total number of iterations, say \texttt{n.iter}, so that the number of posterior samples per chain is \texttt{n.iter\ -\ n.burnin}. NIMBLE also allows discarding samples after burn-in, a procedure known as thinning. Thinning is fixed to 1 by default in NIMBLE so that all simulations are used to summarise posterior distributions. \citet{link2012thinning} offer a discussion of the pros and cons of thinning.

We now have all the ingredients to run our model, that is to sample from the posterior distribution of model parameters using MCMC simulations. This is accomplished using function \texttt{nimbleMCMC()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\end{Highlighting}
\end{Shaded}

NIMBLE goes through several steps that we will explain in Section \ref{under-the-hood}. Function \texttt{nimbleMCMC()} takes other arguments that you might find useful. For example, one is \texttt{setSeed}. Just like with sampling initial values above, setting the seed within the MCMC call allows you to run the \textbf{same} chains (again), thus making your analyses reproducible and problems easier to debug (see Section \ref{tipreproducibility}). You can also get a summary of the outputs by specifying \texttt{summary\ =\ TRUE}. Conversely, if you would rather just get the MCMC samples back (in \texttt{coda\ mcmc} format) you can set \texttt{samplesAsCodaMCMC\ =\ TRUE}. Finally, you can suppress the progress bar if you find it too depressing when running long simulations with \texttt{progressBar\ =\ FALSE}. Check \texttt{?nimbleMCMC} for more details.

Now let's inspect what we have in \texttt{mcmc.output}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(mcmc.output)}
\DocumentationTok{\#\# List of 3}
\DocumentationTok{\#\#  $ chain1: num [1:4000, 1:2] 0.907 0.907 0.907 0.907 0.853 ...}
\DocumentationTok{\#\#   ..{-} attr(*, "dimnames")=List of 2}
\DocumentationTok{\#\#   .. ..$ : NULL}
\DocumentationTok{\#\#   .. ..$ : chr [1:2] "lifespan" "theta"}
\DocumentationTok{\#\#  $ chain2: num [1:4000, 1:2] 0.787 0.894 1.291 1.388 1.388 ...}
\DocumentationTok{\#\#   ..{-} attr(*, "dimnames")=List of 2}
\DocumentationTok{\#\#   .. ..$ : NULL}
\DocumentationTok{\#\#   .. ..$ : chr [1:2] "lifespan" "theta"}
\DocumentationTok{\#\#  $ chain3: num [1:4000, 1:2] 0.745 0.745 0.745 0.886 1.136 ...}
\DocumentationTok{\#\#   ..{-} attr(*, "dimnames")=List of 2}
\DocumentationTok{\#\#   .. ..$ : NULL}
\DocumentationTok{\#\#   .. ..$ : chr [1:2] "lifespan" "theta"}
\end{Highlighting}
\end{Shaded}

The R object \texttt{mcmc.output} is a list with three components, one for each MCMC chain. Let's have a look to \texttt{chain1} for example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1)}
\DocumentationTok{\#\# [1] 4000    2}
\FunctionTok{head}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1)}
\DocumentationTok{\#\#      lifespan  theta}
\DocumentationTok{\#\# [1,]   0.9069 0.3320}
\DocumentationTok{\#\# [2,]   0.9069 0.3320}
\DocumentationTok{\#\# [3,]   0.9069 0.3320}
\DocumentationTok{\#\# [4,]   0.9069 0.3320}
\DocumentationTok{\#\# [5,]   0.8526 0.3095}
\DocumentationTok{\#\# [6,]   0.7987 0.2859}
\end{Highlighting}
\end{Shaded}

Each component of the list is a matrix. In rows, you have 4000 samples from the posterior distribution of \texttt{theta}, which corresponds to \texttt{n.iter\ -\ n.burnin} iterations. In columns, you have the quantities we monitor, \texttt{theta} and \texttt{lifespan}. From there, you can compute the posterior mean of \texttt{theta}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{])}
\DocumentationTok{\#\# [1] 0.3407}
\end{Highlighting}
\end{Shaded}

You can also obtain the 95\% credible interval for \texttt{theta}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{], }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\DocumentationTok{\#\#   2.5\%  97.5\% }
\DocumentationTok{\#\# 0.2219 0.4620}
\end{Highlighting}
\end{Shaded}

Let's visualise the posterior distribution of \texttt{theta} with a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{"theta"}\NormalTok{] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value), }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"survival probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-65-1.pdf}}

There are less painful ways of doing posterior inference. In this book, I will use the R package \texttt{MCMCvis} to summarise and visualize MCMC outputs, but there are other perfectly valid options out there like \texttt{ggmcmc}, \texttt{bayesplot} and \texttt{basicMCMCplots}.

Let's load the package \texttt{MCMCvis}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MCMCvis)}
\end{Highlighting}
\end{Shaded}

To get the most common numerical summaries, the function \texttt{MCMCsummary()} does the job:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#          mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# lifespan 0.94 0.17 0.66 0.92  1.32    1  2513}
\DocumentationTok{\#\# theta    0.34 0.06 0.22 0.34  0.47    1  2533}
\end{Highlighting}
\end{Shaded}

You can use a caterpillar plot to visualise the posterior distributions of \texttt{theta} with \texttt{MCMCplot()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCplot}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }
         \AttributeTok{params =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-68-1.pdf}}

The point represents the posterior median, the thick line is the 50\% credible interval and the thin line the 95\% credible interval.

Visualization of a MCMC chain itself, i.e.~the values of posterior samples plotted against iteration number, is called a trace. The trace and posterior density of theta can be obtained with \texttt{MCMCtrace()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCtrace}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output,}
          \AttributeTok{pdf =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# no export to PDF}
          \AttributeTok{ind =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# separate density lines per chain}
          \AttributeTok{params =} \StringTok{"theta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-69-1.pdf}}
We use the trace and density plots for assessing convergence and get an idea of whether there may be any estimation issues (see Section \ref{convergence-diag}).

You can also add the diagnostics of convergence we discussed in the previous chapter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCtrace}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output,}
          \AttributeTok{pdf =} \ConstantTok{FALSE}\NormalTok{,}
          \AttributeTok{ind =} \ConstantTok{TRUE}\NormalTok{,}
          \AttributeTok{Rhat =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# add Rhat}
          \AttributeTok{n.eff =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# add eff sample size}
          \AttributeTok{params =} \StringTok{"theta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-70-1.pdf}}

We calculated lifespan directly in our model with \texttt{lifespan\ \textless{}-\ -1/log(theta)}. But you can also calculate this quantity from outside NIMBLE. This is a nice by-product of using MCMC simulations: You can obtain the posterior distribution of any quantity that is a function of your model parameters by applying this function to samples from the posterior distribution of these parameters. Especially when working with big models/data, it is recommended to keep any calculations that can be made ``post-hoc'' using the posterior samples outside of NIMBLE as this lessens memory load. In our example, all you need is samples from the posterior distribution of \texttt{theta}, which we pool between the three chains with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta\_samples }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{], }
\NormalTok{                   mcmc.output}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{],}
\NormalTok{                   mcmc.output}\SpecialCharTok{$}\NormalTok{chain3[,}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

To get samples from the posterior distribution of lifespan, we apply the function to calculate lifespan to the samples from the posterior distribution of survival:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifespan }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta\_samples)}
\end{Highlighting}
\end{Shaded}

As usual then, you can calculate the posterior mean and 95\% credible interval:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(lifespan)}
\DocumentationTok{\#\# [1] 0.9398}
\FunctionTok{quantile}\NormalTok{(lifespan, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\DocumentationTok{\#\#   2.5\%  97.5\% }
\DocumentationTok{\#\# 0.6629 1.3194}
\end{Highlighting}
\end{Shaded}

You can also visualise the posterior distribution of lifespan:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifespan }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value), }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"lifespan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-74-1.pdf}}

Now you're good to go. For convenience I have summarized the steps above in the box below. The NIMBLE workflow provided with \texttt{nimbleMCMC()} allows you to build models and make inference. This is what you can achieve with other software like WinBUGS or JAGS.

\begin{blackbox}

\textbf{NIMBLE workflow:}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model building}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \CommentTok{\# derived quantity}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta)}
\NormalTok{\})}
\CommentTok{\# read in data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{released =} \DecValTok{57}\NormalTok{, }\AttributeTok{survived =} \DecValTok{19}\NormalTok{)}
\CommentTok{\# specify parameters to monitor}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"theta"}\NormalTok{, }\StringTok{"lifespan"}\NormalTok{)}
\CommentTok{\# pick initial values}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# specify MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{3}
\CommentTok{\# run NIMBLE}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\CommentTok{\# calculate numerical summaries}
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\CommentTok{\# visualize parameter posterior distribution}
\FunctionTok{MCMCplot}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }
         \AttributeTok{params =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{)}
\CommentTok{\# check convergence}
\FunctionTok{MCMCtrace}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output,}
          \AttributeTok{pdf =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# no export to PDF}
          \AttributeTok{ind =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# separate density lines per chain}
          \AttributeTok{params =} \StringTok{"theta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{blackbox}

But NIMBLE is more than just another MCMC engine. It provides a programming environment so that you have full control when building models and estimating parameters. NIMBLE allows you to write your own functions and distributions to build models, and to choose alternative MCMC samplers or code new ones. This flexibility often comes with faster convergence and often faster runtime.

I have to be honest, learning these improvements over other software takes some reading and experimentation, and it might well be that you do not need to use any of these features. And it's fine. In the next sections, I cover some of this advanced material. You may skip these sections and go back to this material later if you need it.

\section{Programming}\label{functions-in-nimble}

In NIMBLE you can write and use your own functions, or use existing R or C/C++ functions. This allows you to customize models the way you want.

\subsection{NIMBLE functions}\label{nimble-functions}

NIMBLE provides \texttt{nimbleFunctions} for programming. A \texttt{nimbleFunction} is like an R function, plus it can be compiled for faster computation. Going back to our animal survival example, we can write a \texttt{nimbleFunction} to compute lifespan:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{computeLifespan }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
    \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{)) \{ }\CommentTok{\# type declarations}
\NormalTok{        ans }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta)}
        \FunctionTok{return}\NormalTok{(ans)}
        \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))  }\CommentTok{\# return type declaration}
\NormalTok{    \} )}
\end{Highlighting}
\end{Shaded}

Within the nimbleFunction, the \texttt{run} section gives the function to be executed. It is written in the NIMBLE language. The \texttt{theta\ =\ double(0)} and \texttt{returnType(double(0))} arguments tell NIMBLE that the input and output are single numeric values (scalars). Alternatively, \texttt{double(1)} and \texttt{double(2)} are for vectors and matrices, while \texttt{logical()}, \texttt{integer()} and \texttt{character()} are for logical, integer and character values.

You can use your \texttt{nimbleFunction} in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{computeLifespan}\NormalTok{(}\FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# [1] 4.481}
\end{Highlighting}
\end{Shaded}

You can compile it and use the C++ code for faster computation:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CcomputeLifespan }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(computeLifespan)}
\FunctionTok{CcomputeLifespan}\NormalTok{(}\FloatTok{0.8}\NormalTok{)}
\DocumentationTok{\#\# [1] 4.481}
\end{Highlighting}
\end{Shaded}

You can also use your \texttt{nimbleFunction} in a model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \CommentTok{\# derived quantity}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \FunctionTok{computeLifespan}\NormalTok{(theta)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

The rest of the workflow remains the same:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"theta"}\NormalTok{, }\StringTok{"lifespan"}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#          mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# lifespan 0.94 0.16 0.66 0.92  1.31    1  2593}
\DocumentationTok{\#\# theta    0.34 0.06 0.22 0.34  0.47    1  2652}
\end{Highlighting}
\end{Shaded}

With \texttt{nimbleFunctions}, you can mimic basic R syntax, do linear algebra (e.g.~compute eigenvalues), operate on vectors and matrices (e.g.~inverse a matrix), use logical operators (e.g.~and/or) and flow control (e.g.~if-else). There is also a long list of common and less common distributions that can be used with \texttt{nimbleFunctions}.

To learn everything you need to know on writing \texttt{nimbleFunctions}, make sure to read chapter 11 of the NIMBLE manual at \url{https://r-nimble.org/html_manual/cha-RCfunctions.html\#cha-RCfunctions}.

\subsection{Calling R/C++ functions}\label{callrfninnimble}

If you're like me, and too lazy to write your own functions, you can rely on the scientific community and use existing C, C++ or R code. The trick is to write a \texttt{nimbleFunction} that wraps access to that code which can then be used by NIMBLE. As an example, imagine you'd like to use an R function \texttt{myfunction()}, either a function you wrote yourself, or a function available in your favorite R package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myfunction }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(x)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now wrap this function using \texttt{nimbleRcall()} or \texttt{nimbleExternalCall()} for a C or C++ function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rmyfunction }\OtherTok{\textless{}{-}} \FunctionTok{nimbleRcall}\NormalTok{(}\AttributeTok{prototype =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{x =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))\{\}, }
                           \AttributeTok{Rfun =} \StringTok{\textquotesingle{}myfunction\textquotesingle{}}\NormalTok{,}
                           \AttributeTok{returnType =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In the call to \texttt{nimbleRcall()} above, the argument \texttt{prototype} specifies inputs (a single numeric value \texttt{double(0)}) of the R function \texttt{Rfun} that generates outputs \texttt{returnType} (a single numeric value \texttt{double(0)}).

Now you can call your R function from a model (or any \texttt{nimbleFunctions}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \FunctionTok{Rmyfunction}\NormalTok{(theta)}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

The rest of the workflow remains the same:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"theta"}\NormalTok{, }\StringTok{"lifespan"}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#          mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# lifespan 0.94 0.16 0.68 0.92  1.29    1  2597}
\DocumentationTok{\#\# theta    0.34 0.06 0.23 0.34  0.46    1  2643}
\end{Highlighting}
\end{Shaded}

Evaluating an R function from within NIMBLE slows down MCMC sampling, but if you can live with it, the cost is easily offset by the convenience of being able to use existing R functions.

\subsection{User-defined distributions}\label{user-defined-distributions}

With \texttt{nimbleFunctions} you can provide user-defined distributions to NIMBLE. You need to write functions for density (\texttt{d}) and simulation (\texttt{r}) for your distribution. As an example, we write our own binomial distribution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# density}
\NormalTok{dmybinom }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{x =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                 \AttributeTok{prob =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{), }
                 \AttributeTok{log =} \FunctionTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{1}\NormalTok{)) \{}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
    \CommentTok{\# compute binomial coefficient = size! / [x! (n{-}x)!] and take log}
\NormalTok{    lchoose }\OtherTok{\textless{}{-}} \FunctionTok{lfactorial}\NormalTok{(size) }\SpecialCharTok{{-}} \FunctionTok{lfactorial}\NormalTok{(x) }\SpecialCharTok{{-}} \FunctionTok{lfactorial}\NormalTok{(size }\SpecialCharTok{{-}}\NormalTok{ x)}
    \CommentTok{\# binomial density function = size! / [x! (n{-}x)!] * prob\^{}x * (1{-}prob)\^{}(size{-}x) and take log}
\NormalTok{    logProb }\OtherTok{\textless{}{-}}\NormalTok{ lchoose }\SpecialCharTok{+}\NormalTok{ x }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(prob) }\SpecialCharTok{+}\NormalTok{ (size }\SpecialCharTok{{-}}\NormalTok{ x) }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ prob)}
    \ControlFlowTok{if}\NormalTok{(log) }\FunctionTok{return}\NormalTok{(logProb)}
    \ControlFlowTok{else} \FunctionTok{return}\NormalTok{(}\FunctionTok{exp}\NormalTok{(logProb)) }
\NormalTok{  \})}
\CommentTok{\# simulation using the coin flip method (p. 524 in Devroye 1986)}
\CommentTok{\# note: the n argument is required by NIMBLE but is not used, default is 1}
\NormalTok{rmybinom }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n =} \FunctionTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{1}\NormalTok{),}
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                 \AttributeTok{prob =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{)) \{}
\NormalTok{      x }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{      y }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\AttributeTok{n =}\NormalTok{ size, }\AttributeTok{min =} \DecValTok{0}\NormalTok{, }\AttributeTok{max =} \DecValTok{1}\NormalTok{)}
      \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{size)\{}
        \ControlFlowTok{if}\NormalTok{ (y[j] }\SpecialCharTok{\textless{}}\NormalTok{ prob)\{}
\NormalTok{          x }\OtherTok{\textless{}{-}}\NormalTok{ x }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{        \}}\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{          x }\OtherTok{\textless{}{-}}\NormalTok{ x}
\NormalTok{        \}}
\NormalTok{      \}}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
    \FunctionTok{return}\NormalTok{(x)    }
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

You need to define the \texttt{nimbleFunctions} in R's global environment for them to be accessed:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}dmybinom\textquotesingle{}}\NormalTok{, dmybinom, .GlobalEnv)}
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}rmybinom\textquotesingle{}}\NormalTok{, rmybinom, .GlobalEnv)}
\end{Highlighting}
\end{Shaded}

You can try out your function and simulate a single random value (n = 1 by default) from a binomial distribution with size 5 and probability 0.1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rmybinom}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.1}\NormalTok{)}
\DocumentationTok{\#\# [1] 0}
\end{Highlighting}
\end{Shaded}

All set. You can run your workflow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
 \CommentTok{\# likelihood}
\NormalTok{ survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dmybinom}\NormalTok{(}\AttributeTok{prob =}\NormalTok{ theta, }\AttributeTok{size =}\NormalTok{ released)}
 \CommentTok{\# prior}
\NormalTok{ theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{\})}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{released =} \DecValTok{57}\NormalTok{, }\AttributeTok{survived =} \DecValTok{19}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output)}
\DocumentationTok{\#\#       mean      sd   2.5\%    50\%  97.5\% Rhat n.eff}
\DocumentationTok{\#\# theta 0.34 0.05976 0.2286 0.3378 0.4598    1  2970}
\end{Highlighting}
\end{Shaded}

Having \texttt{nimbleFunctions} offers infinite possibilities to customize your models and algorithms. Besides what we covered already, you can write your own samplers. We will see an example in a minute, but I first need to tell you more about the NIMBLE workflow.

\section{Under the hood}\label{under-the-hood}

So far, you have used \texttt{nimbleMCMC()} which runs the default MCMC workflow. This is perfecly fine for most applications. However, in some situations you need to customize the MCMC samplers to improve or speed up convergence. NIMBLE allows you to look under the hood by using a detailed workflow in several steps: \texttt{nimbleModel()}, \texttt{configureMCMC()}, \texttt{buildMCMC()}, \texttt{compileNimble()} and \texttt{runMCMC()}. Note that \texttt{nimbleMCMC()} does all of this at once.

We write the model code, read in data and pick initial values as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \CommentTok{\# derived quantity}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta)}
\NormalTok{\})}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

First step is to create the model as an R object (uncompiled model) with \texttt{nimbleModel()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                        \AttributeTok{data =}\NormalTok{ my.data,}
                        \AttributeTok{inits =}\NormalTok{ initial.values)}
\end{Highlighting}
\end{Shaded}

You can look at its nodes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{getNodeNames}\NormalTok{()}
\DocumentationTok{\#\# [1] "theta"    "lifespan" "survived"}
\end{Highlighting}
\end{Shaded}

You can look at the values stored at each node:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival}\SpecialCharTok{$}\NormalTok{theta}
\DocumentationTok{\#\# [1] 0.5}
\NormalTok{survival}\SpecialCharTok{$}\NormalTok{survived}
\DocumentationTok{\#\# [1] 19}
\NormalTok{survival}\SpecialCharTok{$}\NormalTok{lifespan }
\DocumentationTok{\#\# [1] 1.443}
\CommentTok{\# this is {-}1/log(0.5)}
\end{Highlighting}
\end{Shaded}

We can also calculate the log-likelihood at the initial value for \texttt{theta}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{()}
\DocumentationTok{\#\# [1] {-}5.422}
\CommentTok{\# this is dbinom(x = 19, size = 57, prob = 0.5, log = TRUE)}
\end{Highlighting}
\end{Shaded}

The ability in NIMBLE to access the nodes of your model and to evaluate the model likelihood can help you in identifying bugs in your code. For example, if we provide a negative initial value for \texttt{theta}, \texttt{survival\$calculate()} returns NA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                        \AttributeTok{data =}\NormalTok{ my.data,}
                        \AttributeTok{inits =} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{))}
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{()}
\DocumentationTok{\#\# [1] NaN}
\end{Highlighting}
\end{Shaded}

As another example, if we convey in the data the information that more animals survived than were released, we'll get an infinity value for the log-likelihood:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{61}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                        \AttributeTok{data =}\NormalTok{ my.data,}
                        \AttributeTok{inits =}\NormalTok{ initial.values)}
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{()}
\DocumentationTok{\#\# [1] {-}Inf}
\end{Highlighting}
\end{Shaded}

As a check that the model is correctly initialized and that your code is without bugs, the call to \texttt{model\$calculate()} should return a number and not NA or -Inf:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FloatTok{0.5}\NormalTok{)}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                        \AttributeTok{data =}\NormalTok{ my.data,}
                        \AttributeTok{inits =}\NormalTok{ initial.values)}
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{()}
\DocumentationTok{\#\# [1] {-}5.422}
\end{Highlighting}
\end{Shaded}

You can obtain the graph of the model as in Figure \ref{fig:dag-survival} with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{plotGraph}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-97-1.pdf}}

Second we compile the model with \texttt{compileNimble()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\end{Highlighting}
\end{Shaded}

With \texttt{compileNimble()}, the C++ code is generated, compiled and loaded back into R so that it can be used in R (compiled model):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Csurvival}\SpecialCharTok{$}\NormalTok{theta}
\DocumentationTok{\#\# [1] 0.5}
\end{Highlighting}
\end{Shaded}

Now you have two versions of the model, \texttt{survival} is in R and \texttt{Csurvival} in C++. Being able to separate the steps of model building and parameter estimation is a strength of NIMBLE. This gives you a lot of flexibility at both steps. For example, imagine you would like to fit your model with maximum likelihood, then you can do it by wrapping your model in an R function that gets the likelihood and maximise this function. Using the C version of the model, you can write:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# function for negative log{-}likelihood to minimize}
\NormalTok{f }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(par) \{}
\NormalTok{    Csurvival[[}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{]] }\OtherTok{\textless{}{-}}\NormalTok{ par }\CommentTok{\# assign par to theta }
\NormalTok{    ll }\OtherTok{\textless{}{-}}\NormalTok{ Csurvival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{() }\CommentTok{\# update log{-}likelihood with par value}
    \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ll) }\CommentTok{\# return negative log{-}likelihood}
\NormalTok{\}}
\CommentTok{\# evaluate function at 0.5 and 0.9}
\FunctionTok{f}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}
\DocumentationTok{\#\# [1] 5.422}
\FunctionTok{f}\NormalTok{(}\FloatTok{0.9}\NormalTok{)}
\DocumentationTok{\#\# [1] 55.41}
\CommentTok{\# minimize function}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{optimize}\NormalTok{(f, }\AttributeTok{interval =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{round}\NormalTok{(out}\SpecialCharTok{$}\NormalTok{minimum, }\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# [1] 0.33}
\end{Highlighting}
\end{Shaded}

By maximising the likelihood (or minimising the negative log-likelihood), you obtain the maximum likelihood estimate of animal survival, which is exactly 19 surviving animals over 57 released animals or 0.33.

Third we create a MCMC configuration for our model with \texttt{configureMCMC()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalConf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(survival)}
\DocumentationTok{\#\# ===== Monitors =====}
\DocumentationTok{\#\# thin = 1: theta}
\DocumentationTok{\#\# ===== Samplers =====}
\DocumentationTok{\#\# RW sampler (1)}
\DocumentationTok{\#\#   {-} theta}
\end{Highlighting}
\end{Shaded}

This steps tells you the nodes that are monitored by default, and the MCMC samplers than have been assigned to them. Here \texttt{theta} is monitored, and samples from its posterior distribution are simulated with a random walk sampler similar to the Metropolis sampler we coded in the previous chapter in Section \ref{metropolis-algorithm}.

To monitor \texttt{lifespan} in addition to \texttt{theta}, you write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{addMonitors}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"lifespan"}\NormalTok{))}
\DocumentationTok{\#\# thin = 1: lifespan, theta}
\NormalTok{survivalConf}
\DocumentationTok{\#\# ===== Monitors =====}
\DocumentationTok{\#\# thin = 1: lifespan, theta}
\DocumentationTok{\#\# ===== Samplers =====}
\DocumentationTok{\#\# RW sampler (1)}
\DocumentationTok{\#\#   {-} theta}
\end{Highlighting}
\end{Shaded}

Third, we create a MCMC function with \texttt{buildMCMC()} and compile it with \texttt{compileNimble()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(survivalConf)}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC, }\AttributeTok{project =}\NormalTok{ survival)}
\end{Highlighting}
\end{Shaded}

Note that models and \texttt{nimbleFunctions} need to be compiled before they can be used to specify a project.

Fourth, we run NIMBLE with \texttt{runMCMC()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =}\NormalTok{ n.iter,}
                   \AttributeTok{nburnin =}\NormalTok{ n.burnin)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\end{Highlighting}
\end{Shaded}

We run a single chain but \texttt{runMCMC()} allows you to use multiple chains as with \texttt{nimbleMCMC()}.

You can look into \texttt{samples} which contains values simulated from the posterior distribution of the parameters we monitor:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(samples)}
\DocumentationTok{\#\#      lifespan  theta}
\DocumentationTok{\#\# [1,]   0.9093 0.3330}
\DocumentationTok{\#\# [2,]   0.9093 0.3330}
\DocumentationTok{\#\# [3,]   0.9093 0.3330}
\DocumentationTok{\#\# [4,]   1.2095 0.4374}
\DocumentationTok{\#\# [5,]   1.2095 0.4374}
\DocumentationTok{\#\# [6,]   1.1835 0.4296}
\end{Highlighting}
\end{Shaded}

From here, you can obtain numerical summaries with \texttt{samplesSummary()} (or \texttt{MCMCvis::MCMCsummary()}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{samplesSummary}\NormalTok{(samples)}
\DocumentationTok{\#\#            Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# lifespan 0.9357 0.9194 0.16117    0.6831    1.2969}
\DocumentationTok{\#\# theta    0.3386 0.3370 0.06128    0.2313    0.4625}
\end{Highlighting}
\end{Shaded}

I have summarized the steps above in the box below.

\begin{blackbox}

\textbf{Detailed NIMBLE workflow:}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# model building}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  \CommentTok{\# derived quantity}
\NormalTok{  lifespan }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\FunctionTok{log}\NormalTok{(theta)}
\NormalTok{\})}
\CommentTok{\# read in data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{released =} \DecValTok{57}\NormalTok{, }\AttributeTok{survived =} \DecValTok{19}\NormalTok{)}
\CommentTok{\# pick initial values}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# create model as an R object (uncompiled model)}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                        \AttributeTok{data =}\NormalTok{ my.data,}
                        \AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\CommentTok{\# compile model}
\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\CommentTok{\# create a MCMC configuration}
\NormalTok{survivalConf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(survival)}
\CommentTok{\# add lifespan to list of parameters to monitor}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{addMonitors}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"lifespan"}\NormalTok{))}
\CommentTok{\# create a MCMC function and compile it}
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(survivalConf)}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC, }\AttributeTok{project =}\NormalTok{ survival)}
\CommentTok{\# specify MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\CommentTok{\# run NIMBLE}
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =}\NormalTok{ n.iter,}
                   \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                   \AttributeTok{nchain =}\NormalTok{ n.chains)}
\CommentTok{\# calculate numerical summaries}
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ samples, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\CommentTok{\# visualize parameter posterior distribution}
\FunctionTok{MCMCplot}\NormalTok{(}\AttributeTok{object =}\NormalTok{ samples, }
         \AttributeTok{params =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{)}
\CommentTok{\# check convergence}
\FunctionTok{MCMCtrace}\NormalTok{(}\AttributeTok{object =}\NormalTok{ samples,}
          \AttributeTok{pdf =} \ConstantTok{FALSE}\NormalTok{, }\CommentTok{\# no export to PDF}
          \AttributeTok{ind =} \ConstantTok{TRUE}\NormalTok{, }\CommentTok{\# separate density lines per chain}
          \AttributeTok{params =} \StringTok{"theta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{blackbox}

At first glance, using several steps instead of doing all these at once with \texttt{nimbleMCMC()} seems odds. Why is it useful? Mastering the whole sequence of steps allows you to play around with samplers, by changing the samplers NIMBLE picks by default, or even writing your own samplers.

\section{MCMC samplers}\label{mcmc-samplers}

\subsection{Default samplers}\label{change-sampler}

What is the default sampler used by NIMBLE in our example? You can answer this question by inspecting the MCMC configuration obtained with \texttt{configureMCMC()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#survivalConf \textless{}{-} configureMCMC(survival)}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{printSamplers}\NormalTok{()}
\DocumentationTok{\#\# [1] RW sampler: theta}
\end{Highlighting}
\end{Shaded}

Now that we have control on the MCMC configuration, let's mess it up. We start by removing the default sampler:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{removeSamplers}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{))}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{printSamplers}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

And we change it for a slice sampler:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{addSampler}\NormalTok{(}\AttributeTok{target =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{),}
                        \AttributeTok{type =} \StringTok{\textquotesingle{}slice\textquotesingle{}}\NormalTok{)}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{printSamplers}\NormalTok{()}
\DocumentationTok{\#\# [1] slice sampler: theta}
\end{Highlighting}
\end{Shaded}

Now you can resume the workflow:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new MCMC function and compile it:}
\NormalTok{survivalMCMC2 }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(survivalConf)}
\NormalTok{CsurvivalMCMC2 }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC2, }
                                \AttributeTok{project =}\NormalTok{ survival,}
                                \AttributeTok{resetFunctions =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# to compile new functions }
                                                       \CommentTok{\# into existing project, }
                                                       \CommentTok{\# need to reset nimbleFunctions}
\CommentTok{\# run NIMBLE:}
\NormalTok{samples2 }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC2, }
                    \AttributeTok{niter =}\NormalTok{ n.iter,}
                    \AttributeTok{nburnin =}\NormalTok{ n.burnin)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\CommentTok{\# obtain numerical summaries:}
\FunctionTok{samplesSummary}\NormalTok{(samples2)}
\DocumentationTok{\#\#            Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# lifespan 0.9357 0.9231 0.16002    0.6645    1.2826}
\DocumentationTok{\#\# theta    0.3387 0.3385 0.06098    0.2221    0.4586}
\end{Highlighting}
\end{Shaded}

NIMBLE implements many samplers, and a list is available with \texttt{?samplers}. For example, high correlation in (regression) parameters can make independent samplers inefficient. In that situation, block sampling might help which consists in proposing candidate values from a multivariate distribution that acknowledges correlation between parameters.

\subsection{User-defined samplers}\label{user-defined-samplers}

Allowing you to code your own sampler is another topic on which NIMBLE thrives. As an example, we focus on the Metropolis algorithm of Section \ref{metropolis-algorithm} which we coded in R. In this section, we make it a \texttt{nimbleFunction} so that we can use it within our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_metropolis }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{name =} \StringTok{\textquotesingle{}my\_metropolis\textquotesingle{}}\NormalTok{, }\CommentTok{\# fancy name for our MCMC sampler}
  \AttributeTok{contains =}\NormalTok{ sampler\_BASE,}
  \AttributeTok{setup =} \ControlFlowTok{function}\NormalTok{(model, mvSaved, target, control) \{}
    \CommentTok{\# i) get dependencies for \textquotesingle{}target\textquotesingle{} in \textquotesingle{}model\textquotesingle{}}
\NormalTok{    calcNodes }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\FunctionTok{getDependencies}\NormalTok{(target) }
    \CommentTok{\# ii) get sd of proposal distribution}
\NormalTok{    scale }\OtherTok{\textless{}{-}}\NormalTok{ control}\SpecialCharTok{$}\NormalTok{scale }
\NormalTok{  \},}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{() \{}
    \CommentTok{\# (1) log{-}lik at current value}
\NormalTok{    initialLP }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\FunctionTok{getLogProb}\NormalTok{(calcNodes) }
    \CommentTok{\# (2) current parameter value}
\NormalTok{    current }\OtherTok{\textless{}{-}}\NormalTok{ model[[target]] }
    \CommentTok{\# (3) logit transform}
\NormalTok{    lcurrent }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(current }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ current))}
    \CommentTok{\# (4) propose candidate value}
\NormalTok{    lproposal }\OtherTok{\textless{}{-}}\NormalTok{ lcurrent  }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, scale) }
    \CommentTok{\# (5) back{-}transform}
\NormalTok{    proposal }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(lproposal)}
    \CommentTok{\# (6) plug candidate value in model }
\NormalTok{    model[[target]] }\OtherTok{\textless{}\textless{}{-}}\NormalTok{ proposal }
    \CommentTok{\# (7) log{-}lik at candidate value}
\NormalTok{    proposalLP }\OtherTok{\textless{}{-}}\NormalTok{ model}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{(calcNodes)}
    \CommentTok{\# (8) compute lik ratio on log scale}
\NormalTok{    lMHR }\OtherTok{\textless{}{-}}\NormalTok{ proposalLP }\SpecialCharTok{{-}}\NormalTok{ initialLP }
    \CommentTok{\# (9) spin continuous spinner and compare to ratio}
    \ControlFlowTok{if}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textless{}} \FunctionTok{exp}\NormalTok{(lMHR)) \{ }
      \CommentTok{\# (10) if candidate value is accepted, update current value}
      \FunctionTok{copy}\NormalTok{(}\AttributeTok{from =}\NormalTok{ model, }\AttributeTok{to =}\NormalTok{ mvSaved, }\AttributeTok{nodes =}\NormalTok{ calcNodes, }\AttributeTok{logProb =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row =} \DecValTok{1}\NormalTok{)}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{ \{}
      \DocumentationTok{\#\# (11) if candidate value is accepted, keep current value}
      \FunctionTok{copy}\NormalTok{(}\AttributeTok{from =}\NormalTok{ mvSaved, }\AttributeTok{to =}\NormalTok{ model, }\AttributeTok{nodes =}\NormalTok{ calcNodes, }\AttributeTok{logProb =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row =} \DecValTok{1}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{  \},}
  \AttributeTok{methods =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{reset =} \ControlFlowTok{function}\NormalTok{() \{\}}
\NormalTok{  )}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Compared to \texttt{nimbleFunctions} we wrote earlier, \texttt{my\_metropolis()} contains a \texttt{setup} function which i) gets the dependencies of the parameter to update in the \texttt{run} function with Metropolis, the target node, that would be \texttt{theta} in our example and ii) extracts control parameters, that would be \texttt{scale} the standard deviation of the proposal distribution in our example. Then the \texttt{run} function implements the steps of the Metropolis algorithm: (1) get the log-likelihood function evaluated at the current value, (2) get the current value, (3) apply the logit transform to it, (4) propose a candidate value by perturbing the current value with some normal noise controled by the standard deviation \texttt{scale}, (5) back-transform the candidate value and (6) plug it in the model, (7) calculate the log-likelihood function at the candidate value, (8) compute the Metropolis ratio on the log scale, (9) compare output of a spinner and the Metropolis ratio to decide whether to (10) accept the candidate value and copy from the model to \texttt{mvSaved} or (11) reject it and keep the current value by copying from \texttt{mvSaved} to the model. Because this \texttt{nimbleFunction} is to be used as a MCMC sampler, several constraints need to be respected like having a \texttt{contains\ =\ sampler\_BASE} statement or using the four arguments \texttt{model}, \texttt{mvSaved}, \texttt{target} and \texttt{control} in the \texttt{setup} function. Of course, NIMBLE implements a more advanced and efficient version of the Metropolis algorithm, you can look into it at \url{https://github.com/cran/nimble/blob/master/R/MCMC_samplers.R\#L184}.

Now that we have our user-defined MCMC algorithm, we can change the default sampler for our new sampler as in Section \ref{change-sampler}. We start from scratch:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{\})}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }
                        \AttributeTok{data =}\NormalTok{ my.data, }
                        \AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\NormalTok{survivalConf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(survival)}
\DocumentationTok{\#\# ===== Monitors =====}
\DocumentationTok{\#\# thin = 1: theta}
\DocumentationTok{\#\# ===== Samplers =====}
\DocumentationTok{\#\# RW sampler (1)}
\DocumentationTok{\#\#   {-} theta}
\end{Highlighting}
\end{Shaded}

We print the samplers used by default, remove the default sampler for \texttt{theta}, replace it with our \texttt{my\_metropolis()} sampler with the standard deviation of the proposal distribution set to 0.1, and print again to make sure NIMBLE now uses our new sampler:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{printSamplers}\NormalTok{()}
\DocumentationTok{\#\# [1] RW sampler: theta}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{removeSamplers}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{))}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{addSampler}\NormalTok{(}\AttributeTok{target =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{, }
                        \AttributeTok{type =} \StringTok{\textquotesingle{}my\_metropolis\textquotesingle{}}\NormalTok{, }
                        \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{scale =} \FloatTok{0.1}\NormalTok{)) }\CommentTok{\# standard deviation}
                                                     \CommentTok{\# of proposal distribution}
\NormalTok{survivalConf}\SpecialCharTok{$}\FunctionTok{printSamplers}\NormalTok{()}
\DocumentationTok{\#\# [1] my\_metropolis sampler: theta,  scale: 0.10000000000000001}
\end{Highlighting}
\end{Shaded}

The rest of the workflow is unchanged:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(survivalConf)}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC, }
                               \AttributeTok{project =}\NormalTok{ survival)}
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =} \DecValTok{5000}\NormalTok{, }
                   \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\FunctionTok{samplesSummary}\NormalTok{(samples)}
\DocumentationTok{\#\#        Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.339 0.3377 0.05592    0.2374    0.4528}
\end{Highlighting}
\end{Shaded}

You can re-run the analysis by setting the standard deviation of the proposal to different values, say 1 and 10, and compare the results to traceplots we obtained with our R implementation of the Metropolis algorithm in the previous chapter:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# standard deviation of proposal is 0.1}
\NormalTok{scale }\OtherTok{\textless{}{-}} \FloatTok{0.1}
\NormalTok{Rmodel }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }\AttributeTok{data =}\NormalTok{ my.data, }\AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\NormalTok{conf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(Rmodel, }\AttributeTok{monitors =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{), }\AttributeTok{print =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{removeSamplers}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{))}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{addSampler}\NormalTok{(}\AttributeTok{target =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{, }\AttributeTok{type =} \StringTok{\textquotesingle{}my\_metropolis\textquotesingle{}}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{scale =}\NormalTok{ scale))}
\NormalTok{Rmcmc }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(conf)}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Rmodel, }\AttributeTok{mcmc =}\NormalTok{ Rmcmc))}
\NormalTok{Cmcmc }\OtherTok{\textless{}{-}}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{mcmc}
\NormalTok{samples\_sd01 }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(Cmcmc, }\AttributeTok{niter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{nburnin =} \DecValTok{9000}\NormalTok{, }\AttributeTok{progressBar =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# standard deviation of proposal is 1}
\NormalTok{scale }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{Rmodel }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }\AttributeTok{data =}\NormalTok{ my.data, }\AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\NormalTok{conf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(Rmodel, }\AttributeTok{monitors =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{), }\AttributeTok{print =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{removeSamplers}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{))}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{addSampler}\NormalTok{(}\AttributeTok{target =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{, }\AttributeTok{type =} \StringTok{\textquotesingle{}my\_metropolis\textquotesingle{}}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{scale =}\NormalTok{ scale))}
\NormalTok{Rmcmc }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(conf)}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Rmodel, }\AttributeTok{mcmc =}\NormalTok{ Rmcmc))}
\NormalTok{Cmcmc }\OtherTok{\textless{}{-}}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{mcmc}
\NormalTok{samples\_sd1 }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(Cmcmc, }\AttributeTok{niter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{nburnin =} \DecValTok{9000}\NormalTok{, }\AttributeTok{progressBar =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# standard deviation of proposal is 10}
\NormalTok{scale }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{Rmodel }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }\AttributeTok{data =}\NormalTok{ my.data, }\AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\NormalTok{conf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(Rmodel, }\AttributeTok{monitors =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{), }\AttributeTok{print =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{removeSamplers}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{))}
\NormalTok{conf}\SpecialCharTok{$}\FunctionTok{addSampler}\NormalTok{(}\AttributeTok{target =} \StringTok{\textquotesingle{}theta\textquotesingle{}}\NormalTok{, }\AttributeTok{type =} \StringTok{\textquotesingle{}my\_metropolis\textquotesingle{}}\NormalTok{, }\AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{scale =}\NormalTok{ scale))}
\NormalTok{Rmcmc }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(conf)}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{model =}\NormalTok{ Rmodel, }\AttributeTok{mcmc =}\NormalTok{ Rmcmc))}
\NormalTok{Cmcmc }\OtherTok{\textless{}{-}}\NormalTok{ out}\SpecialCharTok{$}\NormalTok{mcmc}
\NormalTok{samples\_sd10 }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(Cmcmc, }\AttributeTok{niter =} \DecValTok{10000}\NormalTok{, }\AttributeTok{nburnin =} \DecValTok{9000}\NormalTok{, }\AttributeTok{progressBar =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# trace plot for scenario with standard deviation 0.1}
\NormalTok{plot01 }\OtherTok{\textless{}{-}}\NormalTok{ samples\_sd01 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{9001}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{, }\AttributeTok{y =}\NormalTok{ theta) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{title =} \StringTok{"scale = 0.1"}\NormalTok{)}
\CommentTok{\# trace plot for scenario with standard deviation 1}
\NormalTok{plot1 }\OtherTok{\textless{}{-}}\NormalTok{ samples\_sd1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{9001}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{, }\AttributeTok{y =}\NormalTok{ theta) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{title =} \StringTok{"scale = 1"}\NormalTok{)}
\CommentTok{\# trace plot for scenario with standard deviation 10}
\NormalTok{plot10 }\OtherTok{\textless{}{-}}\NormalTok{ samples\_sd10 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{9001}\SpecialCharTok{:}\DecValTok{10000}\NormalTok{, }\AttributeTok{y =}\NormalTok{ theta) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"iterations"}\NormalTok{, }\AttributeTok{title =} \StringTok{"scale = 10"}\NormalTok{)}
\CommentTok{\# Assemble all three trace plots}
\FunctionTok{library}\NormalTok{(patchwork)}
\NormalTok{plot01 }\SpecialCharTok{+}\NormalTok{ plot1 }\SpecialCharTok{+}\NormalTok{ plot10}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/traceown-1.pdf}}

\section{Tips and tricks}\label{tips-and-tricks}

Before closing this chapter on NIMBLE, I thought it'd be useful to have a section gathering a few tips and tricks that would make your life easier.

\subsection{Precision vs standard deviation}\label{precision-vs-standard-deviation}

In other sotware like JAGS, the normal distribution is parameterized with mean \texttt{mu} and a parameter called precision, often denoted \texttt{tau}, the inverse of the variance you are used to. Say we use a normal prior on some parameter \texttt{epsilon} with \texttt{epsilon\ \textasciitilde{}\ dnorm(mu,\ tau)}. We'd like this prior to be vague, therefore \texttt{tau} should be small, say 0.01 so that the variance of the normal distribution is large, 1/0.01 = 100 here. This subtlety is the source of problems (and frustration) when you forget that the second parameter is precision and use \texttt{epsilon\ \textasciitilde{}\ dnorm(mu,\ 100)}, because then the variance is actually 1/100 = 0.01 and the prior is very informative, and peaked on \texttt{mu}. In NIMBLE you can use this parameterisation as well as the more natural parameterisation \texttt{epsilon\ \textasciitilde{}\ dnorm(mu,\ sd\ =\ 100)} which avoids confusion.

\subsection{Indexing}\label{indexing}

NIMBLE does not guess the dimensions of objects. In other software like JAGS you can write \texttt{sum.x\ \textless{}-\ sum(x{[}{]})} to calculate the sum over all components of \texttt{x}. In NIMBLE you need to write \texttt{sum.x\ \textless{}-\ sum(x{[}1:n{]})} to sum the components of \texttt{x} from 1 up to n.~Specifying dimensions can be annoying, but I find it useful as it forces me to think of what I am doing and to keep my code self-explaining.

\subsection{Faster compilation}\label{faster-compilation}

You might have noticed that compilation in NIMBLE takes time. When you have large models (with lots of nodes), compilation can take forever. You can set \texttt{calculate\ =\ FALSE} in \texttt{nimbleModel()} to disable the calculation of all deterministic nodes and log-likelihood. The downside of not doing the \texttt{calculate()}, is that you might not be able to identify issues that could help you save time in the long run. You can also use \texttt{useConjugacy\ =\ FALSE} in \texttt{configureMCMC()} to disable the search for conjugate samplers. With the animal survival example, you would do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{\})}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }
                        \AttributeTok{data =}\NormalTok{ my.data, }
                        \AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{(),}
                        \AttributeTok{calculate =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# first tip}
\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\NormalTok{survivalConf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(survival)}
\DocumentationTok{\#\# ===== Monitors =====}
\DocumentationTok{\#\# thin = 1: theta}
\DocumentationTok{\#\# ===== Samplers =====}
\DocumentationTok{\#\# RW sampler (1)}
\DocumentationTok{\#\#   {-} theta}
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(survivalConf, }\AttributeTok{useConjugacy =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\# second tip}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC, }
                               \AttributeTok{project =}\NormalTok{ survival)}
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =} \DecValTok{5000}\NormalTok{, }
                   \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\FunctionTok{samplesSummary}\NormalTok{(samples)}
\DocumentationTok{\#\#         Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.3402 0.3391 0.06029    0.2258    0.4616}
\end{Highlighting}
\end{Shaded}

\subsection{Updating MCMC chains}\label{updating-mcmc-chains}

Sometimes it is useful to run your MCMC chains a little bit longer to improve convergence. Re-starting from the run in previous section, you can use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{niter\_ad }\OtherTok{\textless{}{-}} \DecValTok{6000}
\NormalTok{CsurvivalMCMC}\SpecialCharTok{$}\FunctionTok{run}\NormalTok{(niter\_ad, }\AttributeTok{reset =} \ConstantTok{FALSE}\NormalTok{)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\end{Highlighting}
\end{Shaded}

Then you can extract the matrix of previous MCMC samples augmented with new ones and obtain numerical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{more\_samples }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(CsurvivalMCMC}\SpecialCharTok{$}\NormalTok{mvSamples)}
\FunctionTok{samplesSummary}\NormalTok{(more\_samples)}
\DocumentationTok{\#\#         Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.3402 0.3382 0.05975    0.2281    0.4632}
\end{Highlighting}
\end{Shaded}

You can check that \texttt{more\_samples} contains 10000 samples, 4000 from the call to \texttt{runMCMC()} plus 6000 additional samples. Note that this only works if you are using the detailed NIMBLE workflow. It does not work with the \texttt{nimbleMCMC()} wrapper function.

\subsection{Reproducibility}\label{tipreproducibility}

If you want your results to be reproducible, you can control the state of R the random number generator with the \texttt{setSeed} argument in functions \texttt{nimbleMCMC()} and \texttt{runMCMC()}. Going back to the animal survival example, you can check that two calls to \texttt{nimbleMCMC()} give the same results when \texttt{setSeed} is set to the same value. Note that we need to specify a seed for each chain, hence a vector of three components here:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first call to nimbleMCMC()}
\NormalTok{mcmc.output1 }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                           \AttributeTok{data =}\NormalTok{ my.data,}
                           \AttributeTok{inits =}\NormalTok{ initial.values,}
                           \AttributeTok{niter =} \DecValTok{5000}\NormalTok{,}
                           \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{,}
                           \AttributeTok{nchains =} \DecValTok{3}\NormalTok{,}
                           \AttributeTok{summary =} \ConstantTok{TRUE}\NormalTok{,}
                           \AttributeTok{setSeed =} \FunctionTok{c}\NormalTok{(}\DecValTok{2024}\NormalTok{, }\DecValTok{2025}\NormalTok{, }\DecValTok{2026}\NormalTok{))}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\CommentTok{\# second call to nimbleMCMC()}
\NormalTok{mcmc.output2 }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model,}
                           \AttributeTok{data =}\NormalTok{ my.data,}
                           \AttributeTok{inits =}\NormalTok{ initial.values,}
                           \AttributeTok{niter =} \DecValTok{5000}\NormalTok{,}
                           \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{,}
                           \AttributeTok{nchains =} \DecValTok{3}\NormalTok{,}
                           \AttributeTok{summary =} \ConstantTok{TRUE}\NormalTok{,}
                           \AttributeTok{setSeed =} \FunctionTok{c}\NormalTok{(}\DecValTok{2024}\NormalTok{, }\DecValTok{2025}\NormalTok{, }\DecValTok{2026}\NormalTok{))}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\CommentTok{\# outputs from both calls are the same}
\NormalTok{mcmc.output1}\SpecialCharTok{$}\NormalTok{summary}\SpecialCharTok{$}\NormalTok{all.chains}
\DocumentationTok{\#\#        Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.339 0.3363 0.06141    0.2245    0.4618}
\NormalTok{mcmc.output2}\SpecialCharTok{$}\NormalTok{summary}\SpecialCharTok{$}\NormalTok{all.chains}
\DocumentationTok{\#\#        Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.339 0.3363 0.06141    0.2245    0.4618}
\end{Highlighting}
\end{Shaded}

Note that to make your workflow reproducible, you need to set the seed not only within the \texttt{nimbleMCMC()} call, but also before setting your initial values if you are using a randomized function for that.

\subsection{Parallelization}\label{parallelization}

To speed up your analyses, you can run MCMC chains in parallel. This is what the package \texttt{jagsUI}\footnote{\url{https://github.com/kenkellner/jagsUI}} accomplishes for JAGS users. Here, we use the \texttt{parallel} package for parallel computation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(parallel)}
\end{Highlighting}
\end{Shaded}

First you create a cluster using the total amount of cores you have but one to make sure your computer can go on working:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nbcores }\OtherTok{\textless{}{-}} \FunctionTok{detectCores}\NormalTok{() }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{my\_cluster }\OtherTok{\textless{}{-}} \FunctionTok{makeCluster}\NormalTok{(nbcores)}
\end{Highlighting}
\end{Shaded}

Then you wrap your workflow in a function to be run in parallel:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{workflow }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(seed, data) \{}
  
  \FunctionTok{library}\NormalTok{(nimble)}
  
\NormalTok{  model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
    \CommentTok{\# likelihood}
\NormalTok{    survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
    \CommentTok{\# prior}
\NormalTok{    theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  \})}
  
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# for reproducibility}
\NormalTok{  initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{theta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
  
\NormalTok{  survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }
                          \AttributeTok{data =}\NormalTok{ data, }
                          \AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}
\NormalTok{  Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\NormalTok{  survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(Csurvival)}
\NormalTok{  CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC)}
  
\NormalTok{  samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                     \AttributeTok{niter =} \DecValTok{5000}\NormalTok{, }
                     \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{,}
                     \AttributeTok{setSeed =}\NormalTok{ seed)}
  
  \FunctionTok{return}\NormalTok{(samples)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we run the code using \texttt{parLapply()}, which uses cluster nodes to execute our workflow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{output }\OtherTok{\textless{}{-}} \FunctionTok{parLapply}\NormalTok{(}\AttributeTok{cl =}\NormalTok{ my\_cluster, }
                    \AttributeTok{X =} \FunctionTok{c}\NormalTok{(}\DecValTok{2022}\NormalTok{, }\DecValTok{666}\NormalTok{),}
                    \AttributeTok{fun =}\NormalTok{ workflow, }
                    \AttributeTok{data =} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

In the call to \texttt{parLapply}, we specify \texttt{X\ =\ c(2022,\ 666)} to ensure reproducibility. We use two alues 2022 and 666 to set the seed in \texttt{workflow()}, which means we run two instances of our workflow, or two MCMC chains. Note that we also have a line \texttt{set.seed(123)} in the \texttt{workflow()} function to ensure reproducibility while drawing randomly initial values.

It's good practice to close the cluster with \texttt{stopCluster()} so that processes do not continue to run in the background and slow down other processes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stopCluster}\NormalTok{(my\_cluster)}
\end{Highlighting}
\end{Shaded}

By inspecting the results, you can see that the object \texttt{output} is a list with two components, one for each MCMC chain:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(output)}
\DocumentationTok{\#\# List of 2}
\DocumentationTok{\#\#  $ : num [1:4000, 1] 0.393 0.369 0.346 0.346 0.346 ...}
\DocumentationTok{\#\#   ..{-} attr(*, "dimnames")=List of 2}
\DocumentationTok{\#\#   .. ..$ : NULL}
\DocumentationTok{\#\#   .. ..$ : chr "theta"}
\DocumentationTok{\#\#  $ : num [1:4000, 1] 0.435 0.435 0.435 0.435 0.243 ...}
\DocumentationTok{\#\#   ..{-} attr(*, "dimnames")=List of 2}
\DocumentationTok{\#\#   .. ..$ : NULL}
\DocumentationTok{\#\#   .. ..$ : chr "theta"}
\end{Highlighting}
\end{Shaded}

Eventually, you can obtain numerical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(output)}
\DocumentationTok{\#\#         mean      sd   2.5\%    50\%  97.5\% Rhat n.eff}
\DocumentationTok{\#\# theta 0.3361 0.06148 0.2215 0.3335 0.4594    1  1779}
\end{Highlighting}
\end{Shaded}

\subsection{Incomplete and incorrect initialization}\label{incomplete-and-incorrect-initialization}

When you run \texttt{nimbleMCMC()} or \texttt{nimbleModel()}, you may get warnings thrown at you by NIMBLE like `This model is not fully initialized' or `value is NA or NaN even after trying to calculate'. This is not necessarily an error, but it `reflects missing values in model variables' (incomplete initialization). In this situation, NIMBLE will initialize nodes with NAs by drawing from priors, and it will work or not. When possible, I try to initialize all nodes (full initialization). The process can be a bit of a headache, but it helps understanding the model structure better. Going back to our animal survival example, let's purposedly forget to provide an initial value for \texttt{theta}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# likelihood}
\NormalTok{  survived }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbinom}\NormalTok{(theta, released)}
  \CommentTok{\# prior}
\NormalTok{  theta }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{\})}
\CommentTok{\#initial.values \textless{}{-} list(theta = runif(1,0,1))}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ model, }
                        \AttributeTok{data =} \FunctionTok{list}\NormalTok{(}\AttributeTok{survived =} \DecValTok{19}\NormalTok{, }\AttributeTok{released =} \DecValTok{57}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To see which variables are not initialized, we use \texttt{initializeInfo()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# survival$calculate() \# gives NA}
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{initializeInfo}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now that we know \texttt{theta} was not initialized, we can fix the issue and resume our workflow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survival}\SpecialCharTok{$}\NormalTok{theta }\OtherTok{\textless{}{-}} \FloatTok{0.5} \CommentTok{\# assign initial value to theta}
\NormalTok{survival}\SpecialCharTok{$}\FunctionTok{calculate}\NormalTok{() }
\DocumentationTok{\#\# [1] {-}5.422}

\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survival)}
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(Csurvival)}
\DocumentationTok{\#\# ===== Monitors =====}
\DocumentationTok{\#\# thin = 1: theta}
\DocumentationTok{\#\# ===== Samplers =====}
\DocumentationTok{\#\# RW sampler (1)}
\DocumentationTok{\#\#   {-} theta}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC)}

\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =} \DecValTok{5000}\NormalTok{, }
                   \AttributeTok{nburnin =} \DecValTok{1000}\NormalTok{)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}

\FunctionTok{samplesSummary}\NormalTok{(samples)}
\DocumentationTok{\#\#         Mean Median St.Dev. 95\%CI\_low 95\%CI\_upp}
\DocumentationTok{\#\# theta 0.3402  0.338 0.06025    0.2277    0.4608}
\end{Highlighting}
\end{Shaded}

When working with larger models, it can happen that NIMBLE's internal simulation produces initial values for different parameters (nodes) that are incompatible with each other and violate certain model assumptions. If we go ahead and run the MCMC on a model where this is the case, a range of different warning messages may appear to indicate the problem. At first, they may not seem very intuitive (e.g.~``Log probability is -Inf'', ``Log probability is NaN'', ``Slice sampler reached the maximum number of contractions'', etc.), but they are signals that you may want to double-check our initialization.

\subsection{Vectorization}\label{vectorization}

Vectorization is the process of replacing a loop by a vector so that instead of processing a single value at a time, you process a set of values at once. As an example, instead of writing:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n)\{ }
\NormalTok{  x[i] }\OtherTok{\textless{}{-}}\NormalTok{ mu }\SpecialCharTok{+}\NormalTok{ epsilon[i] }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

you would write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n] }\OtherTok{\textless{}{-}}\NormalTok{ mu }\SpecialCharTok{+}\NormalTok{ epsilon[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n]}
\end{Highlighting}
\end{Shaded}

Vectorization can make your code more efficient by manipulating one vector node \texttt{x{[}1:n{]}} instead of \texttt{n} nodes \texttt{x{[}1{]}}, \ldots, \texttt{x{[}n{]}}. As an example, you may have a look to the vectorized flavor of the binomial distribution written by Pierre Dupont at \url{https://github.com/nimble-dev/nimbleSCR/blob/master/nimbleSCR/R/dbinom_vector.R}. Note that per now, vectorization only works for deterministic nodes (relationships with \texttt{\textless{}-}) but not stochastic ones (relationships with \texttt{\textasciitilde{}}).

\section{Summary}\label{summary-1}

\begin{itemize}
\item
  NIMBLE is an R package that implements for you MCMC algorithms to generate samples from the posterior distribution of model parameters. You only have to specify a likelihood and priors using the BUGS language to apply the Bayes theorem.
\item
  NIMBLE is more than just another MCMC engine. It provides a programming environment so that you have full control when building models and estimating parameters.
\item
  At the core of NIMBLE are \texttt{nimbleFunctions} which you can write and compile for faster computation. With \texttt{nimbleFunctions} you can mimic basic R syntax, work with vectors and matrices, use logical operators and flow control, and specify many distributions.
\item
  There are two workflows to run NIMBLE. In most situations, \texttt{nimbleMCMC()} will serve you well. When you need more control, you can adopt a detailed workflow with \texttt{nimbleModel()}, \texttt{configureMCMC()}, \texttt{buildMCMC()}, \texttt{compileNimble()} and \texttt{runMCMC()}.
\item
  By having full control of the workflow, you can change default MCMC samplers and even write your own samplers.
\end{itemize}

\section{Suggested reading}\label{suggested-reading-1}

In this chapter, I have only scratched the surface of what NIMBLE is capable of. Below is a list of pointers that should help you going further with NIMBLE.

\begin{itemize}
\item
  The NIMBLE folks make a lot of useful resources available through the official website \url{https://r-nimble.org}.
\item
  The NIMBLE manual \url{https://r-nimble.org/html_manual/cha-welcome-nimble.html} reads like a book with clear explanations and relevant examples.
\item
  You can learn a lot by going through examples at \url{https://r-nimble.org/examples} and training material from NIMBLE workshops at \url{https://github.com/nimble-training}.
\item
  You can keep the NIMBLE cheatsheet \url{https://r-nimble.org/cheatsheets/NimbleCheatSheet.pdf} near you to remind yourself of the workflow, how to write and use models, or which functions and distributions are available.
\item
  If you have questions, feel free to get in touch with the community of NIMBLE users by emailing the discussion group \url{https://groups.google.com/forum/\#!forum/nimble-users}. This is a great place to learn, and folks who take the time to answer questions are kind and provide constructive answers. When possible, make sure to provide a reproducible example illustrating your problem.
\item
  You can cite the following reference when using NIMBLE in a publication:
\end{itemize}

\begin{quote}
de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. Temple Lang, and R. Bodik (2017). Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE. \emph{Journal of Computational and Graphical Statistics} \textbf{26} (2): 403--13.
\end{quote}

\begin{itemize}
\tightlist
\item
  Last, the packages to process NIMBLE results are developed by people whose work should be acknowledged: see \citet{youngflesh2018mcmcvis} for \texttt{MCMCvis}, \citet{turek2022basicmcmcplots} for \texttt{basicMCMCplots}, \citet{gabry2022bayesplot} for \texttt{bayesplot} and \citet{fernandez2016ggmcmc} for \texttt{ggmcmc}.
\end{itemize}

\chapter{Hidden Markov models}\label{hmmcapturerecapture}

\section{Introduction}\label{introduction-3}

In this third chapter, you will learn the basics on Markov models and how to fit them to longitudinal data using NIMBLE. In real life however, individuals may go undetected and their status be unknown. You will also learn how to manipulate the extension of Markov models to hidden states, so-called hidden Markov models.

\section{Longitudinal data}\label{longitudinal-data}

Let's get back to our survival example, and denote \(z_i\) the state of individual \(i\) with \(z_i = 1\) if alive and \(z_i = 0\) if dead. We have a total of \(z = \displaystyle{\sum_{i=1}^{n}{z_i}}\) survivors out of \(n\) released animals with winter survival probability \(\phi\). Our model so far is a combination of a binomial likelihood and a Beta prior with parameters 1 and 1, which is also a uniform distribution between 0 and 1. It can be written as:

\begin{align*}
   z &\sim \text{Binomial}(n, \phi) &\text{[likelihood]}
   \\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}

Because the binomial distribution is just a sum of independent Bernoulli outcomes, you can rewrite this model as:

\begin{align*}
   z_i &\sim \text{Bernoulli}(\phi), \; i = 1, \ldots, N &\text{[likelihood]}
   \\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}

It is like flipping a coin for each individual and get a survivor with probability \(\phi\).

In this set up, we consider a single winter. But for many species, we need to collect data on the long term to get a representative estimate of survival. Therefore what if we had say \(T = 5\) winters?

Let us denote \(z_{i,t} = 1\) if individual \(i\) alive at winter \(t\), and \(z_{i,t} = 2\) if dead. Then longitudinal data look like in the table below. Each row is an individual \(i\), and columns are for winters \(t\), or sampling occasions. Variable \(z\) is indexed by both \(i\) and \(t\), and takes value 1 if individual \(i\) is alive in winter \(t\), and 2 otherwise.

\begin{verbatim}
## # A tibble: 57 x 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    <int>      <int>      <int>      <int>      <int>
##  1     1          1          1          1          1
##  2     2          1          1          1          1
##  3     3          1          1          1          1
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          1          2          2
##  7     7          1          1          1          1
##  8     8          1          2          2          2
##  9     9          1          1          1          1
## 10    10          1          2          2          2
## # i 47 more rows
## # i 1 more variable: `winter 5` <int>
\end{verbatim}

\section{A Markov model for longitudinal data}\label{a-markov-model-for-longitudinal-data}

Let's think of a model for these data. The objective remains the same, estimating survival. To build this model, we'll make assumptions, go through its components and write down its likelihood. Note that we have already encountered Markov models in Section \ref{markovmodelmcmc}.

\subsection{Assumptions}\label{assumptions}

First, we assume that the state of an animal in a given winter, alive or dead, is only dependent on its state the winter before. In other words, the future depends only on the present, not the past. This is a Markov process.

Second, if an animal is alive in a given winter, the probability it survives to the next winter is \(\phi\). The probability it dies is \(1 - \phi\).

Third, if an animal is dead a winter, it remains dead, unless you believe in zombies.

Our Markov process can be represented this way:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-135-1.pdf}}

An example of this Markov process is, for example:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-136-1.pdf}}

Here the animal remains alive over the first two time intervals \((z_{i,1} = z_{i,2} = z_{i,3} = 1)\) with probability \(\phi\) until it dies over the fourth time interval \((z_{i,4} = 2)\) with probability \(1-\phi\) then remains dead from then onwards \((z_{i,5} = 2)\) with probability 1.

\subsection{Transition matrix}\label{transition-matrix}

You might have figured it out already (if not, not a problem), the core of our Markov process is made of transition probabilities between states alive and dead. For example, the probability of transitioning from state alive at \(t-1\) to state alive at \(t\) is \(\Pr(z_{i,t} = 1 | z_{i,t-1} = 1) = \gamma_{1,1}\). It is the survival probability \(\phi\). The probability of dying over the interval \((t-1, t)\) is \(\Pr(z_{i,t} = 2 | z_{i,t-1} = 1) = \gamma_{1,2} = 1 - \phi\). Now if an animal is dead at \(t-1\), then \(\Pr(z_t = 1 | z_{t-1} = 2) = 0\) and \(\Pr(z_{i,t} = 2 | z_{i,t-1} = 2) = 1\).

We can gather these probabilities of transition between states from one occasion to the next in a matrix, say \(\mathbf{\Gamma}\), which we will call the transition matrix:

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\gamma_{1,1} & \gamma_{1,2}\\
\gamma_{2,1} & \gamma_{2,2}
\end{array}\right) =
\left(\begin{array}{cc}
\phi & 1 - \phi\\
0 & 1
\end{array}\right)
\end{align*}

To try and remember that the states at \(t-1\) are in rows, and the states at \(t\) are in columns, I will often write:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=1 & z_t=2 \\ \hdashline
\phi & 1-\phi \\
0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=1 \; \mbox{(alive)} \\ z_{t-1}=2 \; \mbox{(dead)}
    \end{matrix}
\end{matrix}\]

Take the time you need to navigate through this matrix, and get familiar with it. For example, you may start alive at \(t\) (first row) then end up dead at \(t+1\) (first column) with probability \(1-\phi\).

\subsection{Initial states}\label{initial-states}

A Markov process has to start somewhere. We need the probabilities of initial states, i.e.~the states of an individual at \(t = 1\). We will gather the probability of being in each state (alive or 1 and dead or 2) in the first winter in a vector. We will use \(\mathbf{\delta} = \left(\Pr(z_{i,1} = 1), \Pr(z_{i,1} = 2)\right)\). For simplicity, we will assume that all individuals are marked and released in the first winter, hence alive when first captured, which means that they are all in state alive or 1 for sure. Therefore we have \(\mathbf{\delta} = \left(1, 0\right)\).

\subsection{Likelihood}\label{likelihood}

Now that we have built a Markov model, we need its likelihood to apply the Bayes theorem. The likelihood is the probability of the data, given the model. Here the data are the \(z\), therefore we need \(\Pr(\mathbf{z}) = \Pr(z_1, z_2, \ldots, z_{T-2}, z_{T-1}, z_T)\).

We're gonna work backward, starting from the last sampling occasion. Using conditional probabilities, the likelihood can be written as the product of the probability of \(z_T\) i.e.~you're alive or not on the last occasion given your past history, that is the states at previous occasions, times the probability of your past history:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \color{blue}{\Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1)} \\
\end{align*}

Then because we have a Markov model, we're memory less, that is the probabilty of next state, here \(z_T\), depends only on the current state, that is \(z_{T-1}\), and not the previous states:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \color{blue}{\Pr(z_T | z_{T-1})} \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
\end{align*}

You can apply the same reasoning to \(T-1\). First use conditional probabilities:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \color{blue}{\Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
\end{align*}

Then apply the Markovian property:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \color{blue}{\Pr(z_{T-1} | z_{T-2})} \Pr(z_{T-2}, \ldots, z_1)\\
\end{align*}

And so on up to \(z_2\). You end up with this expression for the likelihood:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \ldots \\
                &= \color{blue}{\Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \ldots \Pr(z_{2} | z_{1}) \Pr(z_{1})}\\
\end{align*}

This is a product of conditional probabilities of states given previous states, and the probability of initial states \(\Pr(z_1)\). Using a more compact notation for the product of conditional probabilities, we get:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_T | z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1}, z_{T-2},\ldots, z_1) \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}, \ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \Pr(z_{T-2}, \ldots, z_1)\\
                &= \ldots \\
                &= \Pr(z_T | z_{T-1}) \Pr(z_{T-1} | z_{T-2}) \ldots \Pr(z_{2} | z_{1}) \Pr(z_{1})\\
                &= \color{blue}{\Pr(z_{1}) \prod_{t=2}^T{\Pr(z_{t} | z_{t-1})}}\\
\end{align*}

In the product, you can recognize the transition parameters \(\gamma\) we defined above, so that the likelihood of a Markov model can be written as:

\begin{align*}
\Pr(\mathbf{z}) &= \Pr(z_T, z_{T-1}, z_{T-2}, \ldots, z_1) \color{white}{\Pr(z_{T-1}, z_{T-2},\ldots, z_1) \Pr(z_{T-2}, \ldots, z_1)}\\
                &= \Pr(z_{1}) \prod_{t=2}^T{\gamma_{z_{t-1},z_{t}}}\\
\end{align*}

\subsection{Example}\label{example}

I realise these calculations are a bit difficult to follow. Let's take an example to fix ideas. Let's assume an animal is alive, alive at time 2 then dies at time 3. We have \(\mathbf{z} = (1, 1, 2)\). What is the contribution of this animal to the likelihood? Let's apply the formula we just derived:

\begin{align*}
\Pr(\mathbf{z} = (1, 1, 2)) &= \Pr(z_1 = 1) \; \gamma_{z_{1} = 1, z_{2} = 1} \; \gamma_{z_{2} = 1, z_{3} = 2}\\
                            &= 1 \; \phi \; (1 - \phi).
\end{align*}

The probability of having the sequence alive, alive and dead is the probability of being alive first, then to stay alive, eventually to die. The probability of being alive at first occasion being 1, we have that the contribution of this individual to the likelihood is \(\phi (1 - \phi)\).

\section{Bayesian formulation}\label{bayesian-formulation}

Before implementing this model in NIMBLE, we provide a Bayesian formulation of our model. We first note that the likelihood is a product of conditional probabilities of binary events (alive or dead). Usually binary events are associated with the Bernoulli distribution. Here however, we will use its extension to several outcomes (from a coin with two sides to a dice with more than two faces) known as the categorical distribution. The categorical distribution is a multinomial distribution with a single draw. To get a better idea of how the categorical distribution works, let's simulate from it with the \texttt{rcat()} function. Consider for example a random value drawn from a categorical distribution with probability 0.1, 0.3 and 0.6. Think of a dice with three faces, face 1 has probability 0.1 of occurring, face 2 probability 0.3 and face 3 has probability 0.6, the sum of these probabilities being 1. We expect to get a 3 more often than a 2 and rarely a 1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.6}\NormalTok{))}
\DocumentationTok{\#\# [1] 3}
\end{Highlighting}
\end{Shaded}

Alternatively, you can use the \texttt{sample()} function and \texttt{sample(x\ =\ 1:3,\ size\ =\ 1,\ replace\ =\ FALSE,\ prob\ =\ c(0.1,\ 0.3,\ 0.6))}. Here is another example in which we sample 20 times in a categorical distribution with probabilities 0.1, 0.1, 0.4, 0.2 and 0.2, hence a dice with 5 faces:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{20}\NormalTok{, }\AttributeTok{prob =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{))}
\DocumentationTok{\#\#  [1] 5 3 4 5 4 3 4 2 2 3 1 3 5 5 3 4 2 3 2 3}
\end{Highlighting}
\end{Shaded}

In this chapter, you will familiarise yourself with the categorical distribution in binary situations, which should make the transition to more states than just alive and dead smoother in the next chapters.

Initial state is a categorical random variable with probability \(\delta\). That is you have a dice with two faces, or a coin, and you have some probability to be alive, and one minus that probability to be dead. Of course, it you want your Markov chain to start, you'd better say it's alive so that \(\delta\) is just \((1,0)\):

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
\end{align*}

Now the main part is the dynamic of the states. The state \(z_t\) at \(t\) depends only on the known state \(z_{t-1}\) at \(t-1\), and is a categorical random variable which probabilities are given by row \(z_{t-1}\) of the transition matrix \(\mathbf{\Gamma} = \gamma_{z_{t-1},z_{t}}\):

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(\gamma_{z_{t-1},z_{t}}) &\text{[likelihood, }t > 1 \text{]}\\
\end{align*}

For example, if individual \(i\) is alive over \((t-1,t)\) i.e.~\(z_{t-1} = 1\), we need the first row in \(\mathbf{\Gamma}\),

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\color{blue}{\phi} & \color{blue}{1 - \phi}\\
0 & 1
\end{array}\right)
\end{align*}

that is \(\color{blue}{\gamma_{z_{t-1} = 1,z_{t}} = (\phi, 1-\phi)}\) and \(z_t | z_{t-1} = 1 \sim \text{Categorical}((\phi, 1-\phi))\).

Otherwise, if individual \(i\) dies over \((t-1,t)\) i.e.~\(z_{t-1} = 2\), we need the second row in \(\mathbf{\Gamma}\):

\begin{align*}
\mathbf{\Gamma} =
\left(\begin{array}{cc}
\phi & 1 - \phi\\
\color{blue}{0} & \color{blue}{1}
\end{array}\right)
\end{align*}

that is \(\color{blue}{\gamma_{z_{t-1} = 2,z_{t}} = (0, 1)}\) and \(z_t | z_{t-1} = 2 \sim \text{Categorical}((0, 1))\) (if the individual is dead, it remains dead with probability 1).

We also need a prior on survival. Without surprise, we will use a uniform distribution between 0 and 1, which is also a Beta distribution with parameters 1 and 1. Overall our model is:

\begin{align*}
   z_1 &\sim \text{Categorical}(\delta) &\text{[likelihood, }t = 1 \text{]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(\gamma_{z_{t-1},z_{t}}) &\text{[likelihood, }t > 1 \text{]}\\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
\end{align*}

\section{NIMBLE implementation}\label{nimble-implementation}

How to implement in NIMBLE the Markov model we just built? We need to put in place a few bricks before running our model. Let's start with the prior on survival, the vector of initial state probabilities and the transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{markov.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Alternatively, you can define vectors and matrices in NIMBLE like you would do it in R. You can write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{markov.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior}
\NormalTok{  delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{) }\CommentTok{\# vector of initial state probabilities}
\NormalTok{  gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{( }\FunctionTok{c}\NormalTok{(phi, }\DecValTok{0}\NormalTok{, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{) }\CommentTok{\# transition matrix}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Now there are two important dimensions to our model, along which we need to repeat tasks, namely individual and time. As for time, we describe the successive events of survival using the categorical distribution \texttt{dcat()}, say for individual \(i\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])           }\CommentTok{\# t = 1}
\NormalTok{z[i,}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,}\DecValTok{1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])   }\CommentTok{\# t = 2}
\NormalTok{z[i,}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,}\DecValTok{2}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])   }\CommentTok{\# t = 3}
\NormalTok{...}
\NormalTok{z[i,T] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,T}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = T}
\end{Highlighting}
\end{Shaded}

There is a more efficient way to write this piece of code by using a for loop, that is a sequence of instructions that we repeat. Here, we condense the previous code into:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])             }\CommentTok{\# t = 1}
\ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{ }\CommentTok{\# loop over time t}
\NormalTok{  z[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,t}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = 2,...,T}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we just need to do the same for all individuals. We use another loop:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{ }\CommentTok{\# loop over individual i}
\NormalTok{  z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = 1}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{ }\CommentTok{\# loop over time t}
\NormalTok{    z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = 2,...,T}
\NormalTok{  \} }\CommentTok{\# t}
\NormalTok{\} }\CommentTok{\# i}
\end{Highlighting}
\end{Shaded}

Puting everything together, the NIMBLE code for our Markov model is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{markov.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{ }\CommentTok{\# loop over individual i}
\NormalTok{    z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = 1}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{ }\CommentTok{\# loop over time t}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# t = 2,...,T}
\NormalTok{    \} }\CommentTok{\# t}
\NormalTok{  \} }\CommentTok{\# i}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Note that in this example, \(\delta\) is used as a placeholder for more complex models we will build in chapters to come. Here, you could simply write \texttt{z{[}i,1{]}\ \textless{}-\ 1}.

Now we're ready to resume our NIMBLE workflow. First we read in data. Because we have loops and indices that do not change, we use constants as explained in Section \ref{start-nimble}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \DecValTok{57}\NormalTok{, }\AttributeTok{T =} \DecValTok{5}\NormalTok{)}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{z =}\NormalTok{ z)}
\end{Highlighting}
\end{Shaded}

We also specify initial values for survival with a function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\FunctionTok{initial.values}\NormalTok{()}
\DocumentationTok{\#\# $phi}
\DocumentationTok{\#\# [1] 0.1265}
\end{Highlighting}
\end{Shaded}

There is a single parameter to monitor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{)}
\NormalTok{parameters.to.save}
\DocumentationTok{\#\# [1] "phi"}
\end{Highlighting}
\end{Shaded}

We run 2 chains with 5000 iterations including 1000 iterations as burnin:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

Let's run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ markov.survival,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

Let's calculate the usual posterior numerical summaries for survival:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# phi 0.79 0.03 0.73 0.79  0.85    1  1755}
\end{Highlighting}
\end{Shaded}

Posterior mean and median are close to \(0.8\). This is fortunate since the data was simulated with (actual) survival \(\phi = 0.8\). The code I used was:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1 = alive, 2 = dead}
\NormalTok{nind }\OtherTok{\textless{}{-}} \DecValTok{57}
\NormalTok{nocc }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{phi }\OtherTok{\textless{}{-}} \FloatTok{0.8} \CommentTok{\# survival probability}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{) }\CommentTok{\# (Pr(alive at t = 1), Pr(dead at t = 1))}
\NormalTok{Gamma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{) }\CommentTok{\# transition matrix}
\NormalTok{Gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{Gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{Gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{Gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ nind, }\AttributeTok{ncol =}\NormalTok{ nocc)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2022}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nind)\{}
\NormalTok{  z[i,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ delta) }\CommentTok{\# 1 for sure}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{nocc)\{}
\NormalTok{    z[i,t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ Gamma[z[i,t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\NormalTok{  \}}
\NormalTok{\}}
\FunctionTok{head}\NormalTok{(z) }
\DocumentationTok{\#\#      [,1] [,2] [,3] [,4] [,5]}
\DocumentationTok{\#\# [1,]    1    1    1    1    1}
\DocumentationTok{\#\# [2,]    1    1    1    1    1}
\DocumentationTok{\#\# [3,]    1    1    1    1    1}
\DocumentationTok{\#\# [4,]    1    1    1    1    2}
\DocumentationTok{\#\# [5,]    1    1    1    1    1}
\DocumentationTok{\#\# [6,]    1    1    2    2    2}
\end{Highlighting}
\end{Shaded}

We could replace \texttt{dcat()} by \texttt{dbern()} everywhere in the code because we have binary events alive/dead. Would it make any difference? Although \texttt{dcat()} uses less efficient samplers than \texttt{dbern()}, \texttt{dcat()} is convenient for model building to accomodate more than two outcomes, a feature that will become handy in the next chapters.

\section{Hidden Markov models}\label{hidden-markov-models}

\subsection{Capture-recapture data}\label{capturerecapturedata}

\begin{blackbox}
Unfortunately, the data with alive and dead states is the data we wish we had. In real life, animals cannot be monitored exhaustively, like humans in a medical trial. This is why we use capture-recapture protocols, in which animals are captured, individually marked, and released alive. Then, these animals may be detected again, or go undetected. Whenever animals go undetected, it might be that they were alive but missed, or because they were dead and therefore could not be detected. This issue is usually referred to as that of imperfect detection. As a consequence of imperfect detection, the Markov process for survival is only partially observed: You know an animal is alive when you detect it, but when an animal goes undetected, whether it is alive or dead is unknown to you. This is where hidden Markov models (HMMs) come in.

\end{blackbox}

Let's get back to the data we had in the previous section. The truth is in \(z\) which contains the fate of all individuals with \(z = 1\) for alive, and \(z = 2\) for dead:

\begin{verbatim}
## # A tibble: 57 x 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    <int>      <int>      <int>      <int>      <int>
##  1     1          1          1          1          1
##  2     2          1          1          1          1
##  3     3          1          1          1          1
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          1          2          2
##  7     7          1          1          1          1
##  8     8          1          2          2          2
##  9     9          1          1          1          1
## 10    10          1          2          2          2
## # i 47 more rows
## # i 1 more variable: `winter 5` <int>
\end{verbatim}

Unfortunately, we have only partial access to \(z\). What we do observe is \(y\) the detections and non-detections. How are \(z\) and \(y\) connected?

The easiest connection is with dead animals which go undetected for sure. Therefore when an animal is dead i.e.~\(z = 2\), it cannot be detected, therefore \(y = 0\):

\begin{verbatim}
## # A tibble: 57 x 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    <int>      <int>      <int>      <int>      <int>
##  1     1          1          1          1          1
##  2     2          1          1          1          1
##  3     3          1          1          1          1
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          1          0          0
##  7     7          1          1          1          1
##  8     8          1          0          0          0
##  9     9          1          1          1          1
## 10    10          1          0          0          0
## # i 47 more rows
## # i 1 more variable: `winter 5` <int>
\end{verbatim}

Now alive animals may be detected or not. If an animal is alive \(z = 1\), it is detected \(y = 1\) with probability \(p\) or not \(y = 0\) with probability \(1-p\). In our example, first detection coincides with first winter for all individuals.

\begin{verbatim}
## # A tibble: 57 x 6
##       id `winter 1` `winter 2` `winter 3` `winter 4`
##    <int>      <dbl>      <dbl>      <dbl>      <dbl>
##  1     1          1          0          0          0
##  2     2          1          0          1          0
##  3     3          1          0          0          0
##  4     4          1          1          1          1
##  5     5          1          1          1          1
##  6     6          1          0          0          0
##  7     7          1          0          1          1
##  8     8          1          1          1          1
##  9     9          1          1          1          1
## 10    10          1          1          0          0
## # i 47 more rows
## # i 1 more variable: `winter 5` <dbl>
\end{verbatim}

Compare with the previous \(z\) table. Some 1's for alive have become 0's for non-detection, other 1's for alive have remained 1's for detection. This \(y\) table is what we observe in real life. I hope I have convinced you that to make the connection between observations, the \(y\), and true states, the \(z\), we need to describe how observations are made (or emitted in the HMM terminology) from the states.

\subsection{Observation matrix}\label{observation-matrix}

The novelty in HMMs is the link between observations and states. This link is made through observation probabilities. For example, the probability of detecting an animal \(i\) at \(t\) given it is alive at \(t\) is \(\Pr(y_{i,t}=2|z_{i,t}=1)=\omega_{1,2}\). It is the detection probability \(p\). If individual \(i\) is dead at \(t\), then it is missed for sure, and \(\Pr(y_{i,t}=1|z_{i,t}=2)=\omega_{2,1}=1\).

We can gather these observation probabilities into an observation matrix \(\mathbf{\Omega}\). In rows we have the states alive \(z = 1\) and dead \(z = 2\), while in columns we have the observations non-detected \(y = 1\) and detected \(y = 2\) (previously coded 0 and 1 respectively):

\begin{align*}
\mathbf{\Omega} =
\left(\begin{array}{cc}
\omega_{1,1} & \omega_{1,2}\\
\omega_{2,1} & \omega_{2,2}
\end{array}\right) =
\left(\begin{array}{cc}
1 - p & p\\
1 & 0
\end{array}\right)
\end{align*}

In survival models we will use throughout this book, we condition the fate of individuals on first detection, which boils down to set the corresponding detection probability to 1.

The observation matrix is:

\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 \\
    \mbox{(non-detected)} & \mbox{(detected)} \\ \hdashline
1 - p & p\\
1 & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t}=1 \; \mbox{(alive)}\\ z_{t}=2 \; \mbox{(dead)}
    \end{matrix}
\end{matrix}\]

\subsection{Hidden Markov model}\label{hidden-markov-model}

Our hidden Markov model can be represented this way:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-156-1.pdf}}

States \(z\) are in gray. Observations \(y\) are in white. All individuals are first captured in the first winter \(t = 1\), and are therefore all alive \(z_1 = 1\) and detected \(y_1 = 2\).

\begin{blackbox}
A hidden Markov model is just two time series running in parallel. One for the states with the Markovian property, and the other of for the observations generated from the states. HMM are a special case of state-space models in which latent states are discrete.

\end{blackbox}

Have a look to the example below, in which an individual is detected at first sampling occasion, detected again, then missed for the rest of the study. While on occasion \(t=3\) that individual was alive \(z_3=1\) and went undetected \(y_3=1\), on occasions \(t=4\) and \(t=5\) it went undetected \(y_4=y_5=1\) because it was dead \(z_4=z_5=2\). Because we condition on first detection, the link between state and observation at \(t=1\) is deterministic and \(p = 1\).

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-157-1.pdf}}

\subsection{Likelihood}\label{likelihoodhmm}

In the Bayesian framework, we usually work with the so-called complete likelihood, that is the probability of the observed data \(y\) and the latent states \(z\) given the parameters of our model, here the survival and detection probabilities \(\phi\) and \(p\). The complete likelihood for individual \(i\) is:

\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
\end{align*}

Using the definition of a conditional probability, we have:

\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &= \color{blue}{\Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})}\\
\end{align*}

Then by using the independence of the \(y\) conditional on the \(z\), and the likelihood of a Markov chain, we get that:

\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                &= \color{blue}{\left(\prod_{t=1}^T{\Pr{(y_{i,t} | z_{i,t})}}\right) \left(\Pr(z_{i,1}) \prod_{t=2}^T{\Pr{(z_{i,t} | z_{i,t-1})}}\right)}\\
\end{align*}

Finally, by recognizing the observation and transition probabilities, we have that the complete likelihood for individual \(i\) is:

\begin{align*}
\Pr(\mathbf{y}_i, \mathbf{z}_i) &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                  &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T} | z_{i,1}, z_{i,2}, \ldots, z_{i,T}) \Pr(z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
                &= \color{blue}{\left(\prod_{t=1}^T{\omega_{z_{i,t}, y_{i,t}}}\right) \left(\Pr(z_{i,1}) \prod_{t=2}^T{\gamma_{z_{i,t-1},z_{i,t}}}\right)}\\
\end{align*}

To obtain the complete likelihood of the whole dataset, we need to multiply this individual likelihood for each animal \(\displaystyle{\prod_{i=1}^N{\Pr(\mathbf{y}_i,\mathbf{z}_i)}}\). When several individuals have the same contribution, calculating their individual contribution only once can greatly reduce the computational burden, as illustrated in Section \ref{pooled-likelihood}.

The Bayesian approach with MCMC methods allows treating the latent states \(z_{i,t}\) as if they were parameters, and to be estimated as such. However, the likelihood is rather complex with a large number of latent states \(z_{i,t}\), which comes with computational costs and slow mixing. There are situations where the latent states are the focus of ecological inference and need to be estimated (see Suggested reading below). However, if not needed, you might want to get rid of the latent states and rely on the so-called marginal likelihood. By doing so, you can avoid sampling the latent states, focus on the ecological parameters, and often speeds up computations and improves mixing as shown in Section \ref{marginalization}. Actually, you can even estimate the latent states afterwards, as illustrated in Section \ref{decoding}.

\section{Fitting HMM with NIMBLE}\label{fittinghmmnimble}

If we denote \emph{first} the time of first detection, then our model so far is written as follows:

\begin{align*}
   z_{\text{first}} &\sim \text{Categorical}(1, \delta) &\text{[likelihood]}\\
   z_t | z_{t-1} &\sim \text{Categorical}(1, \gamma_{z_{t-1},z_{t}}) &\text{[likelihood, t>first]}\\
   y_t | z_{t} &\sim \text{Categorical}(1, \omega_{z_{t}}) &\text{[likelihood, t>first]}\\
  \phi &\sim \text{Beta}(1, 1) &\text{[prior for }\phi \text{]} \\
  p &\sim \text{Beta}(1, 1) &\text{[prior for }p \text{]} \\
\end{align*}

It has an observation layer for the \(y\)'s, conditional on the \(z\)'s. We also consider uniform priors for the detection and survival probabilities. How to implement this model in NIMBLE?

We start with priors for survival and detection probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then we define initial states, transition and observation matrices:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
  \CommentTok{\# parameters}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = first) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = first) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then the likelihood:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
    \CommentTok{\# likelihood}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\ErrorTok{\})}
\end{Highlighting}
\end{Shaded}

The loop over time for each individual \texttt{for\ (j\ in\ 2:T)\{\}} starts after the first time individuals are detected (this is time 2 for all of them here), because we work conditional on the first detection.

Overall, the code looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
  \CommentTok{\# likelihood}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = first) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = first) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Now we specify the constants:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{T =} \DecValTok{5}\NormalTok{)}
\NormalTok{my.constants}
\DocumentationTok{\#\# $N}
\DocumentationTok{\#\# [1] 57}
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $T}
\DocumentationTok{\#\# [1] 5}
\end{Highlighting}
\end{Shaded}

The data are made of 0's for non-detections and 1's for detections. To use the categorical distribution, we need to code 1's and 2's. We simply add 1 to get the correct format, that is \(y = 1\) for non-detection and \(y = 2\) for detection:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's write a function for the initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# non{-}detection {-}\textgreater{} alive}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# dead {-}\textgreater{} alive}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

As initial values for the latent states, we assumed that whenever an individual was non-detected, it was alive, with with \texttt{zinits\ \textless{}-\ y\ +\ 1}, and we make sure dead individuals are alive with \texttt{zinits{[}zinits\ ==\ 2{]}\ \textless{}-\ 1}.

We specify the parameters we'd like to monitor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\NormalTok{parameters.to.save}
\DocumentationTok{\#\# [1] "phi" "p"}
\end{Highlighting}
\end{Shaded}

We provide MCMC details:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

At last, we're ready to run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.survival,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{end\_time }\SpecialCharTok{{-}}\NormalTok{ start\_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 27.68 secs
\end{verbatim}

We can have a look to numerical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.61 0.06 0.50 0.61  0.72    1   740}
\DocumentationTok{\#\# phi 0.75 0.04 0.67 0.75  0.83    1   805}
\end{Highlighting}
\end{Shaded}

The estimates for survival and detection are close to true survival \(\phi = 0.8\) and detection \(p = 0.6\) with which we simulated the data. The code I used is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2022}\NormalTok{) }\CommentTok{\# for reproducibility}
\NormalTok{nocc }\OtherTok{\textless{}{-}} \DecValTok{5} \CommentTok{\# nb of winters or sampling occasions}
\NormalTok{nind }\OtherTok{\textless{}{-}} \DecValTok{57} \CommentTok{\# nb of animals}
\NormalTok{p }\OtherTok{\textless{}{-}} \FloatTok{0.6} \CommentTok{\# detection prob}
\NormalTok{phi }\OtherTok{\textless{}{-}} \FloatTok{0.8} \CommentTok{\# survival prob}
\CommentTok{\# Vector of initial states probabilities}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{) }\CommentTok{\# all individuals are alive in first winter}
\CommentTok{\# Transition matrix}
\NormalTok{Gamma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{Gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{Gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{Gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\CommentTok{\# Observation matrix }
\NormalTok{Omega }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{Omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p      }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{Omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p          }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{Omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{Omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\CommentTok{\# Matrix of states}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ nind, }\AttributeTok{ncol =}\NormalTok{ nocc)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ z}
\NormalTok{y[,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# all individuals are detected in first winter, as we condition on first detection}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nind)\{}
\NormalTok{  z[i,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ delta) }\CommentTok{\# 1 for sure}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{nocc)\{}
    \CommentTok{\# state at t given state at t{-}1}
\NormalTok{    z[i,t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ Gamma[z[i,t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
    \CommentTok{\# observation at t given state at t}
\NormalTok{    y[i,t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ Omega[z[i,t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{y}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{{-}} \DecValTok{1} \CommentTok{\# non{-}detection = 0, detection = 1}
\end{Highlighting}
\end{Shaded}

\section{Marginalization}\label{marginalization}

In some situations, you will not be interested in inferring the hidden states \(z_{i,t}\), so why bother estimating them? The good news is that you can get rid of the states, so that the marginal likelihood is a function of survival and detection probabilities \(\phi\) and \(p\) only.

\subsection{Brute-force approach}\label{brute-force-approach}

Using the formula of total probability, you get the marginal likelihood by summing over all possible states in the complete likelihood:

\begin{align*}
\Pr(\mathbf{y}_i) &= \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T})\\
                &= \sum_{\mathbf{z}_i} \Pr(\mathbf{y}_i, \mathbf{z}_i)\\
                &= \sum_{z_{i,1}} \cdots \sum_{z_{i,T}} \Pr(y_{i,1}, y_{i,2}, \ldots, y_{i,T}, z_{i,1}, z_{i,2}, \ldots, z_{i,T})\\
\end{align*}

Going through the same steps as for deriving the complete likelihood, we obtain the marginal likelihood:

\begin{align*}
\Pr(\mathbf{y}_i) &= \sum_{z_{i,1}} \cdots \sum_{z_{i,T}} \left(\prod_{t=1}^T{\omega_{z_{i,t}, y_{i,t}}}\right) \left(\Pr(z_{i,1}) \prod_{t=2}^T{\gamma_{z_{i,t-1},z_{i,t}}}\right)\\
\end{align*}

Let's go through an example. Let's imagine we have \(T = 3\) winters, and we'd like to write the likelihood for an individual having the encounter history detected, detected then non-detected. Remember that non-detected is coded 1 and detected is coded 2, while alive is coded 1 and dead is coded 2. We need to calculate \(\Pr(y_1 = 2, y_2 = 2, y_3 = 1)\) which, according to the formula above, is given by:

\begin{align*}
\begin{split}
\Pr(y_1 = 2, y_2 = 2, y_3 = 1) &= \sum_{i=1}^{2} \sum_{j=1}^{2} \sum_{k=1}^{2} \Pr(y_1 = 2 | z_1 = i) \Pr(y_2 = 2 | z_2 = j) \Pr(y_3 = 1 | z_3 = k) \\ 
& \qquad \Pr(z_1=i) \Pr(z_2 = j | z_1 = i) \Pr(z_3 = k | z_2 = j)\\
\end{split}
\end{align*}

Expliciting all the sums in \(\Pr(y_1 = 2, y_2 = 2, y_3 = 1)\), we get the long and ugly expression:

\begin{align*}
\begin{split}
\Pr(y_1 = 2, y_2 = 2, y_3 = 1) &= \\
& \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 1) \times \\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 1 | z_1 = 1) \Pr(z_3 = 1 | z_2 = 1) +\\
&  \Pr(y_1 = 2 | z_1 = 2) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 1) \times\\ 
& \qquad \Pr(z_1 = 2) \Pr(z_2 = 1 | z_1 = 2) \Pr(z_3 = 1 | z_2 = 1) +\\
&  \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 2) \Pr(y_3 = 1 | z_3 = 1) \times\\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 2 | z_1 = 1) \Pr(z_3 = 1 | z_2 = 2) +\\
&  \Pr(y_1 = 2 | z_1 = 2) \Pr(y_2 = 2 | z_2 = 2) \Pr(y_3 = 1 | z_3 = 1) \times\\ 
& \qquad \Pr(z_1 = 2) \Pr(z_2 = 2 | z_1 = 2) \Pr(z_3 = 1 | z_2 = 2) +\\
&  \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 2) \times\\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 1 | z_1 = 1) \Pr(z_3 = 2 | z_2 = 1) +\\
&  \Pr(y_1 = 2 | z_1 = 2) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 2) \times\\ 
& \qquad \Pr(z_1 = 2) \Pr(z_2 = 1 | z_1 = 2) \Pr(z_3 = 2 | z_2 = 1) +\\
&  \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 2) \Pr(y_3 = 1 | z_3 = 2) \times\\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 2 | z_1 = 1) \Pr(z_3 = 2 | z_2 = 2) +\\
&  \Pr(y_1 = 2 | z_1 = 2) \Pr(y_2 = 2 | z_2 = 2) \Pr(y_3 = 1 | z_3 = 2) \times\\ 
& \qquad \Pr(z_1 = 2) \Pr(z_2 = 2 | z_1 = 2) \Pr(z_3 = 2 | z_2 = 2)\\
\end{split}
\end{align*}

You can simplify this expression by noticing that i) all individuals are alive for sure when marked and released in first winter, or \(\Pr(z_1=2) = 0\) and ii) dead individuals are non-detected for sure, or \(\Pr(y_t = 2|z_t = 2) = 0\), which lead to:

\begin{align*}
\begin{split}
\Pr(y_1 = 2, y_2 = 2, y_3 = 1) &= \\
& \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 1) \times \\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 1 | z_1 = 1) \Pr(z_3 = 1 | z_2 = 1) +\\
&  \Pr(y_1 = 2 | z_1 = 1) \Pr(y_2 = 2 | z_2 = 1) \Pr(y_3 = 1 | z_3 = 2) \times\\ 
& \qquad \Pr(z_1 = 1) \Pr(z_2 = 1 | z_1 = 1) \Pr(z_3 = 2 | z_2 = 1)\\
\end{split}
\end{align*}

Because all individuals are captured in first winter, or \(\Pr(y_1 = 2 | z_1 = 1) = 1\), we get:

\begin{align*}
\Pr(y_1 = 2, y_2 = 2, y_3 = 1) =  1 (1-p) \times 1 \phi \phi + 1 p 1 \times 1 \phi (1-\phi)
\end{align*}

You end up with \(\Pr(y_1 = 2, y_2 = 2, y_3 = 1) = \phi p (1 - p\phi)\).

The latent states are no longer involved in the likelihood for this individual. However, even on a rather simple example, the marginal likelihood is quite complex to evaluate because it involves many operations. If \(T\) is the length of our encounter histories and \(N\) is the number of hidden states (two for alive and dead, but we will deal with more states in some chapters to come), then we need to calculate the sum of \(N^T\) terms (the sums in the formula above), each of which has two products of \(T\) factors (the products in the formula above), hence \(2TN^T\) calculations in total. You can check that in the simple example above, we have \(T^N = 2^3 = 8\) terms that are summed, each of which is a product of \(2T = 2 \times 3 = 6\) terms. This means that the number of operations increases exponentially as the number of states increases. In most cases, this complexity precludes using this method to get rid of the states. Fortunately, we have another algorithm in the HMM toolbox that is useful to calculate the marginal likelihood efficiently.

\subsection{Forward algorithm}\label{forward-algorithm}

In the brute-force approach, some products are computed several times to calculate the marginal likelihood. What if we could store these products and use them later while computing the probability of the observation sequence? This is precisely what the forward algorithm does.

We introduce \(\alpha_t(j)\) the probability for the latent state \(z\) of being in state \(j\) at \(t\) after seeing the first \(j\) observations \(y_1, \ldots, y_t\), that is \(\alpha_t(j) = \Pr(y_1, \ldots, y_t, z_t = j)\).

Using the law of total probability, we can write the marginal likelihood as a function of \(\alpha_T(j)\), namely we have \(\Pr(\mathbf{y}) = \displaystyle{\sum_{j=1}^N\Pr(y_1, \ldots, y_t, z_t = j)} = \displaystyle{\sum_{j=1}^N\alpha_T(j)}\).

How to calculate the the \(\alpha_T(j)\)s? This is where the magic of the forward algorithm happens. We use a recurrence relationship that saves us many computations.

The recurrence states that:

\begin{align*}
\alpha_t(j) &= \sum_{i=1}^N \alpha_{t-1}(i) \gamma_{i,j} \omega_{j,y_t}
\end{align*}

How to obtain this recurrence? First, using the law of total probability with \(z_{t-1}\), we have that:
\begin{align*}
\alpha_t(j) &= \sum_{i=1}^N \Pr(y_1, \ldots, y_t, z_{t-1} = i, z_t = j)\\
\end{align*}

Second, using conditional probabilities, we get:
\begin{align*}
\alpha_t(j) &= \sum_{i=1}^N \Pr(y_t | z_{t-1} = i, z_t = j, y_1, \ldots, y_t) \Pr(z_{t-1} = i, z_t = j, y_1, \ldots, y_t)
\end{align*}

Third, using conditional probabilities again, on the second term of the product, we get:
\begin{align*}
\alpha_t(j) &= \sum_{i=1}^N \Pr(y_t | z_{t-1} = i, z_t = j, y_1, \ldots, y_t) \times \\ & \Pr(z_t = j | z_{t-1} = i, y_1, \ldots, y_t) \Pr(z_{t-1} = i, y_1, \ldots, y_t)
\end{align*}

which, using conditional independence, simplifies into:

\begin{align*}
\alpha_t(j) &= \sum_{i=1}^N \Pr(y_t | z_t = j) \Pr(z_t = j | z_{t-1} = i) \Pr(z_{t-1} = i, y_1, \ldots, y_t)
\end{align*}

Recognizing that \(\Pr(y_{t}|z_{t}=j)=\omega_{j,y_t}\), \(\Pr(z_{t} = j | z_{t-1} = i) = \gamma_{i,j}\) and \(\Pr(z_{t-1} = i, y_1, \ldots, y_t) = \alpha_{t-1}(i)\), we obtain the recurrence.

In practice, the forward algorithm works as follows. First you initialize the procedure by calculating for the states \(j=1,\ldots,N\) the quantities \(\alpha_1(j) = Pr(z_1 = j) \omega_{j,y_1}\). Then you compute for the states \(j=1,\ldots,N\) the relationship \(\alpha_t(j) = \displaystyle{\sum_{i=1}^N \alpha_{t-1}(i) \gamma_{i,j} \omega_{j,y_t}}\) for \(t = 2, \ldots, T\). Finally, you compute the marginal likelihood as \(\Pr(\mathbf{y}) = \displaystyle{\sum_{j=1}^N\alpha_T(j)}\). At each time \(t\), we need to calculate \(N\) values of \(\alpha_t(j)\), and each \(\alpha_t(j)\) is a sum of \(N\) products of \(\alpha_{t-1}\), \(\gamma_{i,j}\) and \(\omega_{j,y_t}\), hence \(TN^2\) computations in total, which is much less than the \(2TN^T\) calculations in the brute-force approach.

Going back to our example, we wish to calculate \(\Pr(y_1 = 2, y_2 = 2, y_3 = 1)\). First we initialize and compute \(\alpha_1(1)\) and \(\alpha_1(2)\). We have:

\begin{align*}
\alpha_1(1) = \Pr(z_1=1) \omega_{1,y_1=2} = 1
\end{align*}

because all animals are alive and captured in first winter. We also have:

\begin{align*}
\alpha_1(2) = \Pr(z_1=2) \omega_{2,y_1=2} = 0
\end{align*}

Then we compute \(\alpha_2(1)\) and \(\alpha_2(2)\). We have:

\begin{align*}
\alpha_2(1) &= \sum_{i=1}^2 \alpha_1(i) \gamma_{i,1} \omega_{1,y_2=2}\\
            &= \gamma_{1,1} \omega_{1,y_2=2}\\
            &= \phi p
\end{align*}

because \(\alpha_1(2) = 0\). Also, we have:

\begin{align*}
\alpha_2(2) &= \sum_{i=1}^2 \alpha_1(i) \gamma_{i,2} \omega_{2,y_2=2}\\
            &= \gamma_{1,2} \omega_{2,y_2=2}\\
            &= (1-\phi) 0
\end{align*}

Finally we compute \(\alpha_3(1)\) and \(\alpha_3(2)\). We have:

\begin{align*}
\alpha_3(1) &= \sum_{i=1}^2 \alpha_2(i) \gamma_{i,1} \omega_{1,y_3=1}\\
            &= \alpha_2(1) \gamma_{1,1} \omega_{1,y_3=1}\\
            &= \phi p \phi (1-p)
\end{align*}

and:

\begin{align*}
\alpha_3(2) &= \sum_{i=1}^2 \alpha_2(i) \gamma_{i,2} \omega_{2,y_3=1}\\
            &= \alpha_2(1) \gamma_{1,2} \omega_{2,y_3=1}\\
            &= \phi p (1-\phi) 1
\end{align*}

Eventually, we compute \(\Pr(y_1=2,y_2=2,y_3=1)\):

\begin{align*}
\Pr(y_1=2,y_2=2,y_3=1) &= \alpha_3(1) + \alpha_3(2)\\
            &= \phi p (\phi) (1-p) + \phi p (1-\phi)\\
            &= \phi p (1-\phi p)
\end{align*}

You can check that we did in total \(3 \times 2^2 = 12\) operations.

\subsection{NIMBLE implementation}\label{nimblemarginalization}

\subsubsection{Do it yourself}\label{diymarginalisation}

In NIMBLE, we use functions to implement the forward algorithm. The only differences with the theory above is that i) we work on the log scale for numerical stability and ii) we use a matrix formulation of the recurrence.

First we write the density function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dHMMhomemade }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{x =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{), }
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{), }\CommentTok{\# vector of initial states}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{), }\CommentTok{\# observation matrix}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{), }\CommentTok{\# transition matrix}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{), }\CommentTok{\# number of sampling occasions}
                 \AttributeTok{log =} \FunctionTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) \{}
\NormalTok{    alpha }\OtherTok{\textless{}{-}}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\CommentTok{\# * probObs[1:2,x[1]] == 1 due to conditioning on first detection}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len) \{}
\NormalTok{      alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\%*\%}\NormalTok{ probTrans[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ probObs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,x[t]]}
\NormalTok{    \}}
\NormalTok{    logL }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]))}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
    \ControlFlowTok{if}\NormalTok{ (log) }\FunctionTok{return}\NormalTok{(logL)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{exp}\NormalTok{(logL))}
\NormalTok{  \}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In passing, this is the function you would maximize in a frequentist approach (see Section \ref{under-the-hood}). Then we write a function to simulate values from a HMM:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rHMMhomemade }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n =} \FunctionTok{integer}\NormalTok{(),}
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{),}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) \{}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{    z }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(len)}
\NormalTok{    z[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# all individuals alive at t = 0}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ z}
\NormalTok{    y[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# all individuals are detected at t = 0}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len)\{}
      \CommentTok{\# state at t given state at t{-}1}
\NormalTok{      z[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probTrans[z[t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
      \CommentTok{\# observation at t given state at t}
\NormalTok{      y[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probObs[z[t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{(y)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

We assign these functions to the global R environment:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}dHMMhomemade\textquotesingle{}}\NormalTok{, dHMMhomemade, .GlobalEnv)}
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}rHMMhomemade\textquotesingle{}}\NormalTok{, rHMMhomemade, .GlobalEnv)}
\end{Highlighting}
\end{Shaded}

Now we resume our workflow:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# code}
\NormalTok{hmm.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
  \CommentTok{\# likelihood}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{T] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dHMMhomemade}\NormalTok{(}\AttributeTok{probInit =}\NormalTok{ delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }
                            \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# observation matrix}
                            \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# transition matrix}
                            \AttributeTok{len =}\NormalTok{ T) }\CommentTok{\# nb of sampling occasions}
\NormalTok{  \}}
\NormalTok{\})}
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{T =} \DecValTok{5}\NormalTok{)}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\CommentTok{\# initial values {-} no need to specify values for z anymore}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# parameters to save}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

And run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.survival,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{end\_time }\SpecialCharTok{{-}}\NormalTok{ start\_time}
\DocumentationTok{\#\# Time difference of 24.58 secs}
\end{Highlighting}
\end{Shaded}

The numerical summaries are similar to those we obtained with the complete likelihood, and effective samples sizes are larger denoting better mixing:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.61 0.06 0.49 0.61  0.72    1  1211}
\DocumentationTok{\#\# phi 0.76 0.04 0.67 0.76  0.84    1  1483}
\end{Highlighting}
\end{Shaded}

\subsubsection{\texorpdfstring{Do it with \texttt{nimbleEcology}}{Do it with nimbleEcology}}\label{nimbleecologyintro}

Writing NIMBLE functions is not easy. Fortunately, the NIMBLE folks got you covered. They developed the package \texttt{nimbleEcology} that implements some of the most popular ecological models with latent states.

We will use the function \texttt{dHMMo} which provides the distribution of a hidden Markov model with time-independent transition matrix and time-dependent observation matrix. Why time-dependent observation matrix? Because we need to tell NIMBLE that detection at first encounter is 1.

We load the package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nimbleEcology)}
\end{Highlighting}
\end{Shaded}

The NIMBLE code is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
  \CommentTok{\# likelihood}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(alive first {-}\textgreater{} non{-}detected first)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(alive first {-}\textgreater{} detected first)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead first {-}\textgreater{} non{-}detected first)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead first {-}\textgreater{} detected first)}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  \}}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dHMMo}\NormalTok{(}\AttributeTok{init =}\NormalTok{ delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# vector of initial state probabilities}
                     \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{], }\CommentTok{\# observation matrix}
                     \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# transition matrix}
                     \AttributeTok{len =} \DecValTok{5}\NormalTok{, }\CommentTok{\# nb of sampling occasions}
                     \AttributeTok{checkRowSums =} \DecValTok{0}\NormalTok{) }\CommentTok{\# skip validity checks}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

You may see that we no longer have the states in the code as we use the marginalized likelihood. The \texttt{dHMMo} takes several arguments, including \texttt{init} the vector of initial state probabilities, \texttt{probObs} the observation matrix, \texttt{probTrans} the transition matrix and \texttt{len} the number of sampling occasions.

Next steps are similar to the workflow we used before. The only difference is that we do not need to specify initial values for the latent states:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y))}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\CommentTok{\# initial values {-} no need to specify values for z anymore}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# parameters to save}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

Now we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.survival,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{end\_time }\SpecialCharTok{{-}}\NormalTok{ start\_time}
\DocumentationTok{\#\# Time difference of 28.74 secs}
\end{Highlighting}
\end{Shaded}

Now we display the numerical summaries of the posterior distributions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.61 0.06 0.49 0.61  0.72    1  1453}
\DocumentationTok{\#\# phi 0.76 0.04 0.67 0.76  0.84    1  1359}
\end{Highlighting}
\end{Shaded}

The results are similar what we obtained previously with our home-made marginalized likelihood (Section \ref{diymarginalisation}), or with the full likelihood (\ref{fittinghmmnimble}).

\section{Pooled encounter histories}\label{pooled-likelihood}

We can go one step further to make convergence even faster. As mentionned earlier in Section \ref{likelihoodhmm}, the likelihood of an HMM fitted to capture-recapture data often involves individuals that share the same encounter histories. Instead of repeating the same calculations several times, the likelihood contribution that is shared by say \(x\) individuals is raised to the power \(x\) in the likelihood of the whole dataset, hence making the same operations only once. This idea is used in routine in capture-recapture software. For Bayesian software however, it is only recently that the trick was tested in NIMBLE by \citet{TurekEtAl2016}.

In this section, we amend the NIMBLE functions we wrote for marginalizing latent states in Section \ref{marginalization} to express the likelihood using pooled encounter histories. We use a vector \texttt{size} that contains the number of individuals with the same encounter history.

The density function is the function \texttt{dHMMhomemade} to which we add a \texttt{size} argument, and raise the individual likelihood to the power \texttt{size}, or multiply by \texttt{size} as we work on the log scale \texttt{log(sum(alpha{[}1:2{]}))\ *\ size}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dHMMpooled }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{x =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{), }
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{),}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                 \AttributeTok{log =} \FunctionTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) \{}
\NormalTok{    alpha }\OtherTok{\textless{}{-}}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len) \{}
\NormalTok{      alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\%*\%}\NormalTok{ probTrans[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ probObs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,x[t]]}
\NormalTok{    \}}
\NormalTok{    logL }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])) }\SpecialCharTok{*}\NormalTok{ size}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
    \ControlFlowTok{if}\NormalTok{ (log) }\FunctionTok{return}\NormalTok{(logL)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{exp}\NormalTok{(logL))}
\NormalTok{  \}}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{rHMMhomemade} function is renamed \texttt{rHMMpooled} for compatibility but remains unchanged:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rHMMpooled }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n =} \FunctionTok{integer}\NormalTok{(),}
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{),}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{)) \{}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{    z }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(len)}
\NormalTok{    z[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\CommentTok{\# all individuals alive at t = 0}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ z}
\NormalTok{    y[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# all individuals are detected at t = 0}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len)\{}
      \CommentTok{\# state at t given state at t{-}1}
\NormalTok{      z[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probTrans[z[t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
      \CommentTok{\# observation at t given state at t}
\NormalTok{      y[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probObs[z[t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{(y)}
\NormalTok{  \})}
\end{Highlighting}
\end{Shaded}

We assign these two function to the global R environment so that we can use them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}dHMMpooled\textquotesingle{}}\NormalTok{, dHMMpooled, .GlobalEnv)}
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}rHMMpooled\textquotesingle{}}\NormalTok{, rHMMpooled, .GlobalEnv)}
\end{Highlighting}
\end{Shaded}

You can now plug your pooled HMM density function in your NIMBLE code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.survival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
  \CommentTok{\# likelihood}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{T] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dHMMpooled}\NormalTok{(}\AttributeTok{probInit =}\NormalTok{ delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }
                          \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# observation matrix}
                          \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# transition matrix}
                          \AttributeTok{len =}\NormalTok{ T, }\CommentTok{\# nb of sampling occasions}
                          \AttributeTok{size =}\NormalTok{ size[i]) }\CommentTok{\# number of individuals with encounter history i}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Before running NIMBLE, we need to actually pool individuals with the same encounter history together:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pooled }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by\_all}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# group}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{size =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# count}
  \FunctionTok{relocate}\NormalTok{(size) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# put size in front}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{size) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# sort along size}
  \FunctionTok{as.matrix}\NormalTok{()}
\NormalTok{y\_pooled}
\DocumentationTok{\#\#       size winter 1 winter 2 winter 3 winter 4 winter 5}
\DocumentationTok{\#\#  [1,]   21        1        0        0        0        0}
\DocumentationTok{\#\#  [2,]    8        1        1        0        0        0}
\DocumentationTok{\#\#  [3,]    8        1        1        1        1        0}
\DocumentationTok{\#\#  [4,]    4        1        1        0        0        1}
\DocumentationTok{\#\#  [5,]    4        1        1        1        0        0}
\DocumentationTok{\#\#  [6,]    2        1        0        0        1        0}
\DocumentationTok{\#\#  [7,]    2        1        0        1        1        0}
\DocumentationTok{\#\#  [8,]    2        1        1        0        1        0}
\DocumentationTok{\#\#  [9,]    1        1        0        1        0        0}
\DocumentationTok{\#\# [10,]    1        1        0        1        0        1}
\DocumentationTok{\#\# [11,]    1        1        0        1        1        1}
\DocumentationTok{\#\# [12,]    1        1        1        0        1        1}
\DocumentationTok{\#\# [13,]    1        1        1        1        0        1}
\DocumentationTok{\#\# [14,]    1        1        1        1        1        1}
\end{Highlighting}
\end{Shaded}

For example, we have 21 individuals with encounter history (1, 0, 0, 0, 0).

Now you can resume the NIMBLE workflow:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y\_pooled), }\AttributeTok{T =} \DecValTok{5}\NormalTok{, }\AttributeTok{size =}\NormalTok{ y\_pooled[,}\StringTok{\textquotesingle{}size\textquotesingle{}}\NormalTok{])}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y\_pooled[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{) }\CommentTok{\# delete size from dataset}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{start\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{mcmc.output }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.survival,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\NormalTok{end\_time }\OtherTok{\textless{}{-}} \FunctionTok{Sys.time}\NormalTok{()}
\NormalTok{end\_time }\SpecialCharTok{{-}}\NormalTok{ start\_time}
\DocumentationTok{\#\# Time difference of 25.96 secs}
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.output, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.61 0.06 0.49 0.61  0.72    1  1455}
\DocumentationTok{\#\# phi 0.76 0.04 0.67 0.76  0.84    1  1524}
\end{Highlighting}
\end{Shaded}

The results are the same as those obtained previously. The gain in computation times will be bigger for more complex models as we will see in the next chapters.

The pooled likelihood is not yet implemented in \texttt{nimbleEcology}, but you can hack the code for the function \texttt{dHMMo} \url{https://github.com/nimble-dev/nimbleEcology/blob/master/R/dHMM.R} to implement it yourself by adding a \texttt{size} argument.

\section{Decoding after marginalization}\label{decoding}

If you need to infer the latent states, and you cannot afford the computation times of the complete likelihood of Section \ref{likelihoodhmm}, you can still use the marginal likelihood with the forward algorithm of Section \ref{forward-algorithm}. You will need an extra step to decode the latent states with the Viterbi algorithm. The Viterbi algorithm allows you to compute the sequence of states that is most likely to have generated the sequence of observations.

\subsection{Theory}\label{viterbi-theory}

In our simulated dataset, animal \#15 has the encounter history (2, 1, 1, 1, 1) which was generated from the sequence of states (1, 1, 2, 2, 2) with survival probability \(\phi = 0.8\) and detection probability \(p = 0.6\).

Imagine you do not know the truth. What is the chance that animal \#15 was alive throughout the study when observing the encounter history detected in first winter, then missed in each subsequent winter? The chance of being alive in the first winter and detected when alive is 1. The chance of being alive in the second winter and non-detected is \(0.8 \times (1-0.6) = 0.32\). The same goes for third, fourth and fifth winters. In total, the probability of being alive throughout the study for an animal with encounter history (2, 1, 1, 1, 1) is \(1 \times 0.32 \times 0.32 \times 0.32 \times 0.32 = 0.01048576\).

Now what is the chance that animal \#15 was alive, then dead for the rest of the study, when observing the encounter history (2, 1, 1, 1, 1)? And the chance of being alive in first and second winters, then dead after when observing the same encounter history? And so on. You need to enumerate all possible sequences of states and compute the probability for each of them, and choose the most probable sequence, that is with maximum probability. In our example, we would need to compute \(2^5 = 32\) of these probabilities, and \(N^T\) in general. Needless to say, these calculations quickly become cumbersome, if not impossible, as the number of states and/or the number of sampling occasions increases.

This is where the Viterbi algorithm comes in. The idea is to decompose this overall complex problem in a sequence of smallers problems that are easier to solve. If dynamic programming rings a bell, the Viterbi algorithm should look familiar to you. The Viterbi algorithm is based on the fact that the optimal path to each winter and each state can be deduced from the optimal path to the previous winter and each state.

For first winter, the probability of being alive and detected is 1, while the probability of being dead and detected is 0. Now what is the probability of being alive in the second winter and non-detected? If the animal was alive in the first winter, it remains alive and is missed with probability \(1 \times \phi (1-p) = 0.32\). If it was dead in the first winter, then this probability is 0. The maximum probability is 0.32 obviously so the most probable scenario to being alive in the second winter is being alive in the first winter. What about being dead in the second winter? If the animal was alive in first winter, then the probability is \(1 \times (1-\phi) \times 1 = 0.2\). If dead, then this probability is \(0 \times 0 \times (1-p) = 0\). The maximum probability is 0.2 obviously so the most probable scenario to being dead in the second winter is being alive in the first winter. Doing these calculations for third, fourth and fifth winters, we get the probabilities:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0992}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.0763}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1603}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.1908}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2214}}
  >{\centering\arraybackslash}p{(\linewidth - 10\tabcolsep) * \real{0.2519}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
winter 1
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
winter2
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
winter 3
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
winter 4
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
winter 5
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{state alive} & 1 & 0.32 = max(0, 0.32) & 0.2304 = max(0, 0.2304) & 0.110592 = max(0, 0.110592) & 0.053084416 = max(0, 0.05308416 \\
\textbf{state dead} & 0 & 1 = max(1, 0.2) & 1 = max(0.096, 1) & 1 = max(0.04608, 1) & 1 = max(0.0221184, 1) \\
\textbf{observation} & detected & missed & missed & missed & missed \\
\end{longtable}

Finally, to get (or decode) the optimal path, you work backwards and trace back the previous value that yielded the maximum probability. The most probable state in the last winter is dead (1 \textgreater{} 0.05308416), dead again in the fourth winter (1 \textgreater{} 0.110592), dead in the third winter (1 \textgreater{} 0.2304), alive in the second winter (1 \textgreater{} 0.48) and alive in the firt winter (1 \textgreater{} 0). According to the Viterbi algorithm, the sequence of states that most likelily generated the sequence of observations (2, 1, 1, 1, 1) is alive then dead, dead, dead and dead or (1 2 2 2 2). This differs slightly from the actual sequence of states (1, 1, 2, 2, 2) in that the state in second winter is decoded dead while animal \#15 only dies in third winter.

In contrast to the brute force approach, calculations are not duplicated but stored and used again like in the forward algorithm. Briefly speaking, the Viterbi algorithm works like the forward algorithm where sums are replaced by calculating maximums.

In practice, the Viterbi algorithm works as illustrated in Figure \ref{fig:treillis-viterbi}. First you initialize the procedure by calculating at \(t=1\) for all states \(j=1,\ldots,N\) the values \(\nu_1(j) = Pr(z_1 = j) \omega_{j,y_1}\). Then you compute for all states \(j=1,\ldots,N\) the values \(\nu_t(j) = \displaystyle{\max_{i=1,\ldots,N} \nu_{t-1}(i) \gamma_{i,j} \omega_{j,y_t}}\) for \(t = 2, \ldots, T\). Here at each time \(t\) we determine the probability of the best path ending at each of the states \(j=1,\ldots,N\). Finally, you compute the probability of the best global path \(\displaystyle{\max_{j=1,\ldots,N}\nu_T(j)}\).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/treillis-viterbi} 

}

\caption{Graphical representation of the Viterbi algorithm with $\phi = 0.8$ and $p = 0.6$. States are alive $z = 1$ or dead $z = 2$ and observations are non-detected $y = 1$ or detected $y = 2$. To be done properly w/ tikz.}\label{fig:treillis-viterbi}
\end{figure}

\subsection{Implementation}\label{implementation}

Let's write a R function to implement the Viterbi algorithm. As parameters, our function will take the transition and observation matrices, the vector of initial state probabilities and the observed sequence of detections and non-detections for which you aim to compute the sequence of states from which it was most likely generated:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# getViterbi() returns sequence of states that most likely generated sequence of observations}
\CommentTok{\# adapted from https://github.com/vbehnam/viterbi}
\NormalTok{getViterbi }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(Omega, Gamma, delta, y) \{}
\CommentTok{\# Omega: transition matrix}
\CommentTok{\# Gamma: observation matrix}
\CommentTok{\# delta: vector of initial state probabilities}
\CommentTok{\# y: observed sequence of detections and non{-}detections}
  
\CommentTok{\# get number of states and sampling occasions}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Gamma)}
\NormalTok{T }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(y)}
  
\CommentTok{\# nu is the corresponding likelihood}
\NormalTok{nu }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ N, }\AttributeTok{ncol =}\NormalTok{ T)}
\CommentTok{\# zz contains the most likely states up until this point}
\NormalTok{zz }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ N, }\AttributeTok{ncol =}\NormalTok{ T)}
\NormalTok{firstObs }\OtherTok{\textless{}{-}}\NormalTok{ y[}\DecValTok{1}\NormalTok{]}
  
\CommentTok{\# fill in first columns of both matrices}
\CommentTok{\#nu[,1] \textless{}{-} initial * emission[,firstObs]}
\CommentTok{\#zz[,1] \textless{}{-} 0}
\NormalTok{nu[,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{) }\CommentTok{\# initial = (1, 0) * emission[,firstObs] = (1, 0)}
\NormalTok{zz[,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# alive at first occasion}

\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{T) \{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
\NormalTok{      obs }\OtherTok{\textless{}{-}}\NormalTok{ y[i]}
      \CommentTok{\# initialize to {-}1, then overwritten by for loop coz all possible values are \textgreater{}= 0}
\NormalTok{      nu[j,i] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}
      \CommentTok{\# loop to find max and argmax for k}
      \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
\NormalTok{        value }\OtherTok{\textless{}{-}}\NormalTok{ nu[k,i}\DecValTok{{-}1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ Gamma[k,j] }\SpecialCharTok{*}\NormalTok{ Omega[j,obs]}
        \ControlFlowTok{if}\NormalTok{ (value }\SpecialCharTok{\textgreater{}}\NormalTok{ nu[j,i]) \{}
          \CommentTok{\# maximizing for k}
\NormalTok{          nu[j,i] }\OtherTok{\textless{}{-}}\NormalTok{ value}
          \CommentTok{\# argmaximizing for k}
\NormalTok{          zz[j,i] }\OtherTok{\textless{}{-}}\NormalTok{ k}
\NormalTok{        \}}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{  \}}
  \CommentTok{\# mlp = most likely path}
\NormalTok{  mlp }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(T)}
  \CommentTok{\# argmax for stateSeq[,T]}
\NormalTok{  am }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(nu[,T])}
\NormalTok{  mlp[T] }\OtherTok{\textless{}{-}}\NormalTok{ zz[am,T]}
  
  \CommentTok{\# backtrace using backpointers}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ T}\SpecialCharTok{:}\DecValTok{2}\NormalTok{) \{}
\NormalTok{    zm }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(nu[,i])}
\NormalTok{    mlp[i}\DecValTok{{-}1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ zz[zm,i]}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(mlp)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Note that instead of writing your own R function, you could use a built-in function from an existing R package to implement the Viterbi algorithm (for example, the \texttt{viterbi()} function from the \texttt{HMM} and \texttt{depmixS4} packages), and call it from NIMBLE as we have seen in Section \ref{callrfninnimble}. The difficulty is that HMM for capture-recapture data have specific features that make standard functions not adapted and requires coding your own Viterbi function. In particular, we have to deal with detection at first encounter, which is not estimated but is always one because an individual has to be captured to be marked and released for the first time. Also, our transition and observation matrices are not always homogeneous and may depend on time.

Let's test our \texttt{getViterbi()} function with our previous example. Remember animal \#15 has the encounter history (2, 1, 1, 1, 1) which was generated from the sequence of states (1, 1, 2, 2, 2). Applying our function to that animal encounter history, we get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delta }\CommentTok{\# Vector of initial states probabilities}
\DocumentationTok{\#\# [1] 1 0}
\NormalTok{Gamma }\CommentTok{\# Transition matrix}
\DocumentationTok{\#\#      [,1] [,2]}
\DocumentationTok{\#\# [1,]  0.8  0.2}
\DocumentationTok{\#\# [2,]  0.0  1.0}
\NormalTok{Omega }\CommentTok{\# Observation matrix}
\DocumentationTok{\#\#      [,1] [,2]}
\DocumentationTok{\#\# [1,]  0.4  0.6}
\DocumentationTok{\#\# [2,]  1.0  0.0}
\FunctionTok{getViterbi}\NormalTok{(}\AttributeTok{Omega =}\NormalTok{ Omega, }
           \AttributeTok{Gamma =}\NormalTok{ Gamma, }
           \AttributeTok{delta =}\NormalTok{ delta, }
           \AttributeTok{y =}\NormalTok{ y[}\DecValTok{15}\NormalTok{,] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\DocumentationTok{\#\# [1] 1 2 2 2 2}
\end{Highlighting}
\end{Shaded}

The Viterbi algorithm does pretty well at recovering the latent states, despite incorrectly decoding a death in the second winter while individual \#15 only dies in the third winter. We obtained the same results by implementing the Viterbi algorithm by hand in Section \ref{viterbi-theory}.

Now that we have a function that implements the Viterbi algorithm, we can use it with our MCMC outputs. You have two options, either you apply Viterbi to each MCMC iteration then you compute the posterior median or mode path for each individual, or you compute the posterior mean or median of the transition and observation matrices then you apply Viterbi to each individual encounter history.

For both options, we will need the values from the posterior distributions of survival and detection probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phi }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{], mcmc.output}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}phi\textquotesingle{}}\NormalTok{])}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.output}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{], mcmc.output}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}p\textquotesingle{}}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\subsection{Compute first, average after}\label{compute-average}

First option is to apply Viterbi to each MCMC sample, then to compute median of the MCMC Viterbi paths for each observed sequence:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{niter }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(p)}
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{ncol =}\NormalTok{ T)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(y))\{}
\NormalTok{  res\_mcmc }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =}\NormalTok{ niter, }\AttributeTok{ncol =}\NormalTok{ T)}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{niter)\{}
    \CommentTok{\# Initial states}
\NormalTok{    delta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
    \CommentTok{\# Transition matrix}
\NormalTok{    transition }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{    transition[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi[j]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    transition[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[j]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    transition[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    transition[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
    \CommentTok{\# Observation matrix }
\NormalTok{    emission }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{    emission[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p[j]      }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    emission[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p[j]          }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    emission[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    emission[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{    res\_mcmc[j,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{T] }\OtherTok{\textless{}{-}} \FunctionTok{getViterbi}\NormalTok{(emission, transition, delta, y[i,] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{  res[i, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(y[}\DecValTok{1}\NormalTok{,])] }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(res\_mcmc, }\DecValTok{2}\NormalTok{, median)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

You can compare the Viterbi decoding to the actual states \(z\):
\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/viterbiaveragecompute-1.pdf}}

Decoding is correct except that the alive actual state is often decoded as the dead state by the Viterbi algorithm. Note that here we compute the Viterbi paths after we run NIMBLE. You could turn the R function \texttt{getViterbi()} into a NIMBLE function and plug it in your model code to apply Viterbi. This would not make any difference except perhaps to increase MCMC computation times.

\subsection{Average first, compute after}\label{average-first-compute-after}

Second option is to compute the posterior mean of the observation and transition matrices, then to apply Viterbi:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initial states}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\CommentTok{\# Transition matrix}
\NormalTok{transition }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{transition[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(phi)      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{transition[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(phi)  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{transition[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}              \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{transition[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}              \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\CommentTok{\# Observation matrix }
\NormalTok{emission }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{emission[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(p)      }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{emission[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(p)          }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{emission[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}                \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{emission[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{res }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{ncol =}\NormalTok{ T)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(y))\{}
\NormalTok{  res[i, }\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(y[}\DecValTok{1}\NormalTok{,]) ] }\OtherTok{\textless{}{-}} \FunctionTok{getViterbi}\NormalTok{(emission, transition, delta, y[i,] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Again, you can compare the result of the Viterbi decoding to the actual states we simulated and used to generate the observations:
\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/viterbicomputeaverage-1.pdf}}

The results are very similar to those we obtained in Section \ref{compute-average}, and Figure \ref{fig:viterbicomputeaverage} is indisguishable from Figure \ref{fig:viterbiaveragecompute}.

\section{Summary}\label{summary-2}

\begin{itemize}
\item
  A HMM is a model that consists of two parts: i) an unobserved sequence of discrete random variables - the states - satisfying the Markovian property (future states depends on current states only and not on past states) and ii) an observed sequence of discrete random variables - the observations - depending only on the current state.
\item
  The Bayesian approach together with MCMC simulations allow estimating survival and detection probabilities as well as individual latent states alive or dead with the complete likelihood. If you can afford the computation times, then using the complete likelihood is the easiest path for model fitting.
\item
  If you do not need to infer the latent states, you can use the marginal likelihood via the forward algorithm. By avoiding to sample the latent states, you usually get better mixing and faster convergence.
\item
  If you do need to infer the latent states, and you cannot afford the computation times of the complete likelihood, then you can go for the marginal likelihood in conjunction with the Viterbi algorithm to decode the latent states.
\item
  If the computational burden is still an issue, and you have individuals that share the same encounter history, you can use a pooled likelihood to speed up the marginal likelihood evaluation and MCMC convergence.
\end{itemize}

\section{Suggested reading}\label{suggested-reading-2}

\begin{itemize}
\item
  A landmark paper on HMM is \citet{Rabiner1989}.
\item
  Check out \citet{JurafskySpeechAL} for a nice introduction to HMM and \citet{ZucchiniEtAl2016} for an excellent book that covers theory and applications.
\item
  The paper by \citet{mcclintock_uncovering_2020} reviews the applications of HMM in ecology.
\item
  The package \texttt{nimbleEcology} is developed by \citet{goldstein2019nimbleecology} and \citet{ponisio2020customizing} for application to occupancy and N--mixture models.
\end{itemize}

\part{Transitions}\label{part-transitions}

\chapter*{Introduction}\label{introduction-4}


WORK IN PROGRESS

This second part \texttt{Transitions} will teach you all about capture-recapture models for open populations, with reproducible R code to ease the learning process.

\chapter{Alive and dead}\label{survival}

\section{Introduction}\label{introduction-5}

In this fourth chapter, you will learn about the Cormack-Jolly-Seber model that allows estimating survival based on capture-recapture data. You will also see how to deal with covariates to try and explain temporal and/or individual variation in survival. This chapter will also be the opportunity to introduce tools to compare models and assess their quality of fit to data.

\section{The Cormack-Jolly-Seber (CJS) model}\label{the-cormack-jolly-seber-cjs-model}

In Chapter \ref{hmmcapturerecapture}, we introduced a capture-recapture model with constant survival and detection probabilities which we formulated as a HMM and fitted to data in NIMBLE. Historically, however, it was a slightly more complicated model that was first proposed -- the so-called Cormack-Jolly-Seber (CJS) model -- in which survival and recapture probabilities are time-varying. This feature of the CJS model is useful to account for variation due to environmental conditions in survival or to sampling effort in detection. Schematically the CJS model can be represented this way:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-199-1.pdf}}

Note that the states (in gray) and the observations (in white) do not change. We still have \(z = 1\) for alive, \(z = 2\) for dead, \(y = 1\) for non-detected, and \(y = 2\) for detected.

Parameters are now indexed by time. The survival probability is defined as the probability of staying alive (or ``ah, ha, ha, ha, stayin' alive'' like the Bee Gees would say) over the interval between \(t\) and \(t+1\), that is \(\phi_t = \Pr(z_{t+1} = 1 | z_t = 1)\). The detection probability is defined as the probability of being observed at \(t\) given you're alive at \(t\), that is \(p_t = \Pr(y_{t} = 1 | z_t = 1)\). It is important to bear in mind for later (see Section \ref{covariates}) that survival operates over an interval while detection occurs at a specific time.

The CJS model is named after the three statisticians -- Richard Cormack, George Jolly and George Seber -- who each published independently a paper introducing more or less the same approach, a year apart ! In fact, Richard Cormack and George Jolly were working in the same corridor in Scotland back in the 1960's. They would meet every day at coffee and would play some game together, but never mention work and were not aware of each other's work.

\section{Capture-recapture data}\label{crdataeg}

Before we turn to fitting the CJS model to actual data, let's talk about capture-recapture for a minute. We said in Section \ref{capturerecapturedata} that animals are individually marked. This can be accomplished in two ways, either with artificial marks like rings for birds or ear tags for mammals, or (non-invasive) natural marks like coat patterns or feces DNA sequencing (Figure \ref{fig:marking}).

\begin{figure}
\includegraphics[width=1\linewidth]{images/marking} \caption{Animal individual marking. Top-left: rings (credit: Emannuelle Cam and Jean-Yves Monat); Top-right: ear-tags (credit: Kelly Powell); Bottom left: coat patterns (credit: Fridolin Zimmermann); Bottom right: ADN feces (credit: Alexander Kopatz)}\label{fig:marking}
\end{figure}

Throughout this chapter, we will use data on the White-throated Dipper (\emph{Cinclus cinclus}; dipper hereafter) kindly provided by Gilbert Marzolin (Figure \ref{fig:pixdipper}). In total, 294 dippers with known sex and wing length were captured and recaptured between 1981 and 1987 during the March-June period. Birds were at least 1 year old when initially banded.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/Marzo_BaguesMance} 

}

\caption{White-throated Dipper (Cinclus cinclus). Credit: Gilbert Marzolin.}\label{fig:pixdipper}
\end{figure}

The data look like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dipper }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"dat/dipper.csv"}\NormalTok{)}
\NormalTok{dipper}
\DocumentationTok{\#\# \# A tibble: 294 x 9}
\DocumentationTok{\#\#    year\_1981 year\_1982 year\_1983 year\_1984 year\_1985}
\DocumentationTok{\#\#        \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1         1         1         1         1         1}
\DocumentationTok{\#\#  2         1         1         1         1         1}
\DocumentationTok{\#\#  3         1         1         1         1         0}
\DocumentationTok{\#\#  4         1         1         1         1         0}
\DocumentationTok{\#\#  5         1         1         0         1         1}
\DocumentationTok{\#\#  6         1         1         0         0         0}
\DocumentationTok{\#\#  7         1         1         0         0         0}
\DocumentationTok{\#\#  8         1         1         0         0         0}
\DocumentationTok{\#\#  9         1         1         0         0         0}
\DocumentationTok{\#\# 10         1         1         0         0         0}
\DocumentationTok{\#\# \# i 284 more rows}
\DocumentationTok{\#\# \# i 4 more variables: year\_1986 \textless{}dbl\textgreater{}, year\_1987 \textless{}dbl\textgreater{},}
\DocumentationTok{\#\# \#   sex \textless{}chr\textgreater{}, wing\_length \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

The first seven columns are years in which Gilbert went on the field and captured the birds. A 0 stands for a non-detection, and a 1 for a detection. The eighth column informs on the sex of the bird, with F for female and M for male. The last column gives a measure wing length the first time a bird was captured.

\section{Fitting the CJS model to the dipper data with NIMBLE}\label{fitting-the-cjs-model-to-the-dipper-data-with-nimble}

To write the NIMBLE code corresponding to the CJS model, we only need to make a few adjustments to the NIMBLE code for the model with constant parameters from Section \ref{fittinghmmnimble}. The main modification concerns the observation and transition matrices which we need to make time-varying. These matrices therefore become arrays and inherit a third dimension for time, besides that for rows and columns. Also we need priors for all time-varying survival \texttt{phi{[}t{]}\ \textasciitilde{}\ dunif(0,\ 1)} and detection \texttt{p{[}t{]}\ \textasciitilde{}\ dunif(0,\ 1)} probabilities. We write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
\CommentTok{\# parameters}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    phi[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{    p[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p[t]    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ p[t]        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  \}}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The likelihood does not change, except that the time-varying observation and transition matrices need to be used appropriately. Also, because we now deal with several cohorts of animals first captured, marked and released each year (in contrast with a single cohort as in Chapter \ref{hmmcapturerecapture}), we need to start the loop over time from the first capture for each individual. Therefore, we write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
\CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

At first capture, all individuals are alive \texttt{z{[}i,first{[}i{]}{]}\ \textasciitilde{}\ dcat(delta{[}1:2{]})} and detection is 1, then after first capture \texttt{for\ (j\ in\ (first{[}i{]}+1):T)} we apply the transition and observation matrices.
Overall, the code looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phitpt }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# parameters}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    phi[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{    p[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p[t]    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ p[t]        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  \}}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We extract the detections and non-detections from the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ dipper }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(year\_1981}\SpecialCharTok{:}\NormalTok{year\_1987) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We get the occasion of first capture for all individuals, by finding the position of detections in the encounter history (\texttt{which(x\ !=0)}), and keeping the first one:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Now we specify the constants:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y),   }\CommentTok{\# number of animals}
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y),   }\CommentTok{\# number of sampling occasions}
                     \AttributeTok{first =}\NormalTok{ first) }\CommentTok{\# first capture for all animales}
\end{Highlighting}
\end{Shaded}

We then put the data in a list. We add 1 to the data to code non-detections as 1's detections as 2's (see Section \ref{fittinghmmnimble}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's write a function for the initial values. For the latent states, we go the easy way, and say that all individuals are alive through the study period:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# non{-}detection {-}\textgreater{} alive}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# dead {-}\textgreater{} alive}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{T}\DecValTok{{-}1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{T}\DecValTok{{-}1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

We specify the parameters we would like to monitor, survival and detection probabilities here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We provide MCMC details:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

And we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phitpt }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phitpt,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We may have a look to the numerical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phitpt, }\AttributeTok{params =} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{,}\StringTok{"p"}\NormalTok{), }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#        mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# phi[1] 0.73 0.14 0.46 0.72  0.99 1.02   199}
\DocumentationTok{\#\# phi[2] 0.45 0.07 0.32 0.44  0.59 1.02   410}
\DocumentationTok{\#\# phi[3] 0.48 0.06 0.35 0.48  0.59 1.01   506}
\DocumentationTok{\#\# phi[4] 0.63 0.06 0.52 0.63  0.75 1.03   415}
\DocumentationTok{\#\# phi[5] 0.60 0.06 0.49 0.60  0.72 1.01   365}
\DocumentationTok{\#\# phi[6] 0.74 0.13 0.51 0.74  0.97 1.10    38}
\DocumentationTok{\#\# p[1]   0.66 0.14 0.38 0.67  0.89 1.01   344}
\DocumentationTok{\#\# p[2]   0.87 0.08 0.68 0.89  0.98 1.02   249}
\DocumentationTok{\#\# p[3]   0.88 0.07 0.73 0.89  0.97 1.02   307}
\DocumentationTok{\#\# p[4]   0.87 0.06 0.74 0.88  0.96 1.05   333}
\DocumentationTok{\#\# p[5]   0.90 0.05 0.77 0.91  0.98 1.01   224}
\DocumentationTok{\#\# p[6]   0.72 0.13 0.50 0.72  0.97 1.08    37}
\end{Highlighting}
\end{Shaded}

There is not so much time variation in the detection probability which is estimated high around 0.90. Note that \texttt{p{[}1{]}} corresponds to the detection probability in 1982 that is \(p_2\), \texttt{p{[}2{]}} to detection in 1983 therefore \(p_3\), and so on. The dippers seem to have experienced a decrease in survival in years 1982-1983 (\texttt{phi{[}2{]}}) and 1983-1984 (\texttt{phi{[}4{]}}). We will get back to that in Section \ref{covariates}.

You may have noticed the small effective sample size for the last survival (\texttt{phi{[}6{]}}) and detection (\texttt{p{[}6{]}}) probabilities. Let's have a look to mixing for parameter \texttt{phi{[}6{]}} for example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{priors }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{3000}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\FunctionTok{MCMCtrace}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.phitpt,}
          \AttributeTok{ISB =} \ConstantTok{FALSE}\NormalTok{,}
          \AttributeTok{exact =} \ConstantTok{TRUE}\NormalTok{, }
          \AttributeTok{params =} \FunctionTok{c}\NormalTok{(}\StringTok{"phi[6]"}\NormalTok{),}
          \AttributeTok{pdf =} \ConstantTok{FALSE}\NormalTok{, }
          \AttributeTok{priors =}\NormalTok{ priors)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-214-1.pdf}}

Clearly mixing (left panel in the plot above) is bad and there is a big overlap between the prior and the posterior for this parameter (right panel) suggesting that its prior was not well updated with the data. What is going on? If you could inspect the likelihood of the CJS model, you would realize that these two parameters \(\phi_6\) and \(p_7\) appear only as the product \(\phi_6 p_7\) and cannot be estimated separately. In other words, one of these parameters is redundant, and you'd need an extra sampling occasion to be able to disentangle them. This is not a big issue as long as you're aware of it and you do not attempt to ecologically interpret these parameters.

\section{CJS model derivatives}\label{cjsderivatives}

Besides the model we considered with constant parameters (see Chapter \ref{hmmcapturerecapture}) and the CJS model with time-varying parameters, you might want to fit in-between or models with time variation on either detection or survival.

But I realize that we did not actually fit the model with constant parameters from Chapter \ref{hmmcapturerecapture}. Let's do it. You should be familiar with the process by now:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# NIMBLE code }
\NormalTok{hmm.phip }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
  \CommentTok{\# likelihood}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\CommentTok{\# occasions of first capture}
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first)}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\CommentTok{\# initial values}
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1} \CommentTok{\# non{-}detection {-}\textgreater{} alive}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# dead {-}\textgreater{} alive}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\CommentTok{\# parameters to monitor}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{2500}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\CommentTok{\# run NIMBLE}
\NormalTok{mcmc.phip }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phip, }
                        \AttributeTok{constants =}\NormalTok{ my.constants,}
                        \AttributeTok{data =}\NormalTok{ my.data,              }
                        \AttributeTok{inits =}\NormalTok{ initial.values,}
                        \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                        \AttributeTok{niter =}\NormalTok{ n.iter,}
                        \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                        \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\CommentTok{\# numerical summaries}
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phip, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.89 0.03 0.83 0.90  0.94 1.02   231}
\DocumentationTok{\#\# phi 0.56 0.02 0.51 0.56  0.61 1.00   595}
\end{Highlighting}
\end{Shaded}

Let's now turn to the model with time-varying survival and constant detection. We modify the CJS model NIMBLE code by no longer having the observation matrix time-specific. I'm just providing the model code to save space:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phitp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    phi[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We obtain the following numerical summaries for parameters, confirming high detection and temporal variation in survival:

\begin{verbatim}
##        mean   sd 2.5%  50% 97.5% Rhat n.eff
## phi[1] 0.63 0.10 0.42 0.63  0.82 1.04   564
## phi[2] 0.46 0.06 0.35 0.46  0.59 1.01   629
## phi[3] 0.48 0.05 0.37 0.48  0.59 1.00   610
## phi[4] 0.62 0.06 0.51 0.62  0.73 1.00   553
## phi[5] 0.61 0.05 0.50 0.61  0.72 1.00   568
## phi[6] 0.59 0.05 0.48 0.59  0.69 1.03   463
## p      0.89 0.03 0.82 0.89  0.95 1.04   211
\end{verbatim}

Now the model with time-varying detection and constant survival, for which the NIMBLE code has a constant over time transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phipt }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    p[t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p[t]    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ p[t]        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  \}}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Numerical summaries for the parameters are:

\begin{verbatim}
##      mean   sd 2.5%  50% 97.5% Rhat n.eff
## phi  0.56 0.03 0.52 0.56  0.61 1.02   381
## p[1] 0.75 0.12 0.48 0.77  0.93 1.03   452
## p[2] 0.85 0.08 0.68 0.86  0.97 1.02   359
## p[3] 0.85 0.07 0.69 0.85  0.96 1.00   316
## p[4] 0.89 0.05 0.77 0.89  0.97 1.00   412
## p[5] 0.91 0.04 0.82 0.92  0.98 1.00   376
## p[6] 0.90 0.07 0.73 0.91  1.00 1.07   111
\end{verbatim}

We note that these two models do no longer have parameter redundancy issues.

Before we move to the next section, you might ask how to fit these models with \texttt{nimbleEcology} as in Section \ref{nimblemarginalization}. Conveniently there are specific NIMBLE functions for the marginalized likelihood of the CJS model and its derivatives. These function are named generically \texttt{dCJSxx()} where the first \texttt{x} is for survival and the second \texttt{x} is for detection and \texttt{x} can be \texttt{s} for scalar or constant or \texttt{v} for vector or time-dependent. For example, to implement the model with constant survival and detection probabilities, you would use \texttt{dCJSss()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load nimbleEcology in case we forgot previously}
\FunctionTok{library}\NormalTok{(nimbleEcology)}
\CommentTok{\# data}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ dipper }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(year\_1981}\SpecialCharTok{:}\NormalTok{year\_1987) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{()}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y[ first }\SpecialCharTok{!=} \FunctionTok{ncol}\NormalTok{(y), ] }\CommentTok{\# get rid of individuals for which first detection = last capture}
\CommentTok{\# NIMBLE code}
\NormalTok{hmm.phip.nimbleecology }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# survival prior}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)   }\CommentTok{\# detection prior}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i, first[i]}\SpecialCharTok{:}\NormalTok{T] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dCJS\_ss}\NormalTok{(}\AttributeTok{probSurvive =}\NormalTok{ phi, }
                               \AttributeTok{probCapture =}\NormalTok{ p, }
                               \AttributeTok{len =}\NormalTok{ T }\SpecialCharTok{{-}}\NormalTok{ first[i] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\})}
\CommentTok{\# occasions of first capture}
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first)}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y) }\CommentTok{\# format is 0 for non{-}detected and 1 for detected}
\CommentTok{\# initial values; we use the marginalized likelihood, so no latent states }
\CommentTok{\# in it, therefore no need for initial values for the latent states}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# parameters to monitor}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{2500}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\CommentTok{\# run NIMBLE}
\NormalTok{mcmc.phip.nimbleecology }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phip.nimbleecology, }
                        \AttributeTok{constants =}\NormalTok{ my.constants,}
                        \AttributeTok{data =}\NormalTok{ my.data,              }
                        \AttributeTok{inits =}\NormalTok{ initial.values,}
                        \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                        \AttributeTok{niter =}\NormalTok{ n.iter,}
                        \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                        \AttributeTok{nchains =}\NormalTok{ n.chains)}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\DocumentationTok{\#\# |{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}|}
\CommentTok{\# numerical summaries}
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phip.nimbleecology, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#     mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p   0.89 0.03 0.84 0.90  0.94 1.02   672}
\DocumentationTok{\#\# phi 0.56 0.02 0.52 0.56  0.61 1.00   716}
\end{Highlighting}
\end{Shaded}

We are left with four models, each saying a different story about the data, with temporal variation or not in either survival or detection probability. How to quantify the most plausible of these four ecological hypotheses? Rendez-vous in the next section.

\section{Model comparison with WAIC}\label{waic}

Which of the four models above is best supported by the data? To answer this question, we need to bear in mind that we used all the observed data to fit these models, and how close to the truth these models will perform when predicting for future data -- also known as predictive accuracy -- should be assessed. A natural candidate measure for predictive accuracy is the likelihood which is often referred in the context of model comparison as the predictive density. However, we know neither the true process, nor the future data, and we can only estimate the predictive density with some bias.

You may have heard about the Akaike Information Criterion (AIC) in the frequentist framework, and the Deviance Information Criterion (DIC) in the Bayesian framework. Here, we will also consider the Widely Applicable Information Criterion or Watanabe Information Criterion (WAIC). AIC, DIC and WAIC each aim to provide an approximation of predictive accuracy.

While AIC is the predictive measure of choice in the frequentist framework for ecologists, DIC has been around for some time for Bayesian applications due to its availability in population BUGS pieces of software. However, AIC and BIC use a point estimate of the unknown parameters, while we have access to their entire (posterior) distribution with the Bayesian approach. Also, DIC has been shown to misbehave when the posterior distribution is not well summarized by its mean. For a more fully Bayesian approach we would like to use the entire posterior distribution to evaluate the predictive performance, which is exactly what WAIC does.

Conveniently, NIMBLE calculates WAIC for you. The only modification you need to make is to add \texttt{WAIC\ =\ TRUE} in the call to the \texttt{nimbleMCMC()} function. For example, for the CJS model, we write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phitpt }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phitpt,}
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                          \AttributeTok{nchains =}\NormalTok{ n.chains,}
                          \AttributeTok{WAIC =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

I re-ran the four models to calculate the WAIC value for each of them

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \FunctionTok{c}\NormalTok{(}\StringTok{"both survival \& detection constant"}\NormalTok{,}
                     \StringTok{"time{-}dependent survival, constant detection"}\NormalTok{,}
                     \StringTok{"constant survival, time{-}dependent detection"}\NormalTok{,}
                     \StringTok{"both survival \& detection time{-}dependent"}\NormalTok{),}
           \AttributeTok{WAIC =} \FunctionTok{c}\NormalTok{(mcmc.phip}\SpecialCharTok{$}\NormalTok{WAIC}\SpecialCharTok{$}\NormalTok{WAIC,}
\NormalTok{                    mcmc.phitp}\SpecialCharTok{$}\NormalTok{WAIC}\SpecialCharTok{$}\NormalTok{WAIC,}
\NormalTok{                    mcmc.phipt}\SpecialCharTok{$}\NormalTok{WAIC}\SpecialCharTok{$}\NormalTok{WAIC,}
\NormalTok{                    mcmc.phitpt}\SpecialCharTok{$}\NormalTok{WAIC}\SpecialCharTok{$}\NormalTok{WAIC))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                                         model  WAIC
## 1          both survival & detection constant 265.9
## 2 time-dependent survival, constant detection 277.6
## 3 constant survival, time-dependent detection 270.2
## 4    both survival & detection time-dependent 308.8
\end{verbatim}

Lower values of WAIC imply higher predictive accuracy, thefore we would favor model with constant parameters.

\section{Goodness of fit}\label{gof}

In the previous section, Section \ref{waic}, we compared models between each other based on their predictive accuracy -- we assessed their \emph{relative} fit. However, even though we were able to rank these models according to predictive accuracy, it could happen that all models actually had poor predictive performance -- this has to do with \emph{absolute} fit.

How to assess the goodness of fit of the CJS model to capture-recapture data?

\subsection{Posterior predictive checks}\label{posterior-predictive-checks}

In the Bayesian framework, we rely on posterior predictive checks to assess absolute fit. Briefly speaking, the idea is to compare the observed data to replicated data generated from the model. If the model is a good fit to the data, then the replicated data predicted from the model should look similar to the observed data. To simplify the comparison, some summary statistics are generally used. For the CJS model, we would use the so-called m-array which gathers the number of individuals released in a year and first recaptured in other years.

Classical m-array (minimal sufficient statistics for CJS model) as in \citet{paganin2023computational}. Individual performance in \citet{chambert2014} and \citet{nater2020trout}. Sojourn time is geometric assumption in \citet{conn2018}.

For the CJS model, we would use the so-called m-array which gathers the elements \(m_{ij}\) for the number of marked individuals initially released at time \(i\) that were first detected again at time \(j\).

Refer to a case study. With m-array and Nimble functions. Refer to paper by Paganin \& de Valpine and use code in \url{https://github.com/salleuska/fastCPPP}. Also papers by Chambert et al.~(individual performance) and Conn et al.~(geometric time, and hidden semi-Markov models). See my repo on ppp checks.

Check out \url{https://r-nimble.org/nimbleExamples/posterior_predictive.html}.

\subsection{Classical tests}\label{classical-tests}

In the frequentist literature, there are well established procedures for assessing absolute fit and departures from specific CJS model assumptions, and it would be a shame to just ignore them.

We focus on two such assumptions that have an ecological interpretation, transience and trap-dependence. The transience procedure assesses whether newly encountered individuals have the same chance to be later detected as previously encountered individuals. Transience is often seen as an excess of individuals never seen again. The trap-dependence procedure assesses whether non-detected individuals have the same chance to be encountered at the next occasion as detected individuals. Trap-dependence is often seen as an effect of trapping on detection. Although these procedures are called \emph{test of transience} and \emph{test of trap-dependence}, when it comes to interpretation, you should keep in mind that transience and trap-dependence are just two specific reasons why the tests might detect a lack of fit.

These tests are implemented in the package \texttt{R2ucare}, and we illustrate its use with the dipper data.

We load the package \texttt{R2ucare}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(R2ucare)}
\end{Highlighting}
\end{Shaded}

We get the capture-recapture data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# capture{-}recapture data}
\NormalTok{dipper }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"dat/dipper.csv"}\NormalTok{)}
\CommentTok{\# get encounter histories}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ dipper }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(year\_1981}\SpecialCharTok{:}\NormalTok{year\_1987) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix}\NormalTok{()}
\CommentTok{\# number of birds with a particular capture{-}recapture history}
\NormalTok{size }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(y))}
\end{Highlighting}
\end{Shaded}

We may perform a test specifically to assess a transient effect:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{test3sr}\NormalTok{(y, size)}
\DocumentationTok{\#\# $test3sr}
\DocumentationTok{\#\#      stat        df     p\_val sign\_test }
\DocumentationTok{\#\#     1.773     5.000     0.880    {-}0.054 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $details}
\DocumentationTok{\#\#   component  stat p\_val signed\_test  test\_perf}
\DocumentationTok{\#\# 1         2  0.08 0.778       0.283 Chi{-}square}
\DocumentationTok{\#\# 2         3 0.232  0.63       0.482 Chi{-}square}
\DocumentationTok{\#\# 3         4 0.847 0.358       {-}0.92 Chi{-}square}
\DocumentationTok{\#\# 4         5 0.288 0.592      {-}0.537 Chi{-}square}
\DocumentationTok{\#\# 5         6 0.326 0.568       0.571 Chi{-}square}
\end{Highlighting}
\end{Shaded}

Or trap-dependence:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{test2ct}\NormalTok{(y, size)}
\DocumentationTok{\#\# $test2ct}
\DocumentationTok{\#\#      stat        df     p\_val sign\_test }
\DocumentationTok{\#\#     9.463     4.000     0.051    {-}1.538 }
\DocumentationTok{\#\# }
\DocumentationTok{\#\# $details}
\DocumentationTok{\#\#   component dof  stat p\_val signed\_test test\_perf}
\DocumentationTok{\#\# 1         2   1     0     1           0    Fisher}
\DocumentationTok{\#\# 2         3   1     0     1           0    Fisher}
\DocumentationTok{\#\# 3         4   1     0     1           0    Fisher}
\DocumentationTok{\#\# 4         5   1 9.463 0.002      {-}3.076    Fisher}
\end{Highlighting}
\end{Shaded}

None of these tests is significant, but what to do if these tests are significant? Transience may occur when animals transit through but do not belong to the study population (true transients), or reproduce only once then die or permanently disperse (cost of first reproduction), or die or permanently disperse due to an effect of marking (marking effect). These transient individuals are never detected again after their initial capture, which means they have a zero probability of local survival after this initial capture. This suggests a way to account for the transience issue by considering a two-age class effect on survival, where age here is time since first capture, not biological age. We consider two age classes. Age 1 is for newly marked and the interval immediately after first capture; this is where transients live, and if they're true transients, they're never seen again because their local survival right after marking is effectively 0. Age 2+ is for established residents and all later intervals for individuals that remained in the study population after that first interval. We then let survival differ between these two ages and define \(\phi_1\) as the apparent survival in the first interval after initial capture and \(\phi_{2+}\) as the apparent survival in subsequent intervals. If there are transients, \(\phi_1\) will be pulled downward relative to \(\phi_{2+}\), because the first interval mixes true residents (who survive locally with probability \(\phi_{2+}\)) and transients (who never survive locally to the next occasion). A useful quantity to compute her is the transient proportion \(\tau\) where \(\displaystyle \tau = 1 - \frac{\phi_{1}}{\phi_{2+}}\). We will see later on in this chapter in Section \ref{agecov} how to encode an age effect on survival, and I will use this trick I just described to account for transience in Section \ref{trapdep}. Note that transience can be encoded through states in a HMM, see \citet{genovart2019} for more details.

Trap-dependence may occur when animals are affected (positively or negatively) by trapping (true trap-dependence), when observers tend to visit some parts of the study area more often when individuals have been detected (preferential sampling), when some patches of a heterogeneous habitat are more accessible so that individuals stationed there have higher detection probabilities (access bias), when age, sex or social status are unknown, but determine individual movements or activity patterns so that the susceptibility to be detected varies (individual heterogeneity), or when non random temporary emigration occurs (skipped reproduction). Trap-dependence therefore designates some correlation between detection events. To account for the trap-dependence issue, this correlation needs to be accounted for, and we show how to do just that in a case study (see Section \ref{trapdep}).

\subsection{Design considerations}\label{design-considerations}

So far, we have addressed assumptions relative to the model. There are also assumptions relative to the design. In particular, survival refers to the study area, and so we need to think carefully about what survival does actually mean. We actually estimate what we usually call \emph{apparent} survival, not exactly \emph{true} survival. Apparent survival probability is the product of true survival and study area fidelity. Consequently, apparent survival is always lower than true survival unless study area fidelity is exactly 1.

There are other assumptions relative to the design, which we simply list here. There should be no mark loss, the identity of individuals should be recorded without error (no false positives), and captured animals should be a random sample of the population.

\section{Covariates}\label{covariates}

In the models we have considered so far, survival and detection probabilities may vary over time, but we do not include ecological drivers that might explain these variation. Luckily, in the spirit of generalized linear models, we can make these parameters dependent on external covariates over time, such as environmental conditions for survival or sampling effort for detection.

Besides variation over time, we will also cover individual variation in these parameters, in which for example survival vary according to sex or some phenotypic characteristics (e.g.~size or body mass).

Let's illustrate the use of covariates with the dipper example.

\subsection{Temporal covariates}\label{temporal-covariates}

\subsubsection{Discrete}\label{discrete}

A major flood occurred during the 1983 breeding season (from October 1982 to May 1983). Because captures during the breeding season occurred before and after the flood, survival in the two years 1982-1983 and 1983-1984 were likely to be affected. Indeed survival of a species living along and feeding in the river in these two flood years was most likely lower than in nonflood years. Here we will use a discrete or categorical covariate, or a group.

Let's use a covariate \texttt{flood} that contains 1s and 2s, indicating whether we are in a flood or nonflood year for each year: 1 if nonflood year, and 2 if flood year.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flood }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\CommentTok{\# 1981{-}1982 (nonflood)}
           \DecValTok{2}\NormalTok{, }\CommentTok{\# 1982{-}1983 (flood)}
           \DecValTok{2}\NormalTok{, }\CommentTok{\# 1983{-}1984 (flood)}
           \DecValTok{1}\NormalTok{, }\CommentTok{\# 1984{-}1985 (nonflood)}
           \DecValTok{1}\NormalTok{, }\CommentTok{\# 1985{-}1986 (nonflood)}
           \DecValTok{1}\NormalTok{) }\CommentTok{\# 1986{-}1987 (nonflood)}
\end{Highlighting}
\end{Shaded}

Then we write the model code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phifloodp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[flood[t]]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[flood[t]]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  phi[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior for survival in nonflood years}
\NormalTok{  phi[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior for survival in flood years}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We use nested indexing above when specifying survival in the transition matrix. E.g. for year \(t = 2\), \texttt{phi{[}flood{[}t{]}{]}} gives \texttt{phi{[}flood{[}2{]}{]}} which will be \texttt{phi{[}2{]}} because \texttt{flood{[}2{]}} is 2 that (a flood year).

Let's provide the constants in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y),}
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y),}
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{flood =}\NormalTok{ flood)}
\end{Highlighting}
\end{Shaded}

Now a function to generate initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

The parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"p"}\NormalTok{, }\StringTok{"phi"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We're all set, and we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phifloodp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phifloodp, }
                             \AttributeTok{constants =}\NormalTok{ my.constants,}
                             \AttributeTok{data =}\NormalTok{ my.data,              }
                             \AttributeTok{inits =}\NormalTok{ initial.values,}
                             \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                             \AttributeTok{niter =}\NormalTok{ n.iter,}
                             \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                             \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

The numerical summaries are given by:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phifloodp, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean   sd 2.5%  50% 97.5% Rhat n.eff
## p      0.89 0.03 0.83 0.90  0.95    1   609
## phi[1] 0.61 0.03 0.55 0.61  0.67    1  1474
## phi[2] 0.47 0.04 0.38 0.47  0.56    1  1700
\end{verbatim}

Having a look to the numerical summaries, we see that as expected, survival in flood years (\texttt{phi{[}2{]}}) is much lower than survival in non-flood years (\texttt{phi{[}1{]}}). You could formally test this difference by considering the difference \texttt{phi{[}1{]}\ -\ phi{[}2{]}}. Alternatively, this can be done afterwards and calculating the probability that this difference is positive (or \texttt{phi{[}1{]}\ \textgreater{}\ phi{[}2{]}}). Using a single chain for convenience, we do:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phi1 }\OtherTok{\textless{}{-}}\NormalTok{ mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}phi[1]\textquotesingle{}}\NormalTok{]}
\NormalTok{phi2 }\OtherTok{\textless{}{-}}\NormalTok{ mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}phi[2]\textquotesingle{}}\NormalTok{]}
\FunctionTok{mean}\NormalTok{(phi1 }\SpecialCharTok{{-}}\NormalTok{ phi2 }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\DocumentationTok{\#\# [1] 0.9952}
\end{Highlighting}
\end{Shaded}

An important point here is that we formulated an ecological hypothesis which we translated into a model. The next step would consist in calculating WAIC for this model and compare it with that of the four other model we have fitted so far (see Section \ref{waic}).

Another method to include a discrete covariate consists in considering the effect of the difference between its levels. For example, we could consider survival in nonflood years as the reference and test the difference in survival with flood years.

To do so, we write survival as a linear function of our covariate on some scale, e.g.~\(\text{logit}(\phi_t) = \beta_1 + \beta_2 \;\text{flood}_t\) where the \(\beta\)'s are regression coefficients we need to estimate (intercept \(\beta_1\) and slope \(\beta_2\)), and \(\text{logit}(x) = \log \displaystyle{\left(\frac{x}{1-x}\right)}\) is the logit function. The logit function lives between \(-\infty\) and \(+\infty\), and sends values between 0 and 1 onto the real line. For example \(\log(0.2/(1-0.2))=-1.39\), \(\log(0.5/(1-0.5))=0\) and \(\log(0.8/(1-0.8))=1.39\). Why use the logit function? Because survival is a probability bounded between 0 and 1, and if we used directly \(\phi_t = \beta_1 + \beta_2 \;\text{flood}_t\) we might end up with estimates for the regression coefficients that make survival out of bound. Therefore, we consider survival as a linear function of our covariates on the scale of the logit function, working on the real line, then back-transform using the inverse-logit (reciprocal function) to obtain survival on its natural scale. The inverse-logit function is \(\displaystyle{\text{logit}^{-1}(x) = \frac{\exp(x)}{1+\exp(x)} = \frac{1}{1+\exp(-x)}}\). The logit function is often called a link function like in generalized linear models. Other link functions exist, we'll see an example in Section \ref{multinomiallogit}.

Another point of attention is the prior we will assign to regression coefficients. We no longer assign a prior to survival directly like previously, but we need to assign prior to the \(\beta\)'s which will induce some prior on survival. And sometimes, your priors on the regression coefficients are non-informative, but the prior on survival is not. Consider for example the case of a single intercept with no covariate. If you assign as a prior to this regression coefficient a normal distribution with mean 0 and large standard deviation (left figure below), which would be my first reflex, then you end up with a very informative prior on survival with a bathtub shape, putting much importance on low and high values (right figure below):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 1000 random values from a N(0,10)}
\NormalTok{intercept }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{) }
\CommentTok{\# plogis() is the inverse{-}logit function in R}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(intercept) }
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ intercept, }\AttributeTok{survival =}\NormalTok{ survival)}
\NormalTok{plot1 }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ intercept)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"prior on intercept"}\NormalTok{)}
\NormalTok{plot2 }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"prior on survival"}\NormalTok{)}
\NormalTok{plot1 }\SpecialCharTok{+}\NormalTok{ plot2}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-237-1.pdf}}

Now if you go for a lower standard deviation for the intercept prior (left figure below), e.g.~1.5, the prior on survival is non-informative, looking like a uniform distribution between 0 and 1 (right figure below):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\# 1000 random values from a N(0,1.5)}
\NormalTok{intercept }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }
\CommentTok{\# plogis() is the inverse{-}logit function in R}
\NormalTok{survival }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(intercept) }
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{intercept =}\NormalTok{ intercept, }\AttributeTok{survival =}\NormalTok{ survival)}
\NormalTok{plot1 }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ intercept)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"prior on intercept"}\NormalTok{)}
\NormalTok{plot2 }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ survival)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"prior on survival"}\NormalTok{)}
\NormalTok{plot1 }\SpecialCharTok{+}\NormalTok{ plot2}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-238-1.pdf}}

Now let's go back to our model. We first define our flood covariate with 0 if nonflood year, and 1 if flood year:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flood }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\CommentTok{\# 1981{-}1982 (nonflood)}
           \DecValTok{1}\NormalTok{, }\CommentTok{\# 1982{-}1983 (flood)}
           \DecValTok{1}\NormalTok{, }\CommentTok{\# 1983{-}1984 (flood)}
           \DecValTok{0}\NormalTok{, }\CommentTok{\# 1984{-}1985 (nonflood)}
           \DecValTok{0}\NormalTok{, }\CommentTok{\# 1985{-}1986 (nonflood)}
           \DecValTok{0}\NormalTok{) }\CommentTok{\# 1986{-}1987 (nonflood)}
\end{Highlighting}
\end{Shaded}

Then we write the NIMBLE code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phifloodp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
    \FunctionTok{logit}\NormalTok{(phi[t]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ flood[t]}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior intercept}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior slope}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We wrote \(\text{logit}(\phi_t) = \beta_1 + \beta_2 \; \text{flood}_t\), meaning that survival in nonflood years (\(\text{flood}_t = 0\)) is \(\text{logit}(\phi_t) = \beta_1\) and survival in flood years (\(\text{flood}_t = 1\)) is \(\text{logit}(\phi_t) = \beta_1 + \beta_2\). We see that \(\beta_1\) is survival in nonflood years (on the logit scale) and \(\beta_2\) is the difference between survival in flood years and survival in nonflood years (again, on the logit scale). In passing we assigned the same prior for both \(\beta_1\) and \(\beta_2\) but in certain situations, we might think twice before doing that as \(\beta_2\) is a difference between two survival probabilities (on the logit scale).

Let's put our constants in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y),}
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y),}
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{flood =}\NormalTok{ flood)}
\end{Highlighting}
\end{Shaded}

Then our function for generating initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

And the parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"p"}\NormalTok{, }\StringTok{"phi"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Finaly, we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phifloodp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phifloodp, }
                             \AttributeTok{constants =}\NormalTok{ my.constants,}
                             \AttributeTok{data =}\NormalTok{ my.data,              }
                             \AttributeTok{inits =}\NormalTok{ initial.values,}
                             \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                             \AttributeTok{niter =}\NormalTok{ n.iter,}
                             \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                             \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

You may check that we get the same numerical summaries as above for survival in nonflood years (\texttt{phi{[}1{]}}, \texttt{phi{[}4{]}}, \texttt{phi{[}5{]}} and \texttt{phi{[}6{]}}) and flood years (\texttt{phi{[}2{]}} and \texttt{phi{[}3{]}}):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phifloodp, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd  2.5%   50% 97.5% Rhat n.eff
## beta[1]  0.44 0.13  0.17  0.44  0.69 1.01   725
## beta[2] -0.54 0.22 -0.96 -0.54 -0.11 1.00   770
## p        0.89 0.03  0.83  0.89  0.94 1.00   631
## phi[1]   0.61 0.03  0.54  0.61  0.67 1.01   724
## phi[2]   0.47 0.04  0.39  0.47  0.56 1.00  1597
## phi[3]   0.47 0.04  0.39  0.47  0.56 1.00  1597
## phi[4]   0.61 0.03  0.54  0.61  0.67 1.01   724
## phi[5]   0.61 0.03  0.54  0.61  0.67 1.01   724
## phi[6]   0.61 0.03  0.54  0.61  0.67 1.01   724
\end{verbatim}

You may also check how to go from the \(\beta\)'s to the survival probabilities \(\phi\). Let's get the draws from the posterior distribution of the \(\beta\)'s first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{], }\CommentTok{\# beta1 chain 1}
\NormalTok{           mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# beta1 chain 2}
\NormalTok{beta2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{], }\CommentTok{\# beta2 chain 1}
\NormalTok{           mcmc.phifloodp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# beta2 chain 2}
\end{Highlighting}
\end{Shaded}

Then apply the inverse-logit function to get survival in nonflood years, e.g.~its posterior mean and credible interval:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{plogis}\NormalTok{(beta1))}
\DocumentationTok{\#\# [1] 0.6066}
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{plogis}\NormalTok{(beta1), }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\DocumentationTok{\#\#   2.5\%  97.5\% }
\DocumentationTok{\#\# 0.5435 0.6651}
\end{Highlighting}
\end{Shaded}

Same thing for survival in flood years:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(}\FunctionTok{plogis}\NormalTok{(beta1 }\SpecialCharTok{+}\NormalTok{ beta2))}
\DocumentationTok{\#\# [1] 0.4744}
\FunctionTok{quantile}\NormalTok{(}\FunctionTok{plogis}\NormalTok{(beta1 }\SpecialCharTok{+}\NormalTok{ beta2), }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{2.5}\NormalTok{, }\FloatTok{97.5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\DocumentationTok{\#\#   2.5\%  97.5\% }
\DocumentationTok{\#\# 0.3895 0.5606}
\end{Highlighting}
\end{Shaded}

\subsubsection{Continuous}\label{continuous}

Instead of a discrete covariate varying over time, we may want to consider a continuous covariate, say \(x_t\), through \(\text{logit}(\phi_t) = \beta_1 + \beta_2 x_t\). For example, let's investigate the effect of water flow on dipper survival, which should reflect the flood that occurred during the 1983 breeding season.

We build a covariate with water flow in liters per second measured during the March to May period each year, starting with year 1982:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# water flow in L/s}
\NormalTok{water\_flow }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{443}\NormalTok{,  }\CommentTok{\# March{-}May 1982}
                \DecValTok{1114}\NormalTok{, }\CommentTok{\# March{-}May 1983}
                \DecValTok{529}\NormalTok{,  }\CommentTok{\# March{-}May 1984}
                \DecValTok{434}\NormalTok{,  }\CommentTok{\# March{-}May 1985}
                \DecValTok{627}\NormalTok{,  }\CommentTok{\# March{-}May 1986}
                \DecValTok{466}\NormalTok{)  }\CommentTok{\# March{-}May 1987}
\end{Highlighting}
\end{Shaded}

We do not need water flow in 1981 because we will write the probability \(\phi_t\) of being alive in year \(t + 1\) given a bird was alive in year \(t\) as a linear function of the water flow in year \(t + 1\).

You may have noticed the high value of water flow for 1983, twice as much as in the other years, corresponding to the flood. Importantly, we standardize our covariate to improve convergence:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{water\_flow\_st }\OtherTok{\textless{}{-}}\NormalTok{ (water\_flow }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(water\_flow))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(water\_flow)}
\end{Highlighting}
\end{Shaded}

Now we write the model code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phiflowp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
    \FunctionTok{logit}\NormalTok{(phi[t]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ flow[t] }
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior intercept}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior slope}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We put the constants in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y),}
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y),}
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{flow =}\NormalTok{ water\_flow\_st)}
\end{Highlighting}
\end{Shaded}

Initial values as usual:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

And parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"p"}\NormalTok{, }\StringTok{"phi"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Eventually, we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phiflowp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phiflowp, }
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,              }
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We can have a look to the results through a caterpillar plot of the regression parameters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCplot}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.phiflowp, }\AttributeTok{params =} \StringTok{"beta"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-258-1.pdf}}

The posterior distribution of the slope (\texttt{beta{[}2{]}}) is centered on negative values, suggesting that as water flow increases, survival decreases.

Let's inspect the time-dependent survival probability:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCplot}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.phiflowp, }\AttributeTok{params =} \StringTok{"phi"}\NormalTok{, }\AttributeTok{ISB =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-260-1.pdf}}

Survival between 1982 and 1983 (\texttt{phi{[}2{]}}) was greatly affected and much lower than on average. This decrease corresponds to the high water flow in 1983 and the flood. These results are in line with our previous findings obtained by considering a discrete covariate for nonflood vs.~flood years.

\subsection{Individual covariates}\label{individual-covariates}

In the previous section, we learnt how to explain temporal heterogeneity in survival and detection. Heterogeneity could also originate from individual differences between animals. You may think of a diffence in survival between males and females for a discrete covariate example, or size and body mass for examples of a continuous covariate. Let's illustrate both discrete and continuous covariates on the dipper.

\subsubsection{Discrete}\label{discrete-1}

We first consider a covariate \texttt{sex} that contains 1's and 2's indicating the sex of each bird: 1 if male, and 2 if female. We implement the model with sex effect using nested indexing, similarly to the model with flood vs.~nonflood years. The section of the NIMBLE code that needs to be amended is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phi[sex[i]]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[sex[i]]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}                \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1}                \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{\}}
\NormalTok{phi[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\CommentTok{\# male survival}
\NormalTok{phi[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\CommentTok{\# female survival}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

After running NIMBLE, we get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(}\AttributeTok{object =}\NormalTok{ mcmc.phisexp.ni, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        mean   sd 2.5%  50% 97.5% Rhat n.eff
## p      0.90 0.03 0.84 0.90  0.95    1   643
## phi[1] 0.57 0.03 0.50 0.57  0.64    1  1668
## phi[2] 0.55 0.03 0.48 0.55  0.62    1  1482
\end{verbatim}

Male survival (\texttt{phi{[}1{]}}) looks very similar to female survival (\texttt{phi{[}2{]}}).

\subsubsection{Continuous}\label{continuous-1}

Besides discrete individual covariates, you might want to have continuous individual covariates, e.g.~wing length in the dipper example. Note that we're considering an individual trait that takes the same value whatever the occasion. We consider wing length here, and more precisely its measurement at first detection. We first standardize the covariate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wing.length.st }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{scale}\NormalTok{(dipper}\SpecialCharTok{$}\NormalTok{wing\_length))}
\FunctionTok{head}\NormalTok{(wing.length.st)}
\DocumentationTok{\#\# [1]  0.7581 {-}0.8671  0.5260 {-}1.5637 {-}1.3315  1.2225}
\end{Highlighting}
\end{Shaded}

Now we write the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phiwlp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{    p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \FunctionTok{logit}\NormalTok{(phi[i]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ winglength[i]}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}           \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1}           \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, i])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We put the constants in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{winglength =}\NormalTok{ wing.length.st)}
\end{Highlighting}
\end{Shaded}

We write a function for generating initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

And we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phiwlp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phiwlp, }
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,              }
                          \AttributeTok{inits =}\NormalTok{ initial.values,}
                          \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                          \AttributeTok{niter =}\NormalTok{ n.iter,}
                          \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                          \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

Let's inspect the numerical summaries for the regression parameters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phiwlp, }\AttributeTok{params =} \StringTok{"beta"}\NormalTok{, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd  2.5%   50% 97.5% Rhat n.eff
## beta[1]  0.25 0.10  0.04  0.25  0.45    1  1472
## beta[2] -0.02 0.09 -0.20 -0.02  0.17    1  1555
\end{verbatim}

Wing length does not seem to explain much individual-to-individual variation in survival -- the posterior distribution of the slope (\texttt{beta{[}2{]}}) is centered on 0 as we can see from the credible interval.

Let's plot the relationship between survival and wing length. First, we gather the values generated from the posterior distribution of the regression parameters in the two chains:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phiwlp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{], }\CommentTok{\# intercept, chain 1}
\NormalTok{           mcmc.phiwlp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# intercept, chain 2}
\NormalTok{beta2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phiwlp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{], }\CommentTok{\# slope, chain 1}
\NormalTok{           mcmc.phiwlp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# slope, chain 2}
\end{Highlighting}
\end{Shaded}

Then we define a grid of values for wing length, and predict survival for each MCMC iteration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predicted\_survival }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }
                             \AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(beta1), }
                             \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(beta1))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))\{}
\NormalTok{    predicted\_survival[i,j] }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(beta1[i] }\SpecialCharTok{+} 
\NormalTok{                               beta2[i] }\SpecialCharTok{*}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[j])}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we calculate posterior mean and the credible interval (note the ordering):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_survival }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survival, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{lci }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survival, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{uci }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survival, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{ord }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{wing\_length =}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[ord],}
                 \AttributeTok{survival =}\NormalTok{ mean\_survival[ord],}
                 \AttributeTok{lci =}\NormalTok{ lci[ord],}
                 \AttributeTok{uci =}\NormalTok{ uci[ord])}
\end{Highlighting}
\end{Shaded}

Now time to visualize:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wing\_length, }\AttributeTok{y =}\NormalTok{ survival) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lci, }\AttributeTok{ymax =}\NormalTok{ uci), }
              \AttributeTok{fill =} \StringTok{"grey70"}\NormalTok{, }
              \AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"wing length"}\NormalTok{, }\AttributeTok{y =} \StringTok{"estimated survival"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-274-1.pdf}}

The flat relationship between survival and wing length is confirmed.

\subsection{Several covariates}\label{several-covariates}

You may wish to have an effect of both sex and wing length in a model. Let's consider an additive effect of both covariates. We use a covariate \texttt{sex} that takes value 0 if male, and 1 if female. We have \(\text{logit}(\phi_i) = \beta_1 + \beta_2 \text{sex}_i + \beta_3 \text{winglength}_i\) for bird \(i\), so that male survival is \(\beta_1 + \beta_3 \text{winglength}_i\) and female survival is \(\beta_1 + \beta_2 + \beta_3 \text{winglength}_i\) (both on the logit scale). The relationship between survival and wing length is parallel between males and females, on the logit scale, and the gap between the two is measured by \(\beta_2\) (hence the term \emph{additive} effect).

The NIMBLE code is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phisexwlp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \FunctionTok{logit}\NormalTok{(phi[i]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ sex[i] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ winglength[i]}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}           \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1}           \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# intercept male}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# difference bw male and female}
\NormalTok{  beta[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# slope wing length}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, i])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We put constants and data in lists:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\NormalTok{wing.length.st }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{scale}\NormalTok{(dipper}\SpecialCharTok{$}\NormalTok{wing\_length))}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{winglength =}\NormalTok{ wing.length.st,}
                     \AttributeTok{sex =} \FunctionTok{if\_else}\NormalTok{(dipper}\SpecialCharTok{$}\NormalTok{sex }\SpecialCharTok{==} \StringTok{"M"}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We write a fuction to generate initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

We specify the parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And now we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phisexwlp }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phisexwlp, }
                             \AttributeTok{constants =}\NormalTok{ my.constants,}
                             \AttributeTok{data =}\NormalTok{ my.data,              }
                             \AttributeTok{inits =}\NormalTok{ initial.values,}
                             \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                             \AttributeTok{niter =}\NormalTok{ n.iter,}
                             \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                             \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

Let's display numerical summaries for all parameters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phisexwlp, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd  2.5%   50% 97.5% Rhat n.eff
## beta[1]  0.47 0.24 -0.02  0.48  0.94 1.01   102
## beta[2] -0.43 0.43 -1.28 -0.44  0.46 1.01    85
## beta[3] -0.19 0.20 -0.60 -0.19  0.22 1.00   106
## p        0.89 0.03  0.83  0.90  0.94 1.01   643
\end{verbatim}

The slope \texttt{beta{[}3{]}} is the same for both males and females. Although its posterior mean is negative, its crebible interval suggests that its posterior distribution largely encompasses 0, therefore a very weak signal, if any.

Let's visualize survival as a function of wing length for both sexes. First we put together the values from the two chains we generated in the posterior distributions of the regression parameters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{], }\CommentTok{\# beta1 chain 1}
\NormalTok{           mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[1]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# beta1 chain 2}
\NormalTok{beta2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{], }\CommentTok{\# beta2 chain 1}
\NormalTok{           mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[2]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# beta2 chain 2}
\NormalTok{beta3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{\textquotesingle{}beta[3]\textquotesingle{}}\NormalTok{], }\CommentTok{\# beta3 chain 1}
\NormalTok{           mcmc.phisexwlp}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{\textquotesingle{}beta[3]\textquotesingle{}}\NormalTok{]) }\CommentTok{\# beta3 chain 2}
\end{Highlighting}
\end{Shaded}

We get survival estimates for each MCMC iteration:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predicted\_survivalM }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(beta1), }
                              \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))}
\NormalTok{predicted\_survivalF }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(beta1), }
                              \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(beta1))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))\{}
\NormalTok{    predicted\_survivalM[i,j] }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(beta1[i] }\SpecialCharTok{+} 
\NormalTok{                                beta3[i] }\SpecialCharTok{*}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[j]) }
\NormalTok{    predicted\_survivalF[i,j] }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(beta1[i] }\SpecialCharTok{+} 
\NormalTok{                                beta2[i] }\SpecialCharTok{+} 
\NormalTok{                                beta3[i] }\SpecialCharTok{*}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[j])}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

From here, we may calculate posterior mean and credible intervals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_survivalM }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalM, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{lciM }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalM, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{uciM }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalM, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{mean\_survivalF }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalF, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{lciF }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalF, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{uciF }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_survivalF, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{prob =} \FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{)}
\NormalTok{ord }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{wing\_length =} \FunctionTok{c}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength[ord], }
\NormalTok{                                 my.constants}\SpecialCharTok{$}\NormalTok{winglength[ord]),}
                 \AttributeTok{survival =} \FunctionTok{c}\NormalTok{(mean\_survivalM[ord], }
\NormalTok{                              mean\_survivalF[ord]),}
                 \AttributeTok{lci =} \FunctionTok{c}\NormalTok{(lciM[ord],lciF[ord]),}
                 \AttributeTok{uci =} \FunctionTok{c}\NormalTok{(uciM[ord],uciF[ord]),}
                 \AttributeTok{sex =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\FunctionTok{length}\NormalTok{(mean\_survivalM)), }
                         \FunctionTok{rep}\NormalTok{(}\StringTok{"female"}\NormalTok{, }\FunctionTok{length}\NormalTok{(mean\_survivalF))))}
\end{Highlighting}
\end{Shaded}

Now on a plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wing\_length, }\AttributeTok{y =}\NormalTok{ survival, }\AttributeTok{color =}\NormalTok{ sex) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ lci, }\AttributeTok{ymax =}\NormalTok{ uci, }\AttributeTok{fill =}\NormalTok{ sex), }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"wing length"}\NormalTok{, }\AttributeTok{y =} \StringTok{"estimated survival"}\NormalTok{, }\AttributeTok{color =} \StringTok{""}\NormalTok{, }\AttributeTok{fill =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-285-1.pdf}}

Note that the two curves are not exactly parallel because we back-transformed the linear part of the relationship between survival and wing length. You may check that parallelism occurs on the logit scale:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predicted\_lsurvivalM }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(beta1), }
                              \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))}
\NormalTok{predicted\_lsurvivalF }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{length}\NormalTok{(beta1), }
                              \AttributeTok{ncol =} \FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(beta1))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength))\{}
\NormalTok{    predicted\_lsurvivalM[i,j] }\OtherTok{\textless{}{-}}\NormalTok{ beta1[i] }\SpecialCharTok{+}\NormalTok{ beta3[i] }\SpecialCharTok{*}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[j] }
\NormalTok{    predicted\_lsurvivalF[i,j] }\OtherTok{\textless{}{-}}\NormalTok{ beta1[i] }\SpecialCharTok{+}\NormalTok{ beta2[i] }\SpecialCharTok{+}\NormalTok{ beta3[i] }\SpecialCharTok{*}\NormalTok{ my.constants}\SpecialCharTok{$}\NormalTok{winglength[j]}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{mean\_lsurvivalM }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_lsurvivalM, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{mean\_lsurvivalF }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(predicted\_lsurvivalF, }\DecValTok{2}\NormalTok{, mean)}
\NormalTok{ord }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{wing\_length =} \FunctionTok{c}\NormalTok{(my.constants}\SpecialCharTok{$}\NormalTok{winglength[ord], }
\NormalTok{                                 my.constants}\SpecialCharTok{$}\NormalTok{winglength[ord]),}
                 \AttributeTok{survival =} \FunctionTok{c}\NormalTok{(mean\_lsurvivalM[ord], }
\NormalTok{                              mean\_lsurvivalF[ord]),}
                 \AttributeTok{sex =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\FunctionTok{length}\NormalTok{(mean\_lsurvivalM)), }
                         \FunctionTok{rep}\NormalTok{(}\StringTok{"female"}\NormalTok{, }\FunctionTok{length}\NormalTok{(mean\_lsurvivalF))))}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ wing\_length, }\AttributeTok{y =}\NormalTok{ survival, }\AttributeTok{color =}\NormalTok{ sex) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{ylim}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"wing length"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"estimated survival (on the lgit scale)"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-286-1.pdf}}

\subsection{Random effects}\label{random-effects}

If individual variation in survival is not fully explained by our covariates, we may add random effects through \(\text{logit}(\phi_i) = \beta_1 + \beta_2 x_i + \varepsilon_i\) where \(\varepsilon_i \sim N(0,\sigma^2)\). We consider individual variation in this section, but the reasoning holds for temporal variation. In essence, we are treating the individual survival probabilities \(\phi_i\) as a sample from a population of survival probabilities, and we assume a normal distribution with mean the linear component (with or without a covariate) and standard deviation \(\sigma\). The variation that is unexplained by the covariate \(x_i\) is measured by the variation \(\sigma\) in the residuals \(\varepsilon_i\).

Why is it important? Ignoring individual heterogeneity generated by individuals having contrasted performances over life may mask senescence or hamper the understanding of life history trade-offs. Overall, failing to incorporate unexplained residual variance may induce bias in parameter estimates and lead to detecting an effect of the covariate more often than it should be.

Let's go back to our dipper example with wing length as a covariate, and write the NIMBLE code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phiwlrep }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{    p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{    omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \FunctionTok{logit}\NormalTok{(phi[i]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ winglength[i] }\SpecialCharTok{+}\NormalTok{ eps[i]}
\NormalTok{    eps[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =}\NormalTok{ sdeps)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}           \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1}           \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{  \}}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{  sdeps }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, i])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

The prior on the standard deviation of the random effect is uniform between 0 and 10.

We now write a function for generating initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{1.5}\NormalTok{),}
                                  \AttributeTok{sdeps =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

We specify the parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"beta"}\NormalTok{, }\StringTok{"sdeps"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We increase the number of iterations and the length of the burn-in period to reach better convergence:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

And finally, we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phiwlrep }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phiwlrep, }
                            \AttributeTok{constants =}\NormalTok{ my.constants,}
                            \AttributeTok{data =}\NormalTok{ my.data,              }
                            \AttributeTok{inits =}\NormalTok{ initial.values,}
                            \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                            \AttributeTok{niter =}\NormalTok{ n.iter,}
                            \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                            \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We inspect the numerical summaries:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phiwlrep, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd  2.5%   50% 97.5% Rhat n.eff
## beta[1]  0.22 0.12 -0.02  0.22  0.44 1.24  1335
## beta[2] -0.01 0.10 -0.20 -0.01  0.19 1.02  1894
## p        0.90 0.03  0.84  0.90  0.95 1.03   752
## sdeps    0.27 0.25  0.01  0.16  0.83 4.68    12
\end{verbatim}

The effective sample size for the standard deviation of the random effect is very low. Let's try something else. We use non-centering to reparameterize our model. \textbf{Explain.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \FunctionTok{logit}\NormalTok{(phi[i]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ winglength[i] }\SpecialCharTok{+}\NormalTok{ sdeps }\SpecialCharTok{*}\NormalTok{ eps[i]}
\NormalTok{    eps[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

After running NIMBLE, we inspect the numerical summaries, and see that effective sample sizes are much better:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phiwlrep, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd  2.5%   50% 97.5% Rhat n.eff
## beta[1]  0.21 0.12 -0.04  0.21  0.43 1.00  1202
## beta[2] -0.01 0.10 -0.20 -0.01  0.19 1.00  1789
## p        0.90 0.03  0.83  0.90  0.95 1.01   790
## sdeps    0.40 0.26  0.02  0.37  0.95 1.01   170
\end{verbatim}

\subsection{Individual time-varying covariates}\label{agecov}

So far, we allowed covariates to vary along a single dimension, either time or individual. What if we need to consider a covariate that varies from one animal to the other, and over time. Think of age for example, its value is specific to each individual, and it (sadly) changes over time.

Age has a particular meaning in the capture-recapture framework. It is the time elapsed since first encounter, which is a proxy of true age, but not true age. If age is known at first encounter, then it is true age. For example, if the dippers were marked as young, then we would have the true biological age of each bird.

The convenient thing is that age has no missing value because age at \(t+1\) is just age at \(t\) to which we add 1 (we will see an example of an individual covariate with missing values in Section \ref{naincov}). This suggests a way to code the age effect in NIMBLE as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phiage.in }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ first[i]}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
    \CommentTok{\# phi1 = beta1 + beta2; phi1+ = beta1}
    \FunctionTok{logit}\NormalTok{(phi[i,t]) }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*} \FunctionTok{equals}\NormalTok{(t, first[i]) }
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i,t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{0}             \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1}             \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# phi1+}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# phi1 {-} phi1+}
\NormalTok{  phi1plus }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(beta[}\DecValTok{1}\NormalTok{])         }\CommentTok{\# phi1+}
\NormalTok{  phi1 }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ beta[}\DecValTok{2}\NormalTok{])   }\CommentTok{\# phi1}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, i, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

Here we used \texttt{equals(t,\ first{[}i{]})} which renders 1 when \(t\) is the first encounter \texttt{first{[}i{]}} and 0 otherwise. Therefore we distinguish survival over the first interval after first encounter \(\phi_1\) (\texttt{logit(phi{[}i,t{]})\ \textless{}-\ beta{[}1{]}\ +\ beta{[}2{]}}) from survival afterwards \(\phi_{1+}\) (\texttt{logit(phi{[}i,t{]})\ \textless{}-\ beta{[}1{]}}).

We put all constants in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first)}
\end{Highlighting}
\end{Shaded}

And the data in a list:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We write a function to generate initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

And specify the parameters to be monitored:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi1"}\NormalTok{, }\StringTok{"phi1plus"}\NormalTok{, }\StringTok{"p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We now run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phi.age.in }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phiage.in, }
                              \AttributeTok{constants =}\NormalTok{ my.constants,}
                              \AttributeTok{data =}\NormalTok{ my.data,              }
                              \AttributeTok{inits =}\NormalTok{ initial.values,}
                              \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                              \AttributeTok{niter =}\NormalTok{ n.iter,}
                              \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                              \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We display the results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phi.age.in, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd 2.5%  50% 97.5% Rhat n.eff
## p        0.89 0.03 0.83 0.90  0.94 1.00   402
## phi1     0.56 0.03 0.49 0.55  0.63 1.01   689
## phi1plus 0.57 0.04 0.50 0.57  0.64 1.00   309
\end{verbatim}

Age or time elapsed since first encounter does not seem to have an effect on survival here.

Another method to include an age effect is to create an individual by time covariate and use nested indexing (as in the flood/nonflood example) to distinguish survival over the interval after first detection from survival afterwards:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{ncol =} \FunctionTok{ncol}\NormalTok{(y) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(age))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(age))\{}
    \ControlFlowTok{if}\NormalTok{ (j }\SpecialCharTok{==}\NormalTok{ first[i]) age[i,j] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# age = 1}
    \ControlFlowTok{if}\NormalTok{ (j }\SpecialCharTok{\textgreater{}}\NormalTok{ first[i]) age[i,j] }\OtherTok{\textless{}{-}} \DecValTok{2}  \CommentTok{\# age \textgreater{} 1}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now we may write the NIMBLE code for this model. We just need to remember that survival is no longer defined on the logit scale as in the previous model, so we simply use uniform priors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hmm.phiage.out }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{  p }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p    }\CommentTok{\# Pr(alive t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p        }\CommentTok{\# Pr(alive t {-}\textgreater{} detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# Pr(dead t {-}\textgreater{} detected t)}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ first[i]}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{    phi[i,t] }\OtherTok{\textless{}{-}}\NormalTok{ beta[age[i,t]] }\CommentTok{\# beta1 = phi1, beta2 = phi1+}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i,t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{0}           \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{    gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1}           \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  beta[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# phi1}
\NormalTok{  beta[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# phi1+}
\NormalTok{  phi1 }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{1}\NormalTok{]}
\NormalTok{  phi1plus }\OtherTok{\textless{}{-}}\NormalTok{ beta[}\DecValTok{2}\NormalTok{]}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(alive t = 1) = 1}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = 1) = 0}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{T)\{}
\NormalTok{      z[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,j}\DecValTok{{-}1}\NormalTok{], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, i, j}\DecValTok{{-}1}\NormalTok{])}
\NormalTok{      y[i,j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,j], }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We put all constants in a list, including the age covariate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{T =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{age =}\NormalTok{ age)}
\end{Highlighting}
\end{Shaded}

We re-write a function to generate initial values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y}
\NormalTok{zinits[zinits }\SpecialCharTok{==} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{beta =} \FunctionTok{runif}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{p =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)}
\end{Highlighting}
\end{Shaded}

And we run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.phi.age.out }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phiage.out, }
                               \AttributeTok{constants =}\NormalTok{ my.constants,}
                               \AttributeTok{data =}\NormalTok{ my.data,              }
                               \AttributeTok{inits =}\NormalTok{ initial.values,}
                               \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                               \AttributeTok{niter =}\NormalTok{ n.iter,}
                               \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                               \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We display numerical summaries for the model parameters, and acknowledge that we obtain similar results to the other parameterization:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.phi.age.out, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          mean   sd 2.5%  50% 97.5% Rhat n.eff
## p        0.90 0.03 0.84 0.90  0.95 1.02   438
## phi1     0.55 0.03 0.48 0.55  0.62 1.00   878
## phi1plus 0.57 0.04 0.50 0.57  0.64 1.02  1048
\end{verbatim}

Like I mentioned earlier, age is easy to deal with as it does not contain missing values. Now think of size or body mass for a minute. The problem is that we cannot record size or body mass when an animal is non-detected. The easiest way to cope with individual time-varying covariates is to discretize e.g.~in small, medium and large as in Chapter \ref{dispersal}. Another option is to come up with a model for the covariate and fill in missing values by simulating from this model (see case study in Section \ref{naincov}).

\section{Why Bayes? Incorporate prior information}\label{elicitprior}

\subsection{Prior elicitation}\label{prior-elicitation}

Before we close this section, I'd like to cover one last topic. Think again of the CJS model with constant parameters. So far, we have assumed a non-informative prior on survival \(\text{Beta}(1,1) = \text{Uniform}(0,1)\). With this prior, we have seen in Section \ref{cjsderivatives} that mean posterior survival is \(\phi = 0.56\) with credible interval \([0.52,0.62]\).

The thing is that we know a lot about passerines and it is a shame not to be able to use this information and act as if we have to start from scratch and know nothing.

We illustrate how to incorporate prior information by acknowledging that species with similar body masses have similar survival. By gathering information on several other European passerines than the dipper, let's assume we have built a regression of survival vs.~body mass -- an allometric relationship.

Now knowing dippers weigh on average 59.8g, we're in the position to build a prior for dipper survival probability by predicting its value using the regression. We obtain a predicted survival of 0.57 and a standard deviation of 0.075. Using an informative prior \texttt{phi\ \textasciitilde{}\ dnorm(0.57,\ sd\ =\ 0.073)} in NIMBLE instead of our \texttt{phi\ \textasciitilde{}\ dunif(0,1)}, we get a mean posterior of \(0.56\) with credible interval \([0.52, 0.61]\). There's barely no difference with the non-informative prior, quite a disappointment.

Now let's assume that we had only the three first years of data, what would have happened? We fit the model with constant parameters with both the non-informative and informative priors to the dataset from which we delete the final 4 years of data. Now the benefit of using the prior information becomes clear as the credible interval when prior information is ignored has a width of 0.53, which is more than twice as much as when prior information is used (0.24), illustrating the increased precision provided by the prior. We may assess visually this gain in precision by comparing the survival posterior distributions with and without informative prior:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-312-1.pdf}}

If the aim is to get an estimate of survival, Gilbert did not have to conduct further data collection after 3 years, and he could have reached the same precision as with 7 years of data by using prior information derived from body mass. In brief, the prior information was worth 4 years of field data. Of course, this is assuming that the ecological question remains the same whether you have 3 or 7 years of data, which is unlikely to be the case, as with long-term data, there is so much we can ask, more than ``just'' what annual survival probability is.

\subsection{Moment matching}\label{moment-matching}

The prior \texttt{phi\ \textasciitilde{}\ dnorm(0.57,\ sd\ =\ 0.073)} is not entirely satisfying because it is not constrained to be positive or less than one, which is the minimum for a probability (of survival) to be well defined. In our specific example, the prior distribution is centered on positive values far from 0, and the standard deviation is small enough so that the chances to get values smaller than 0 or higher than 1 are null (to convince yourself, just type in \texttt{hist(rnorm(1000,\ mean\ =\ 0.57,\ sd\ =\ 0.073))} in R). Can we do better? The answer is yes.

Remember the Beta distribution? Recall that the Beta distribution is a continuous distribution with values between 0 and 1, which is very convenient to specify priors for survival and detection probabilities. Plus we know everything about the Beta distribution, in particular its moments. If \(X \sim Beta(\alpha,\beta)\), then the first (mean) and second moments (variance) of \(X\) are \(\mu = \text{E}(X) = \frac{\alpha}{\alpha + \beta}\) and \(\sigma^2 = \text{Var}(X) = \frac{\alpha\beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}\).

In the capture-recapture example, we know a priori that the mean of the probability we're interested in is \(\mu = 0.57\) and its variance is \(\sigma^2 = 0.073^2\). Parameters \(\mu\) and \(\sigma^2\) are seen as the moments of a \(Beta(\alpha,\beta)\) distribution. Now we look for values of \(\alpha\) and \(\beta\) that match the observed moments of the Beta distribution \(\mu\) and \(\sigma^2\). We need another set of equations:

\[\alpha = \bigg(\frac{1-\mu}{\sigma^2}- \frac{1}{\mu} \bigg)\mu^2\]
\[\beta = \alpha \bigg(\frac{1}{\mu}-1\bigg)\]
For our model, that means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(alpha }\OtherTok{\textless{}{-}}\NormalTok{ ( (}\DecValTok{1} \SpecialCharTok{{-}} \FloatTok{0.57}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FloatTok{0.073}\SpecialCharTok{*}\FloatTok{0.073}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\FloatTok{0.57}\NormalTok{) )}\SpecialCharTok{*}\FloatTok{0.57}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\# [1] 25.65}
\NormalTok{(beta }\OtherTok{\textless{}{-}}\NormalTok{ alpha }\SpecialCharTok{*}\NormalTok{ ( (}\DecValTok{1}\SpecialCharTok{/}\FloatTok{0.57}\NormalTok{) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\DocumentationTok{\#\# [1] 19.35}
\end{Highlighting}
\end{Shaded}

Now we simply have to use \texttt{phi\ \textasciitilde{}\ dbeta(25.6,19.3)} as a prior instead of our \texttt{phi\ \textasciitilde{}\ dnorm(0.57,\ sd\ =\ 0.073)}.

\section{Summary}\label{summary-3}

\begin{itemize}
\item
  The CJS model is a HMM with time-varying parameters to account for variation due to environmental conditions in survival or to sampling effort in detection.
\item
  Covariates can be considered to try and explain temporal and/or individual variation in survival and detection probabilities. If needed, random effects can be added to cope with unexplained variation.
\item
  For model comparison, the WAIC can be used to evaluate the relative predictive performance of capture-recapture models.
\item
  Statistical models rely on assumptions, and the CJS model makes no exception. There are procedures to assess the goodness of fit of the CJS model to capture-recapture data.
\end{itemize}

\section{Suggested reading}\label{suggested-reading-3}

\begin{itemize}
\item
  \citet{buckland2016} provides a historical perspective on the CJS model and anecdotes. The monography by \citet{LebretonEtAl1992} is a must-read to better understand the CJS model and its applications. The HMM formulation of the CJS model was proposed by \citet{gimenez2007} and \citet{royle2008}.
\item
  \citet{gimenez2009} deals with parameter redundancy in capture-recapture models in a Bayesian framework. For an exhaustive treatment, see \citet{cole2020} excellent book.
\item
  Relative to model comparison, I warmly recommend \citet{mcelreathbook} to better understand WAIC (there is a video as well, see \url{https://www.youtube.com/watch?v=vSjL2Zc-gEQ}). The paper by \citet{gelman2004} is also much helpful.
\item
  On posterior predictive checks, you may check out \citet{conn2018}. The \texttt{R2ucare} package is introduced in \citet{gimenez2018r2ucare}.
\item
  Temporal heterogeneity is addressed in papers by \citet{grosbois_assessing_2008} and \citet{frederiksen2014}, while individual heterogeneity is reviewed by \citet{gimenez2018ih}.
\item
  Regarding covariates, I did not use the formalization of linear models and sticked to an intuitive (hopefully) illustration of the use of covariates. More details can be found in Chapter 6 of \citet{cooch2017intromark}.
\item
  The example on how to incorporate prior information is in \citet{mccarthy2005}.
\end{itemize}

\chapter{Sites and states}\label{dispersal}

\section{Introduction}\label{introduction-6}

In this fifth chapter, you will learn about the Arnason-Schwarz model that allows estimating transitions between sites and states based on capture-recapture data. You will also see how to deal with uncertainty in the assignment of states to individuals.

\section{The Arnason-Schwarz (AS) model}\label{ASmodel}

In Chapter \ref{survival}, we got acquainted with the Cormack-Jolly-Seber (CJS) model which accommodates transitions between the states alive and dead while accounting for imperfect detection. It is often the case that besides being alive, more detailed information is collected on the state of animals when they are detected. For example, if the study area is split into several discrete sites, you may record where an animal is detected, the state being now alive in this particular site. The Arnason-Schwarz (AS) model can be viewed as an extension to the CJS model in which we estimate movements between sites on top of survival. The AS model is named after the two statisticians -- Neil Arnason and Carl Schwarz -- who came up with the idea.

\subsection{Theory}\label{theory}

Let's assume for now that we have two sites, say 1 and 2. The way we usually think of analyzing the data is to start from the detections and non-detections and infer the transitions between sites and movements. Schematically, when a animal is detected in site 1 or site 2, it obviously means that it is alive in that site, whereas when it is not detected, it may be dead or alive in either site. Schematically, we have:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-315-1.pdf}}

Observations and states are indeed closely related, but we do not have a perfect states to observations correspondence, and the HMM framework will help you make the distinction clear, which in turn will make the modelling easier. Usually, we focus our energy on the observations but what we'd really like is to spend time thinking of the ecological processes that we observed imperfectly. In the HMM framework, when we are to build a model, we think of the states and their dynamic over time, and these states emit the observations we're given to make. Going back to the our example, when an animal is alive in either site, it may get detected in that site or go undetected. When an animal is dead, then it goes undetected for sure. Schematically, we obtain:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-316-1.pdf}}

We have \(z = 1\) for alive in site 1, \(z = 2\) for alive in site 2 and \(z = 3\) for dead. We will code \(y = 1\) for non-detected, \(y = 2\) for detected in site 1 and \(y = 3\) for detected in site 2. The parameters are:\\
- \(\pi^r\) is the probability the a newly encountered individual is in state \(r\);\\
- \(p_t^r\) is the probability of detection at \(t\) for a bird in site \(r\) at \(t\);\\
- \(\phi_t^r\) is the survival probability for birds in site \(r\) at between \(t\) and \(t+1\);\\
- \(\psi_t^{rs}\) is the probability of being in site \(s\) at time \(t+1\) for animals that were in site \(r\) at \(t\) and have survived to \(t+1\), in short movement conditional on survival.

These parameters can be modelled as functions of covariates as in Section \ref{covariates}. But for now we will drop the time index for simplicity.

We follow the presentation of HMM as in Chapter \ref{hmmcapturerecapture}. Let's start with the vector of initial states. At first encounter, an animal may be alive in site 1 with probability \(\pi^1\) or in site 2 with the complementary probability \(1 - \pi^{1}\), but it cannot be dead. Therefore, we have:

\[\begin{matrix}
& \\
\mathbf{\delta} =
    \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=1 & z_t=2 & z_t=3 \\ \hdashline
\pi^1 & 1 - \pi^{1} & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
    \begin{matrix}
    \end{matrix}
\end{matrix}\]

We move on to the transition matrix which connects the states at \(t-1\) in rows to the states at \(t\) in columns. For example, the probability of moving from site 1 at \(t-1\) to site \(2\) at \(t\) is the product of the survival probability in site 1 over that time interval, times the probability of moving from site 1 to 2 for those animals who survived. We have:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_{t}=1 & z_{t}=2 & z_{t}=3 \\ \hdashline
\phi^1 (1-\psi^{12}) & \phi^1 \psi^{12} & 1 - \phi^1\\
\phi^2 \psi^{21} & \phi^2 (1-\psi^{21}) & 1 - \phi^2\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=1 \\ z_{t-1}=2 \\ z_{t-1}=3
    \end{matrix}
\end{matrix}\]

Finally, the observation matrix relates the states an animal is in at \(t\) in rows to the observations at \(t\) in columns. If an animal is dead (\(z_t=3\)), it cannot be detected (\(\Pr(y_t=1|z_t=3)=\Pr(y_t=2|z_t=3)=0\) and \(\Pr(y_t=3|z_t=3)=1\)), whereas when it is alive in either site, it can be detected or not. We have:

\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 & y_t=3 \\ \hdashline
1 - p^1 & p^1 & 0\\
1 - p^2 & 0 & p^2\\
1 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t}=1 \\ z_{t}=2 \\ z_{t}=3
    \end{matrix}
\end{matrix}\]

The vector of initial state probabilities sums up to one, as well as the rows of the transition and observation matrices.

\section{Fitting the AS model}\label{ASmodelfitting}

\subsection{Geese data}\label{geese-data}

To introduce this chapter, we will use data on the Canada goose (\emph{Branta canadensis}; geese hereafter) kindly provided by Jay Hestbeck (Figure \ref{fig:pixdipper}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/goose} 

}

\caption{Canada goose (Branta canadensis). Credit: Max McCarthy.}\label{fig:pixgeese}
\end{figure}

In total, 21277 geese were captured, marked with coded neck bands and recaptured between 1984 and 1989 in their wintering locations. Specifically, geese were monitored in the Atlantic flyway, in large areas along the East coast of the USA, namely 3 sites in the mid--Atlantic (New York, Pennsylvania, New Jersey), Chesapeake (Delaware, Maryland, Virginia), and Carolinas (North and South Carolina). Birds were adults and sub-adults when banded.

You may see the data below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"dat/geese.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.matrix}\NormalTok{()}
\FunctionTok{head}\NormalTok{(y)}
\DocumentationTok{\#\#      year\_1984 year\_1985 year\_1986 year\_1987 year\_1988}
\DocumentationTok{\#\# [1,]         0         2         2         0         0}
\DocumentationTok{\#\# [2,]         0         0         0         0         0}
\DocumentationTok{\#\# [3,]         0         0         0         1         0}
\DocumentationTok{\#\# [4,]         0         0         2         0         0}
\DocumentationTok{\#\# [5,]         0         3         0         0         3}
\DocumentationTok{\#\# [6,]         0         0         0         2         0}
\DocumentationTok{\#\#      year\_1989}
\DocumentationTok{\#\# [1,]         0}
\DocumentationTok{\#\# [2,]         2}
\DocumentationTok{\#\# [3,]         0}
\DocumentationTok{\#\# [4,]         0}
\DocumentationTok{\#\# [5,]         2}
\DocumentationTok{\#\# [6,]         0}
\end{Highlighting}
\end{Shaded}

The six columns are years in which the geese were captured, banded and recapture. A 0 stands for a non-detection, and detections were coded in the 3 wintering sites 1, 2 and 3 for mid--Atlantic, Chesapeake and Carolinas respectively. This is only a subsample of 500 individuals of the whole dataset that I will use for illustration here.

\subsection{NIMBLE implementation}\label{nimble-implementation-1}

To write the NIMBLE code corresponding to the AS model, we will make our life easier and start with 2 sites only -- we drop the Carolinas wintering site for now. We replace all 3's by 0's in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y[y}\SpecialCharTok{==}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

Also we consider parameters constant over time.

We start with comments to define the quantities -- parameters, states and observations -- we will use in the NIMBLE code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# phi1: survival probability site 1}
  \CommentTok{\# phi2: survival probability site 2}
  \CommentTok{\# psi12: movement probability from site 1 to site 2}
  \CommentTok{\# psi21: movement probability from site 2 to site 1}
  \CommentTok{\# p1: detection probability site 1}
  \CommentTok{\# p2: detection probability site 2}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (z):}
  \CommentTok{\# 1 alive at site 1}
  \CommentTok{\# 2 alive at site 2}
  \CommentTok{\# 3 dead}
  \CommentTok{\# Observations (y):}
  \CommentTok{\# 1 not seen}
  \CommentTok{\# 2 seen at site 1}
  \CommentTok{\# 3 seen at site 2}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The we specify priors for the survival, transition and detection probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# Priors}
\NormalTok{  phi1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  phi2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psi12 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psi21 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  p1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  p2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

We now write the vector of initial state probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# initial state probabilities}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pi1          }\CommentTok{\# Pr(alive in 1 at t = first)}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi1      }\CommentTok{\# Pr(alive in 2 at t = first)}
\NormalTok{  delta[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}            \CommentTok{\# Pr(dead at t = first) = 0}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Actually, the initial state is known exactly: It is alive at site of initial capture, and \(\pi^1\) is the proportion of individuals first captured in site 1, there is no need to make it explicit in the model and estimate it. Therefore, in the likelihood, instead of \texttt{z{[}i,first{[}i{]}{]}\ \textasciitilde{}\ dcat(delta{[}1:3{]})}, you can use \texttt{z{[}i,first{[}i{]}{]}\ \textless{}-\ y{[}i,first{[}i{]}{]}\ -\ 1} and just forget about \(\pi^1\), for now. Note that the same trick applies to the CJS model.

We write the transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
  \CommentTok{\# (read as gamma[z(t),z(t+1)] = gamma[fromState,toState])}

\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi12)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi12}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi1}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi21}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi21)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi2}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

In the same way, the observation matrix is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of y(t) given z(t)}
  \CommentTok{\# (read as omega[y(t),z(t)] = omega[Observation,State])}

\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p1     }\CommentTok{\# Pr(alive 1 t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p1         }\CommentTok{\# Pr(alive 1 t {-}\textgreater{} detected site 1 t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive 1 t {-}\textgreater{} detected site 2 t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p2     }\CommentTok{\# Pr(alive 2 t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive 2 t {-}\textgreater{} detected site 1 t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p2         }\CommentTok{\# Pr(alive 2 t {-}\textgreater{} detected site 2 t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected site 1 t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected site 2 t)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

At last, we are ready to specify the likelihood which, and this this the magic of HMM, is the same as the likelihood of the CJS model, only the vector of initial states, the transition and observation matrices were changed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \CommentTok{\# latent state at first capture}
\NormalTok{    z[i,first[i]] }\OtherTok{\textless{}{-}}\NormalTok{ y[i,first[i]] }\SpecialCharTok{{-}} \DecValTok{1}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{K)\{}
      \CommentTok{\# z(t) given z(t{-}1)}
\NormalTok{      z[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
      \CommentTok{\# y(t) given z(t)}
\NormalTok{      y[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We need to provide NIMBLE with constants, data, initial values, some parameters to monitor and MCMC details:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# occasions of first capture}
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=}\DecValTok{0}\NormalTok{)))}
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{first =}\NormalTok{ first, }
                     \AttributeTok{K =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y))}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\CommentTok{\# initial values }
\NormalTok{zinits }\OtherTok{\textless{}{-}}\NormalTok{ y }\CommentTok{\# say states are observations, detections in A or B are taken as alive in same sites }
\NormalTok{zinits[zinits}\SpecialCharTok{==}\DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{), }\FunctionTok{sum}\NormalTok{(zinits}\SpecialCharTok{==}\DecValTok{0}\NormalTok{), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# non{-}detections become alive in site A or B}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{()\{}\FunctionTok{list}\NormalTok{(}\AttributeTok{phi1 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{phi2 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{psi12 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{psi21 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{p1 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{p2 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{pi1 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{z =}\NormalTok{ zinits)\}}
\CommentTok{\# parameters to monitor}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi1"}\NormalTok{, }\StringTok{"phi2"}\NormalTok{,}\StringTok{"psi12"}\NormalTok{, }\StringTok{"psi21"}\NormalTok{, }\StringTok{"p1"}\NormalTok{, }\StringTok{"p2"}\NormalTok{, }\StringTok{"pi1"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

Now we may run NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mcmc.multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ multisite, }
                             \AttributeTok{constants =}\NormalTok{ my.constants,}
                             \AttributeTok{data =}\NormalTok{ my.data,              }
                             \AttributeTok{inits =}\NormalTok{ initial.values,}
                             \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                             \AttributeTok{niter =}\NormalTok{ n.iter,}
                             \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                             \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We may have a look to the results via a caterpillar plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCplot}\NormalTok{(mcmc.multisite)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-328-1.pdf}}

Remember mid--Atlantic is site 1, and Chesapeake site 2. Detection in mid--Atlantic (around 0.5) is higher than in Cheasapeake (around 0.4) although it comes with more uncertainty (wider credible interval). Survival in both sites are similar estimated at around 0.6--0.7. Note that by going multisite, we could make these parameters site-specific and differences might reflect habitat quality for example. Now the novelty lies in our capability to estimate movements from site 1 to site 2 and from site 2 to site 1 from a winter to the next. The annual probability of remaining in the same site for two successive winters, used as a measure of site fidelity, was lower in the mid--Atlantic (around 0.7) than in the Chesapeake (0.9). The estimated probability of moving to the Chesapeake from the mid--Atlantic was three times as high as the probability of moving in the opposite direction.

We may also have a look to numerical summaries, which confirm our ecological interpretation of the model parameter estimates:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multisite, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#       mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p1    0.56 0.10 0.37 0.56  0.76 1.00   157}
\DocumentationTok{\#\# p2    0.40 0.04 0.32 0.39  0.48 1.00   217}
\DocumentationTok{\#\# phi1  0.56 0.08 0.40 0.56  0.72 1.04   155}
\DocumentationTok{\#\# phi2  0.72 0.05 0.63 0.72  0.83 1.02   120}
\DocumentationTok{\#\# pi1   0.37 0.10 0.21 0.36  0.60 1.02    62}
\DocumentationTok{\#\# psi12 0.23 0.09 0.08 0.22  0.43 1.03   171}
\DocumentationTok{\#\# psi21 0.06 0.02 0.02 0.05  0.11 1.01   336}
\end{Highlighting}
\end{Shaded}

You could add time-dependence to the demographic parameters, e.g.~survival and movements, and assess the effect of winter harshness with some temporal covariates. Individual covariates could also be considered.

Before we move to the next section, I will illustrate the use of \texttt{nimbleEcology} to fit the AS to the geese data with 2 sites (see Section \ref{nimblemarginalization}). Using the function \texttt{dHMM()} which implements HMM with time-independent observation and transition matrices, we have:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in data}
\NormalTok{geese }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"geese.csv"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{as.matrix}\NormalTok{(geese)}
\CommentTok{\# drop Carolinas}
\NormalTok{y[y}\SpecialCharTok{==}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0} \CommentTok{\# act as if there was no detection in site 3 Carolinas}
\NormalTok{mask }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, sum)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y[mask}\SpecialCharTok{!=}\DecValTok{0}\NormalTok{,] }\CommentTok{\# remove rows w/ 0s only}
\CommentTok{\# get occasions of first encounter}
\NormalTok{get.first }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{min}\NormalTok{(}\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{!=} \DecValTok{0}\NormalTok{))}
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(y, }\DecValTok{1}\NormalTok{, get.first)}
\CommentTok{\# filter out individuals that are first captured at last occasion. }
\CommentTok{\# These individuals do not contribute to parameter estimation, }
\CommentTok{\# and also they cause problems with nimbleEcology}
\NormalTok{mask }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(first}\SpecialCharTok{!=}\FunctionTok{ncol}\NormalTok{(y)) }
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y[mask, ]                }\CommentTok{\# keep only these}
\NormalTok{first }\OtherTok{\textless{}{-}}\NormalTok{ first[mask]}

\CommentTok{\# NIMBLE code }
\NormalTok{multisite.marginalized }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# phi1: survival probability site 1}
  \CommentTok{\# phi2: survival probability site 2}
  \CommentTok{\# psi12: movement probability from site 1 to site 2}
  \CommentTok{\# psi21: movement probability from site 2 to site 1}
  \CommentTok{\# p1: recapture probability site 1}
  \CommentTok{\# p2: recapture probability site 2}
  \CommentTok{\# pi1: prop of being in site 1 at initial capture}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (z):}
  \CommentTok{\# 1 alive at 1}
  \CommentTok{\# 2 alive at 2}
  \CommentTok{\# 3 dead}
  \CommentTok{\# Observations (y):  }
  \CommentTok{\# 1 not seen}
  \CommentTok{\# 2 seen at site 1 }
  \CommentTok{\# 3 seen at site 2}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  
  \CommentTok{\# priors}
\NormalTok{  phi1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  phi2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psi12 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psi21 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  p1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  p2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  pi1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi12)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi12}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi1}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi21}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi21)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi2}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
  
  \CommentTok{\# probabilities of y(t) given z(t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p1     }\CommentTok{\# Pr(alive 1 t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p1        }\CommentTok{\# Pr(alive 1 t {-}\textgreater{} detected 1 t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive 1 t {-}\textgreater{} detected 2 t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p2     }\CommentTok{\# Pr(alive 2 t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive 2 t {-}\textgreater{} detected 1 t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p2         }\CommentTok{\# Pr(alive 2 t {-}\textgreater{} detected 2 t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected 1 t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t {-}\textgreater{} detected 2 t)}
  
  \CommentTok{\# likelihood }
  \CommentTok{\# initial state probs}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
\NormalTok{    init[i, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ gamma[ y[i, first[i] ] }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{ ] }\CommentTok{\# first state propagation}
\NormalTok{  \}}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i,(first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{K] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dHMM}\NormalTok{(}\AttributeTok{init =}\NormalTok{ init[i,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],  }\CommentTok{\# count data from first[i] + 1}
                               \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],     }\CommentTok{\# observation matrix}
                               \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],   }\CommentTok{\# transition matrix}
                               \AttributeTok{len =}\NormalTok{ K }\SpecialCharTok{{-}}\NormalTok{ first[i],           }\CommentTok{\# nb of occasions}
                               \AttributeTok{checkRowSums =} \DecValTok{0}\NormalTok{)             }\CommentTok{\# do not check whether elements in a row sum up to 1}
\NormalTok{  \}}
\NormalTok{\})}
\CommentTok{\# constants}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{first =}\NormalTok{ first, }
                     \AttributeTok{K =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y))}
\CommentTok{\# data}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\CommentTok{\# initial values }
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{()\{}\FunctionTok{list}\NormalTok{(}\AttributeTok{phi1 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{phi2 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{psi12 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{psi21 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{p1 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }
                                  \AttributeTok{p2 =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))\}}
\CommentTok{\# parameters to monitor}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi1"}\NormalTok{, }\StringTok{"phi2"}\NormalTok{,}\StringTok{"psi12"}\NormalTok{, }\StringTok{"psi21"}\NormalTok{, }\StringTok{"p1"}\NormalTok{, }\StringTok{"p2"}\NormalTok{)}
\CommentTok{\# MCMC details}
\NormalTok{n.iter }\OtherTok{\textless{}{-}} \DecValTok{5000}
\NormalTok{n.burnin }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n.chains }\OtherTok{\textless{}{-}} \DecValTok{2}
\CommentTok{\# run NIMBLE}
\NormalTok{mcmc.multisite.marginalized }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ multisite.marginalized, }
                             \AttributeTok{constants =}\NormalTok{ my.constants,}
                             \AttributeTok{data =}\NormalTok{ my.data,              }
                             \AttributeTok{inits =}\NormalTok{ initial.values,}
                             \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                             \AttributeTok{niter =}\NormalTok{ n.iter,}
                             \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                             \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

We obtain:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multisite.marginalized, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#       mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p1    0.53 0.09 0.36 0.53  0.71    1   651}
\DocumentationTok{\#\# p2    0.40 0.04 0.32 0.39  0.49    1   618}
\DocumentationTok{\#\# phi1  0.60 0.05 0.51 0.60  0.71    1   935}
\DocumentationTok{\#\# phi2  0.70 0.04 0.63 0.70  0.76    1   804}
\DocumentationTok{\#\# psi12 0.27 0.06 0.17 0.26  0.39    1   973}
\DocumentationTok{\#\# psi21 0.07 0.02 0.04 0.07  0.11    1   965}
\end{Highlighting}
\end{Shaded}

There are slight differences in these parameters estimates compared to those we obtained earlier. This is probably due to the effective sample sizes being much bigger here (by a factor 3) with the marginalized likelihood for the same number of MCMC iterations.

\subsection{Goodness of fit}\label{gofas}

Like for the CJS model, we need to ensure that the AS model provides a good fit to the data. To do so, both posterior predictive checks and classical procedures can be used. I illustrate the use of posterior predictive checks in Section \ref{ppchecks}. With regard to classical tests, the goodness-of-fit testing procedures we covered in Section \ref{gof} for the CJS model have been extended to the AS model. These procedure are also available in the package \texttt{R2ucare}. While the transience and trap-dependence tests can be used on each site, it is worth mentionning a test that is specific to the AS model with several sites. The ``where before where after'' (WBWA) tests for memory, in other words it's a test for the Markovian property of the HMM in the particular context of capture-recapture. WBWA quantifies whether there is a relationship between where an animal has last been seen and where it will next be seen again. If a relationship exists, positive or negative, this suggests memory in the movements, and the probability for an animal of moving to a site at \(t\), given it is present in a site at \(t-1\) should be made dependent of where it was at \(t-2\). This is called a second-order Markov process.

Going back to the geese data, the WBWA test is implemented as follows on the whole dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(R2ucare)}
\NormalTok{geese }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv2}\NormalTok{(}\StringTok{"dat/allgeese.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as.matrix}\NormalTok{()}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ geese[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{]}
\NormalTok{size }\OtherTok{\textless{}{-}}\NormalTok{ geese[,}\DecValTok{7}\NormalTok{]}
\NormalTok{wbwa }\OtherTok{\textless{}{-}} \FunctionTok{test3Gwbwa}\NormalTok{(y, size)}
\NormalTok{wbwa}\SpecialCharTok{$}\NormalTok{test3Gwbwa}
\DocumentationTok{\#\#  stat    df p\_val }
\DocumentationTok{\#\# 472.9  20.0   0.0}
\end{Highlighting}
\end{Shaded}

There is clearly a strong (not to say significant) positive relationship judging by the value of the statistic. I will demonstrate how to account for this memory issue in an extension of the AS model in a case study in Section \ref{memorymodel}.

\section{What if more than 2 sites?}\label{what-if-more-than-2-sites}

So far we have considered two sites only, for the sake of simplicity. Indeed when going for three sites (or more), a difficulty arises. While the movement probabilities still need to be between 0 and 1, the sum of all probabilities of moving from a site should also sum to one. This is because an individual alive in site 1 for example, has to stay in 1, move to 2 or move to 3, it has no other choice. This is no problem when you have only two sites because the probability of moving from 1 to 2 is always estimated between 0 and 1, and its complementary, the probability of moving from 2 to 1 will be too. When you have three sites, it might happen that the sum of the estimates for \(\psi^{12}\) and \(\psi^{13}\) is larger than one, even though they're both between 0 and 1, which would make \(\psi^{11} = 1 - \psi^{12} - \psi^{13}\) negative.

There are basically two methods to fulfill both constraints, either to assign a Dirichlet (which is pronounced deer-eesh-lay) prior to the movement probabilities, or to use a multinomial logit link function.

\subsection{Dirichlet prior}\label{dirichletprior}

The Dirichlet distribution extends the Beta distribution multivariate we have seen previously (Figure \ref{fig:betadistribution}) for values between 0 and 1 that add up to 1. Going back to our example with 3 sites, we would like to come up with a prior for parameters of moving from 1, which are \(\psi^{11}\), \(\psi^{12}\) and \(\psi^{13}\). The Dirichlet distribution of dimension 3 has a vector of parameters \((\alpha_1, \alpha_2, \alpha_3)\) that controls its shape. The sum of all \(\alpha\)'s can be interpreted as a measure of precision: The higher the sum, the more peaked is the distribution on the mean value, the mean value along each dimension being the ratio of the corresponding over the sum of all \(\alpha\)'s. When all \(\alpha\)'s are equal, the distribution is symmetric (Figure \ref{fig:dirichletdistribution}). Its mean is \((1/3, 1/3, 1/3)\) in our example with 3 parameters, whatever the value of \(\alpha\). When \(\alpha_1 = \alpha_2 = \alpha_3 = 1\), we obtain a uniform marginal distribution for the \(\psi\)'s, while values below 1 (\(\alpha_1 = \alpha_2 = \alpha_3 = 0.1\)) result in the distribution concentrating in the corners (skewed U-shaped forms) and values above 1 (\(\alpha_1 = \alpha_2 = \alpha_3 = 10\)) result in unimodal marginal distributions.



\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gtools) }\CommentTok{\# to make the rdirichlet() function available}
\FunctionTok{library}\NormalTok{(ggtern) }\CommentTok{\# to visually represent multidim prob distribution }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }\CommentTok{\# for reproducibility}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{1000} \CommentTok{\# number of values drawn from Dirichlet distribution}
\NormalTok{alpha1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(.}\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{, .}\DecValTok{1}\NormalTok{)}
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{rdirichlet}\NormalTok{(n, alpha1)}
\NormalTok{alpha2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{rdirichlet}\NormalTok{(n, alpha2)}
\NormalTok{alpha3 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{rdirichlet}\NormalTok{(n, alpha3)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(p1, p2, p3), }\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"alpha = c(0.1, 0.1, 0.1)"}\NormalTok{, n),}
                                     \FunctionTok{rep}\NormalTok{(}\StringTok{"alpha = c(1, 1, 1)"}\NormalTok{, n),}
                                     \FunctionTok{rep}\NormalTok{(}\StringTok{"alpha = c(10, 10, 10)"}\NormalTok{, n))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{x =} \FunctionTok{as.numeric}\NormalTok{(V1),}
         \AttributeTok{y =} \FunctionTok{as.numeric}\NormalTok{(V2),}
         \AttributeTok{z =} \FunctionTok{as.numeric}\NormalTok{(V3),}
         \AttributeTok{alpha =}\NormalTok{ V4)}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggtern}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y, }\AttributeTok{z =}\NormalTok{ z)) }\SpecialCharTok{+}
  \FunctionTok{stat\_density\_tern}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{..level.., }\AttributeTok{alpha=}\NormalTok{..level..),}
                    \AttributeTok{geom =} \StringTok{\textquotesingle{}polygon\textquotesingle{}}\NormalTok{,}
                    \AttributeTok{bdl =} \FloatTok{0.005}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# a 2D kernel density estimation of the distribution}
  \FunctionTok{scale\_fill\_viridis\_b}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.3}\NormalTok{, }\AttributeTok{pch =} \StringTok{"+"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_showarrows}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_T\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_L\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_R\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{),}
                     \AttributeTok{labels =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.2}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{,}
       \AttributeTok{y =} \StringTok{""}\NormalTok{,}
       \AttributeTok{z =} \StringTok{""}\NormalTok{,}
       \AttributeTok{Tarrow =} \StringTok{"psi11"}\NormalTok{,}
       \AttributeTok{Larrow =} \StringTok{"psi12"}\NormalTok{,}
       \AttributeTok{Rarrow =} \StringTok{"psi13"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{color =} \StringTok{"none"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"none"}\NormalTok{, }\AttributeTok{alpha =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{alpha, }\AttributeTok{ncol =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/dirichletdistribution-1.pdf}}
\caption{\label{fig:dirichletdistribution}The Dirichlet distribution as a prior for \((\psi^{11}, \psi^{12}, \psi^{13})\) with vector of parameters \(\alpha\). Here all components of \(\alpha\) are equal which makes the distribution symmetric, and its mean is \((1/3, 1/3, 1/3)\). When \(\alpha = 1\), the prior for the \(\psi\)'s is uniform (middle panel), unimodal when \(\alpha = 10\) (right panel) and concentrated in the corners (0 and 1) when \(\alpha = 0.1\) (left panel).}
\end{figure}

Going back to our example, in NIMBLE, we consider a Dirichlet prior for each triplet of movement parameters, from site 1 (\(\psi^{11}\), \(\psi^{12}\) and \(\psi^{13}\)), from site 2 (\(\psi^{21}\), \(\psi^{22}\) and \(\psi^{23}\)) and from site 3 (\(\psi^{31}\), \(\psi^{32}\) and \(\psi^{33}\)).

We start by setting the scene with comments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# phi1: survival probability site 1}
  \CommentTok{\# phi2: survival probability site 2}
  \CommentTok{\# phi3: survival probability site 3}
  \CommentTok{\# psi11 = psi1[1]: movement probability from site 1 to site 1 (reference)}
  \CommentTok{\# psi12 = psi1[2]: movement probability from site 1 to site 2}
  \CommentTok{\# psi13 = psi1[3]: movement probability from site 1 to site 3 }
  \CommentTok{\# psi21 = psi2[1]: movement probability from site 2 to site 1}
  \CommentTok{\# psi22 = psi2[2]: movement probability from site 2 to site 2 (reference)}
  \CommentTok{\# psi23 = psi2[3]: movement probability from site 2 to site 3}
  \CommentTok{\# psi31 = psi3[1]: movement probability from site 3 to site 1}
  \CommentTok{\# psi32 = psi3[2]: movement probability from site 3 to site 2}
  \CommentTok{\# psi33 = psi3[3]: movement probability from site 3 to site 3 (reference)}
  \CommentTok{\# p1: recapture probability site 1}
  \CommentTok{\# p2: recapture probability site 2}
  \CommentTok{\# p3: recapture probability site 3}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (z):}
  \CommentTok{\# 1 alive at 1}
  \CommentTok{\# 2 alive at 2}
  \CommentTok{\# 2 alive at 3}
  \CommentTok{\# 3 dead}
  \CommentTok{\# Observations (y):  }
  \CommentTok{\# 1 not seen}
  \CommentTok{\# 2 seen at 1 }
  \CommentTok{\# 3 seen at 2}
  \CommentTok{\# 3 seen at 3}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# transitions: Dirichlet priors}
\NormalTok{  psi1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# psi11, psi12, psi13}
\NormalTok{  psi2[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# psi21, psi22, psi23}
\NormalTok{  psi3[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# psi31, psi32, psi33}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then we use these parameters (which now respect the constraints) to define the transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi1}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi2}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi3}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

When we fit this model to the geese dataset with the detections in the Carolinas wintering site back in, and with \texttt{alpha\ \textless{}-\ c(1,\ 1,\ 1)} passed to the constants, we obtain the following results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multisite, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#         mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p1      0.51 0.08 0.36 0.51  0.67 1.02   306}
\DocumentationTok{\#\# p2      0.45 0.05 0.36 0.45  0.55 1.00   217}
\DocumentationTok{\#\# p3      0.26 0.06 0.15 0.25  0.39 1.01   166}
\DocumentationTok{\#\# phi1    0.60 0.05 0.51 0.60  0.70 1.01   384}
\DocumentationTok{\#\# phi2    0.70 0.04 0.63 0.70  0.77 1.00   233}
\DocumentationTok{\#\# phi3    0.75 0.06 0.62 0.76  0.87 1.03   218}
\DocumentationTok{\#\# psi1[1] 0.74 0.05 0.63 0.75  0.84 1.01   818}
\DocumentationTok{\#\# psi1[2] 0.24 0.05 0.14 0.24  0.35 1.02   817}
\DocumentationTok{\#\# psi1[3] 0.02 0.02 0.00 0.01  0.07 1.05   487}
\DocumentationTok{\#\# psi2[1] 0.07 0.02 0.04 0.07  0.12 1.00   668}
\DocumentationTok{\#\# psi2[2] 0.84 0.04 0.75 0.84  0.90 1.00   292}
\DocumentationTok{\#\# psi2[3] 0.09 0.03 0.04 0.08  0.17 1.00   220}
\DocumentationTok{\#\# psi3[1] 0.02 0.01 0.00 0.02  0.06 1.00  1022}
\DocumentationTok{\#\# psi3[2] 0.22 0.05 0.12 0.21  0.33 1.01   525}
\DocumentationTok{\#\# psi3[3] 0.76 0.06 0.64 0.76  0.86 1.01   506}
\end{Highlighting}
\end{Shaded}

Survival probabilities are similar among sites, but the detection probability in Carolinas seems much lower than in the two other wintering sites. The estimated probability of moving to the Chesapeake from the Carolinas is 2 times as high as the probability of moving in the opposite direction.

In theory, you could include covariates as in \ref{covariates} through the \(\alpha\) parameters and the use a log link function (to ensure \(\alpha > 0\)), e.g.~\(\log(\alpha) = \beta_1 + \beta_2 x\). However, NIMBLE does not allow that. Fortunately, there is another way to specify the Dirichlet distribution through a ratio of gamma distributions that allows to incorporate covariates.

The gamma distribution is continuous. It has two parameters \(\alpha\) and \(\theta\) that control its shape and scale (Figure \ref{fig:gammadistribution}). Another parameterization considers its shape and rate which is the inverse of the scale.



\begin{figure}
\centering
\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/gammadistribution-1.pdf}}
\caption{\label{fig:gammadistribution}The distribution gamma(\(\alpha\),\(\theta\)) for different values of \(\alpha\) and \(\theta\). The shape argument \(\alpha\) determines the overall shape while the scale parameter \(\theta\) only affects the scale values (compare the values on X- and Y-axes between the bottom and upper panels). The exponential and chi-square distributions are particular cases of the gamma distribution. If the parameter shape is close to zero, the gamma is very similar to the exponential (bottom and upper left panels). If the parameter shape is large, then the gamma is similar to the chi-squared distribution (bottom and upper right panels).}
\end{figure}

If we have three independent random variables \(Y_1, Y_2\) and \(Y_3\) distributed as gamma distributions with shape parameters the \(\alpha\)'s and same scale parameter \(\theta\), that is \(Y_j \sim \text{Gamma}(\alpha_j, \theta)\), then it can be shown that the vector \((Y_1/V, Y_2/V, Y_3/V)\) is Dirichlet with vector of parameters the \(\alpha\)'s, where \(V\) is the sum of the \(Y\)'s (which is also gamma distributed). In NIMBLE, this suggests using \(\theta = 1\) to get a uniform prior:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
\CommentTok{\# transitions: Dirichlet priors with Gamma formulation}
\ControlFlowTok{for}\NormalTok{ (s }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)\{}
\NormalTok{  lpsi1[s] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dgamma}\NormalTok{(alpha[s], }\DecValTok{1}\NormalTok{) }\CommentTok{\# Y1, Y2, Y3 for psi11, psi12, psi13}
\NormalTok{  psi1[s] }\OtherTok{\textless{}{-}}\NormalTok{ lpsi1[s]}\SpecialCharTok{/}\NormalTok{V1 }\CommentTok{\# psi11, psi12, psi13}
\NormalTok{  lpsi2[s] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dgamma}\NormalTok{(alpha[s], }\DecValTok{1}\NormalTok{) }\CommentTok{\# Y\textquotesingle{}1, Y\textquotesingle{}2, Y\textquotesingle{}3 for psi21, psi22, psi23}
\NormalTok{  psi2[s] }\OtherTok{\textless{}{-}}\NormalTok{ lpsi2[s]}\SpecialCharTok{/}\NormalTok{V2 }\CommentTok{\# psi21, psi22, psi23}
\NormalTok{  lpsi3[s] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dgamma}\NormalTok{(alpha[s], }\DecValTok{1}\NormalTok{) }\CommentTok{\# Y\textquotesingle{}\textquotesingle{}1, Y\textquotesingle{}\textquotesingle{}2, Y\textquotesingle{}\textquotesingle{}3 for psi31, psi32, psi33}
\NormalTok{  psi3[s] }\OtherTok{\textless{}{-}}\NormalTok{ lpsi3[s]}\SpecialCharTok{/}\NormalTok{V3 }\CommentTok{\# psi31, psi32, psi33}
\NormalTok{\}}
\NormalTok{V1 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(lpsi1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{V2 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(lpsi2[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{V3 }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(lpsi3[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

From there, we can express the shape parameter of the gamma distribution (precisely the \(\alpha\)'s here) as a function of covariates as in \(\log(\alpha) = \beta_1 + \beta_2 x\) in the spirit of a generalized linear model with a gamma response.

\subsection{Multinomial logit}\label{multinomiallogit}

Another possibility to build a prior that ensures the movement probabilities are between 0 and 1 and sum up to 1 is to extend the logit link we used for the CJS model in Section \ref{covariates}. Remember we had \(\text{logit}(\phi) = \beta\), we specified a prior on \(\beta\) say \(\beta \sim N(0,1.5)\) then got a prior on \(\phi\) by back-transforming \(\beta\) with \(\phi = \text{logit}^{-1}(\beta)\).

Going back to our example with \(3\) sites, and focusing on the movement probabilities say, from site 1, we first choose a reference (or pivot) site, say 1, then \(\log\left(\displaystyle{\frac{\psi^{12}}{\psi^{11}}}\right) = \beta_2\) and \(\log\left(\displaystyle{\frac{\psi^{13}}{\psi^{11}}}\right) = \beta_3\). Interestingly, when exponentiated, the \(\beta\)'s here can be interpreted as the increase in the odds of moving versus staying on site resulting from a one-unit increase in the covariate. Any of the sites can be chosen to be the reference, this will not change the likelihood and you will get the same results. Now we specify a normal prior distribution for the \(\beta\)'s. Eventually, to back-transform, we use \(\psi^{12} = \displaystyle{\frac{\exp(\beta_2)}{1+\displaystyle{\exp(\beta_2)+\displaystyle{\exp(\beta_3)}}}}\) and \(\psi^{13} = \displaystyle{\frac{\exp(\beta_3)}{1+\displaystyle{\exp(\beta_2)+\displaystyle{\exp(\beta_3)}}}}\). The reference parameter, here \(\psi^{11}\), is calculated as \(\psi^{11} = \displaystyle{\frac{1}{1 + \displaystyle{\exp(\beta_2)+\displaystyle{\exp(\beta_3)}}}}\), or simply as the complementary probability \(\psi^{11} = 1 - \psi^{12} - \psi^{13}\).

Note that when there are only 2 sites instead of 3 or more, then the multinomial logit reduces to the logit link.

In NIMBLE, we write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# transitions: multinomial logit}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)\{}
    \CommentTok{\# normal priors on logit of all but one movement prob}
\NormalTok{    beta1[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{    beta2[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
\NormalTok{    beta3[i] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{)}
    \CommentTok{\# constrain the transitions such that their sum is \textless{} 1}
\NormalTok{    psi1[i] }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(beta1[i]) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta1[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta1[}\DecValTok{2}\NormalTok{]))}
\NormalTok{    psi2[i] }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(beta2[i]) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta2[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta2[}\DecValTok{2}\NormalTok{]))}
\NormalTok{    psi3[i] }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(beta3[i]) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta3[}\DecValTok{1}\NormalTok{]) }\SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(beta3[}\DecValTok{2}\NormalTok{]))}
\NormalTok{  \}}
  \CommentTok{\# reference movement probability}
\NormalTok{  psi1[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi1[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ psi1[}\DecValTok{2}\NormalTok{]}
\NormalTok{  psi2[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi2[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ psi2[}\DecValTok{2}\NormalTok{]}
\NormalTok{  psi3[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psi3[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ psi3[}\DecValTok{2}\NormalTok{]}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then we use these parameters (which now respect the constraints) to define the transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi1 }\SpecialCharTok{*}\NormalTok{ psi1[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi1}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi2 }\SpecialCharTok{*}\NormalTok{ psi2[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi2}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi3 }\SpecialCharTok{*}\NormalTok{ psi3[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi3}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

You may check that the results are very similar to those we obtained with the Dirichlet prior:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multisite, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#         mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p1      0.52 0.08 0.37 0.52  0.69 1.01   297}
\DocumentationTok{\#\# p2      0.46 0.05 0.37 0.45  0.57 1.13   209}
\DocumentationTok{\#\# p3      0.23 0.06 0.14 0.23  0.36 1.05   137}
\DocumentationTok{\#\# phi1    0.60 0.05 0.50 0.60  0.70 1.00   403}
\DocumentationTok{\#\# phi2    0.70 0.04 0.63 0.70  0.77 1.14   281}
\DocumentationTok{\#\# phi3    0.77 0.06 0.64 0.77  0.89 1.05   199}
\DocumentationTok{\#\# psi1[1] 0.74 0.06 0.62 0.74  0.83 1.00   727}
\DocumentationTok{\#\# psi1[2] 0.22 0.05 0.13 0.22  0.33 1.03   795}
\DocumentationTok{\#\# psi1[3] 0.04 0.03 0.01 0.04  0.12 1.05    88}
\DocumentationTok{\#\# psi2[1] 0.07 0.02 0.04 0.07  0.11 1.01   655}
\DocumentationTok{\#\# psi2[2] 0.83 0.04 0.74 0.84  0.90 1.04   153}
\DocumentationTok{\#\# psi2[3] 0.10 0.04 0.04 0.09  0.19 1.03   122}
\DocumentationTok{\#\# psi3[1] 0.03 0.02 0.01 0.02  0.07 1.01   794}
\DocumentationTok{\#\# psi3[2] 0.22 0.05 0.13 0.21  0.32 1.02   477}
\DocumentationTok{\#\# psi3[3] 0.76 0.05 0.64 0.76  0.85 1.02   444}
\end{Highlighting}
\end{Shaded}

Both the Dirichlet prior and the multinomial logit link give the same results, and there is not much difference in terms of runtime or quality of convergence, so you should use the option you feel the most comfortable with.

\section{Sites may be states}\label{states}

So far, we have considered geographical locations (or sites) to refine the alive information when an animal is detected. However, it was quickly realized that sites could actually be states defined by physiology or behavior, hence opening up an avenue for applications of capture-recapture models in many fields of ecology.

Examples of states include:

\begin{itemize}
\tightlist
\item
  Epidemiological or disease states: sick/healthy, uninfected/infected/recovered;\\
\item
  Morphological states: small/medium/big, light/medium/heavy;\\
\item
  Life-history states: e.g.~breeder/non-breeder, failed breeder, first-time breeder;\\
\item
  Developmental states: e.g.~juvenile/subadult/adult;\\
\item
  Social states: e.g.~solitary/group-living, subordinate/dominant;\\
\item
  Death states: e.g.~alive, dead from harvest, dead from natural causes.
\end{itemize}

In brief, states are individual, time-specific discrete covariates.

\subsection{Titis data}\label{titis-data}

To illustrate this section, we will consider data collected between 1942 and 1956 by Lance Richdale on the Sooty shearwaters (\emph{Ardenna grisea}), also known as titis (Figure \ref{fig:pixtiti}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/titi} 

}

\caption{Sooty shearwater (*Ardenna grisea*). Credit: John Harrison.}\label{fig:pixtiti}
\end{figure}

You may see the data below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{titis }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv2}\NormalTok{(}\StringTok{"dat/titis.csv"}\NormalTok{, }
                   \AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{titis }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{year\_1942 =}\NormalTok{ X1,}
         \AttributeTok{year\_1943 =}\NormalTok{ X2,}
         \AttributeTok{year\_1944 =}\NormalTok{ X3,}
         \AttributeTok{year\_1949 =}\NormalTok{ X4,}
         \AttributeTok{year\_1952 =}\NormalTok{ X5,}
         \AttributeTok{year\_1953 =}\NormalTok{ X6,}
         \AttributeTok{year\_1956 =}\NormalTok{ X7)}
\DocumentationTok{\#\# \# A tibble: 1,013 x 7}
\DocumentationTok{\#\#    year\_1942 year\_1943 year\_1944 year\_1949 year\_1952}
\DocumentationTok{\#\#        \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}}
\DocumentationTok{\#\#  1         0         0         0         0         0}
\DocumentationTok{\#\#  2         0         0         0         0         0}
\DocumentationTok{\#\#  3         0         0         0         0         0}
\DocumentationTok{\#\#  4         0         0         0         0         0}
\DocumentationTok{\#\#  5         0         0         0         0         0}
\DocumentationTok{\#\#  6         0         0         0         0         0}
\DocumentationTok{\#\#  7         0         0         0         0         0}
\DocumentationTok{\#\#  8         0         0         0         0         0}
\DocumentationTok{\#\#  9         0         0         0         0         0}
\DocumentationTok{\#\# 10         0         0         0         0         0}
\DocumentationTok{\#\# \# i 1,003 more rows}
\DocumentationTok{\#\# \# i 2 more variables: year\_1953 \textless{}dbl\textgreater{}, year\_1956 \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

In total, 1013 titis were captured, marked and recaptured on a small colony on Whero Island in southern New Zealand. These data were previously analyzed by Richard Scofield who kindly provided us with the data.

Following the way the data were collected, four states were originally considered: Alive breeder; Accompanied by another bird in a burrow; Alone in a burrow; On the surface; Dead. For simplicity, we pooled all alive states (except breeder) together in a non-breeder state (NB) that includes failed breeders (birds that had bred previously -- skip reproduction or divorce) and pre-breeders (birds that had yet to breed). Because burrows were not checked before hatching, some birds in the category NB might have already failed. Therefore birds in the breeder state (B) should be seen as successful breeders, and those in the NB state as nonbreeders plus prebreeders and failed breeders.

In summary, we code the states as 1 for alive and breeding, 2 for alive and non-breeding and 3 for dead. To make the modelling process more self-explanatory, we will use letters B, NB and D but you should keep in mind that these are actually numbers. Observations are non-detected coded as 1, and detected as breeder or non-breeder coded as 2 and 3 respectively.

The aim here is to study life-history trade--offs. Specifically, we ask the questions: Does breeding affect survival? Does breeding in current year affect breeding next year?

\subsection{The AS model for states}\label{the-as-model-for-states}

Basically, the AS model with states is the same model as in the geese example with two sites, see Sections \ref{ASmodel} and \ref{ASmodelfitting}.

If we let \(\phi^B\) and \(\phi^{NB}\) be the survival probabilities of breeders and non-breeders, \(\psi^{NBB}\) the probability of becoming breeder for a non-breeder, and \(\psi^{BNB}\) the probability of skipping reproduction, then the transition matrix is:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=B & z_t=NB & z_t=D \\ \hdashline
\phi^B (1-\psi^{BNB}) & \phi^B \psi^{BNB} & 1 - \phi^B\\
\phi^{NB} \psi^{NBB} & \phi^{NB} (1-\psi^{NBB}) & 1 - \phi^{NB}\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=B \\ z_{t-1}=NB \\ z_{t-1}=D
    \end{matrix}
\end{matrix}\]

The costs or reproduction would reflect in future reproduction if breeders have a lower probability of breed next year than non-breeders \(\psi^{BB} = 1 - \psi^{BNB} < \psi^{NBB}\) or in survival if the survival of breeders is lower than that of non-breeders \(\phi^B < \phi^{NB}\).

The observation matrix is:

\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 & y_t=3 \\ \hdashline
1 - p^B & p^B & 0\\
1 - p^{NB} & 0 & p^{NB}\\
1 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t}=B \\ z_{t}=NB \\ z_{t}=D
    \end{matrix}
\end{matrix}\]

where \(p^B\) and \(p^{NB}\) are the detection proabilities of non-breeders and breeders respectively.

\subsection{NIMBLE implementation}\label{nimble-implementation-2}

We first write the NIMBLE code, which is exactly the same as in the geese example with two sites, see Section \ref{ASmodelfitting}.

First some definitions which we have as comments in the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multistate }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# B is for breeder, NB for non{-}breeder}
  \CommentTok{\# phiB: survival probability state B}
  \CommentTok{\# phiNB: survival probability state NB}
  \CommentTok{\# psiBNB: transition probability from B to NB}
  \CommentTok{\# psiNBB: transition probability from NB to B}
  \CommentTok{\# pB: detection probability B}
  \CommentTok{\# pNB: detection probability NB}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (z):}
  \CommentTok{\# 1 alive B}
  \CommentTok{\# 2 alive NB}
  \CommentTok{\# 3 dead}
  \CommentTok{\# Observations (y):}
  \CommentTok{\# 1 not seen}
  \CommentTok{\# 2 seen as B}
  \CommentTok{\# 3 seen as NB}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then the priors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multistate }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# Priors}
\NormalTok{  phiB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  phiNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psiBNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psiNBB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  pB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  pNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multistate }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psiBNB)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiB }\SpecialCharTok{*}\NormalTok{ psiBNB}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiB}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiNB }\SpecialCharTok{*}\NormalTok{ psiNBB}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiNB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psiNBB)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiNB}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The observation matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multistate }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of y(t) given z(t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pB    }\CommentTok{\# Pr(alive B t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pB        }\CommentTok{\# Pr(alive B t {-}\textgreater{} detected B t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# Pr(alive B t {-}\textgreater{} detected NB t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pNB   }\CommentTok{\# Pr(alive NB t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# Pr(alive NB t {-}\textgreater{} detected B t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pNB       }\CommentTok{\# Pr(alive NB t {-}\textgreater{} detected NB t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}         \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# Pr(dead t {-}\textgreater{} detected N t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# Pr(dead t {-}\textgreater{} detected NB t)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

And the likelihood:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multistate }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \CommentTok{\# latent state at first capture}
\NormalTok{    z[i,first[i]] }\OtherTok{\textless{}{-}}\NormalTok{ y[i,first[i]] }\SpecialCharTok{{-}} \DecValTok{1}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{K)\{}
      \CommentTok{\# z(t) given z(t{-}1)}
\NormalTok{      z[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
      \CommentTok{\# y(t) given z(t)}
\NormalTok{      y[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

We run NIMBLE and get the following results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multistate, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#        mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# pB     0.60 0.03 0.54 0.59  0.66 1.00   202}
\DocumentationTok{\#\# pNB    0.57 0.03 0.51 0.57  0.62 1.01   281}
\DocumentationTok{\#\# phiB   0.80 0.02 0.77 0.80  0.83 1.01   313}
\DocumentationTok{\#\# phiNB  0.85 0.02 0.82 0.85  0.88 1.00   404}
\DocumentationTok{\#\# psiBNB 0.25 0.02 0.21 0.25  0.30 1.00   434}
\DocumentationTok{\#\# psiNBB 0.24 0.02 0.20 0.24  0.29 1.03   478}
\end{Highlighting}
\end{Shaded}

Non-breeder individuals seem to have a survival higher than breeder individuals, suggesting a trade-off between reproduction and survival. Let's compare graphically the survival of breeder and non-breeder individuals. First we gather the values generated for \(\phi^B\) and \(\phi^{NB}\) for the two chains:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phiB }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{"phiB"}\NormalTok{], mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{"phiB"}\NormalTok{])}
\NormalTok{phiNB }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{"phiNB"}\NormalTok{], mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{"phiNB"}\NormalTok{])}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{param =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"phiB"}\NormalTok{, }\FunctionTok{length}\NormalTok{(phiB)), }
                           \FunctionTok{rep}\NormalTok{(}\StringTok{"phiNB"}\NormalTok{, }\FunctionTok{length}\NormalTok{(phiB))), }
                 \AttributeTok{value =} \FunctionTok{c}\NormalTok{(phiB, phiNB))}
\end{Highlighting}
\end{Shaded}

Then, we plot the two posterior distributions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ param)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{position =} \StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#69b3a2"}\NormalTok{, }\StringTok{"\#404080"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{fill =} \StringTok{""}\NormalTok{, }\AttributeTok{x =} \StringTok{"survival"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-354-1.pdf}}

There is little overlap between the two distributions, suggesting an actual trade--off. A formal test of the trade-off would consist in fitting the model with survival irrespective of the state, and compare its WAIC value to the model we just fitted.

What about a potential trade-off on reproduction?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psiBNB }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{"psiBNB"}\NormalTok{], mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{"psiBNB"}\NormalTok{])}
\NormalTok{psiBB }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psiBNB}
\NormalTok{psiNBB }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain1[,}\StringTok{"psiNBB"}\NormalTok{], mcmc.multistate}\SpecialCharTok{$}\NormalTok{chain2[,}\StringTok{"psiNBB"}\NormalTok{])}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{param =} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"psiBB"}\NormalTok{, }\FunctionTok{length}\NormalTok{(phiB)), }
                           \FunctionTok{rep}\NormalTok{(}\StringTok{"psiNBB"}\NormalTok{, }\FunctionTok{length}\NormalTok{(phiB))), }
                 \AttributeTok{value =} \FunctionTok{c}\NormalTok{(psiBB, psiNBB))}
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ value, }\AttributeTok{fill =}\NormalTok{ param)) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{position =} \StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#69b3a2"}\NormalTok{, }\StringTok{"\#404080"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{fill =} \StringTok{""}\NormalTok{, }\AttributeTok{x =} \StringTok{"breeding probabilities"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-355-1.pdf}}

There is no overlap whatsoever, so the two transition probabilities are clearly different. Interestingly, breeder individuals do much better than non-breeder individuals. This failure at detecting a trade-off is probably due to individual heterogeneity that should be accounted for. We will do just that in a case study in Section \ref{casestudytradeoff}.

\section{Issue of local minima}\label{localminima}

In the frequentist approach, we use the maximum likelihood theory to estimate parameters. The maximum likelihood estimates are the values that get you to the maximum of the model likelihood. To find out the maximum of the likelihood, we use iterative optimization algorithms (e.g.~the default method is that of Nelder and Mead in the R \texttt{optim()} function). However, sometimes, our model likelihood contains several maxima and there is no guarantee that the algorithms will find the global maximum corresponding to the maximum likelihood estimates, and it may get stuck in a local maximum. Let's illustrate this issue with some simulated data that were kindly provided by JÃ©rÃ´me Dupuis. We consider 2 sites (or alive states), say 1 and 2, and 7 sampling occasions. The survival probability is constant \(\phi = 1\) as well as the detection probability \(p = 0.6\). The probability of moving from 1 to 2 is \(\psi^{12} = 0.6\) and \(\psi^{21} = 0.85\) in the opposite direction. Here are the encounter histories of the 27 individuals that were simulated by JÃ©rÃ´me:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{,}
                \DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
              \AttributeTok{byrow =}\NormalTok{ T,}
              \AttributeTok{ncol =} \DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In Figure \ref{fig:inits}, we provide an illustration of the influence of the choice of initial values when trying to maximize the likelihood, or rather to minimize the deviance (which is minus two times the log of the likelihood). The black curve is the what we called the profile deviance for \(\psi^{21}\). Profiling the deviance consists in taking a slice of it in the direction of a parameter of interest and treating the other parameters as nuisance parameters. In our example, we set \(\psi^{21}\) to a value (on the x-axis) an minimize the deviance (on the y-axis) with respect to the other parameters. There are two minima, but only the global minimum (corresponding to the lowest value of deviance) corresponding to \(\psi^{21}\) around 0.8 is of interest to us. The thing is that if you start your optimization algorithm by picking value in the red area, then it will get stuck in the local minimum and will tell you the maximum likelihood estimate of \(\psi^{21}\) is around 0.35, which is obviously far from the value we used to simulate the data. In contrast, if you pick initial values in the green area, then the algorithm will converge to the global minimum.

\begin{figure}
\includegraphics[width=1\linewidth]{images/multistate_local_minimav2_Page_07} \caption{Influence of the choice of initial values on the convergence to the global minimum of the deviance illustrated with simulated data. The black curve is the profile deviance of the probability to move from site 2 to 1. If an initial value is picked in the red area, we end up in the local minimum while if it is picked in the green area, then we get the global minimum which corresponds to the maximum lilkelihood estimate.}\label{fig:inits}
\end{figure}

You might argue that this is a problem of the optimization algorithm and therefore inherent to the frequentist approach. Well, it turns out that MCMC algorithms are not immune to the issue. If you fit the AS model with constant parameters to the simulated data, here is the trace for the probability of moving from 2 to 1:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-357-1.pdf}}

Clearly, there are two regimes. The chain spends most of its time around high values of \(\psi^{21}\) close to the true value represented by the blue dashed line. But sometimes, the chain jumps to values around 0.3-0.4. This behavior translates into two modes in the posterior distribution for \(\psi^{21}\) where the mode on the right is closer to the truth represented by the dashed blue vertical line:

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-358-1.pdf}}

The issue of local minima is a difficult problem. How to get out of this problematic situation? In the frequentist approach, the trick is to fit your model several times with different initial values each time, hoping that you'll get to fall in the green area somehow as in Figure \ref{fig:inits}. In the Bayesian approach, the key to handle distributions with multiple modes is to sample the posteriors efficiently. Assuming the chains we run in NIMBLE spend more time in the region of the parameter space corresponding to the global minimum, then we recommend using the median or the mode to summarize the posterior distribution. In the simulated example, we get a median of 0.79 for \(\psi^{21}\), not too bad given that the data were simulated with a value of 0.85 for that parameter:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multisite, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#       mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# p     0.58 0.03 0.51 0.58  0.65 1.01  1866}
\DocumentationTok{\#\# phi   0.99 0.01 0.98 1.00  1.00 1.01   691}
\DocumentationTok{\#\# psi12 0.53 0.14 0.21 0.56  0.72 1.05    69}
\DocumentationTok{\#\# psi21 0.72 0.18 0.28 0.79  0.92 1.04    37}
\end{Highlighting}
\end{Shaded}

As a general advice, we recommend to always inspect the trace plots to find out whether you have posterior distributions with multiple modes which would suggest local minima.

\section{Uncertainty}\label{multievent}

In the AS model, we assume that we can without a doubt assign a site or a state to an animal whenever it is detected. But this is not always the case. For example, when the breeding status in mammals or birds is ascertained based on the presence of offspring or eggs, we are uncertain of whether a female is breeding or not when the offspring or eggs are not seen. Another example is when the epidemiological status in mammals or birds is ascertained based on some tests run on some animals when captured, we are uncertain whether these animals are healthy or sick when detected from distance without possibility to manipulate them for testing. In this section we will cover the extension of the AS model to uncertain states through two examples, one on breeding states and the other on disease states. We will cover other examples in the part on Case studies, in particular in Chapters \ref{tradeoffs} and \ref{lackoffit}.

\subsection{Breeding states}\label{breedingmultievent}

We will revisit the titis example \ref{states} and try to assess life-history trade-offs while accounting for uncertainty in breeding status. We still have 3 states, which are alive and breeding, alive and non-breeding and dead. With regard to observations, a bird may be not encountered. It may also be encountered, but in contrast with our previous analysis of the titis data, we don't know its state for sure. It may be found and ascertained (or classified) as breeder. It may be found and ascertained as non-breeder. It may be found be we are unable to determine whether it's breeding or non-breeding.

How do the states generate the observations?

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-361-1.pdf}}

Each alive state can generate 3 observations. The only deterministic link is that between the dead state and the observation non-encountered, because if a bird is dead, it cannot be detected for sure.

Let's specify the model. First thing we need, and it's a big difference with the AS model, we need initial state probabilities because we cannot assign states to individuals with certainty. We write down the probability for each state at first encounter, or the vector of initial state probabilities:

\[\begin{matrix}
& \\
\mathbf{\delta} =
    \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=B & z_t=NB & z_t=D \\ \hdashline
\pi^B & 1 - \pi^{B} & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
    \begin{matrix}
    \end{matrix}
\end{matrix}\]

where \(\pi^B\) is the probability that a newly encountered individual is a breeder, and \(\pi^{NB} = 1 - \pi^B\) is the probability that a newly encountered individual is a non-breeder (the complementary probability of \(\pi^B\)). The probability of being dead at first encounter is 0 (a bird is alive when it is first encountered).

Now the transition matrix, this is the easy part as it doesn't change. We have:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=B & z_t=NB & z_t=D \\ \hdashline
\phi^B (1-\psi^{BNB}) & \phi^B \psi^{BNB} & 1 - \phi^B\\
\phi^{NB} \psi^{NBB} & \phi^{NB} (1-\psi^{NBB}) & 1 - \phi^{NB}\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=B \\ z_{t-1}=NB \\ z_{t-1}=D
    \end{matrix}
\end{matrix}\]

where \(\phi^B\) is the breeder survival, \(\phi_ {NB}\) that of non-breeders, \(\psi^{BNB}\) is the probability for an individual breeding a year to be a non-breeder the next year, and \(\psi^{NBB}\) is the probability for an non-breeder individual to breeder the next year.

Last, the observation matrix. The main difference between multisite/multistate and multievent models is here, in the observation parameters. Besides \(p^B\) the detection probability of breeders and \(p^{NB}\) that of non-breeders, we introduce two new parameters: \(\beta^B\) is the probability to correctly assign an individual that is in state B to state B, and \(\beta^{NB}\) is the probability to correctly assign an individual that is in state NB to state NB. The complementary of these \(\beta\) parameters are often called false positive probabilities. We put everything in a matrix, as usual. In rows we have the states: breeding, non-breeding and dead. In columns, at the same occasion, we have the observations: non-detected, detected and ascertained B, detected and ascertained NB, and detected but state unknown:

\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 & y_t=3 & y_t=4\\ \hdashline
1 - p^B & p^B \beta^B & 0 & p^B (1-\beta_ B) \\
1-p^{NB} & 0 & p^{NB} \beta^{NB} & p^{NB} (1-\beta^{NB})\\
1 & 0 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
    \begin{matrix}
    z_{t}=B \\ z_{t}=NB \\ z_{t}=D
    \end{matrix}
\end{matrix}\]

For example, the probability of being detected and assigned to state B, given that you're in state B is the product of \(p^B\) the detection probability in B and \(\beta^B\) the probability of correctly assigning a breeding individual to state B.

At first encounter, all individuals are captured, but you still need to assign them a state. This means that we should set \(p^B = p^{NB} = 1\) and use:

\[\begin{matrix}
& \\
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_{t = \text{first}}=1 & y_{t = \text{first}}=2 & y_{t = \text{first}}=3 & y_{t = \text{first}}=4\\ \hdashline
 0 & \beta^B & 0 & (1-\beta^B)\\
0 & 0 & \beta^{NB} & (1-\beta^{NB})\\
1 & 0 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
    \begin{matrix}
    z_{t = \text{first}}=B \\ z_{t = \text{first}}=NB \\ z_{t = \text{first}}=D
    \end{matrix}
\end{matrix}\]

To implement this model in NIMBLE, we start by using comments to define the parameters, states and observations in the header of the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# phiB: survival probability state B}
  \CommentTok{\# phiNB: survival probability state NB}
  \CommentTok{\# psiBNB: transition probability from B to NB}
  \CommentTok{\# psiNBB: transition probability from NB to B}
  \CommentTok{\# pB: recapture probability B}
  \CommentTok{\# pNB: recapture probability NB}
  \CommentTok{\# piB prob. of being in initial state breeder}
  \CommentTok{\# betaNB prob to ascertain the breeding status of an individual encountered as non{-}breeder}
  \CommentTok{\# betaB prob to ascertain the breeding status of an individual encountered as breeder}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (z):}
  \CommentTok{\# 1 alive B}
  \CommentTok{\# 2 alive NB}
  \CommentTok{\# 3 dead}
  \CommentTok{\# Observations (y):}
  \CommentTok{\# 1 = non{-}detected}
  \CommentTok{\# 2 = seen and ascertained as breeder}
  \CommentTok{\# 3 = seen and ascertained as non{-}breeder}
  \CommentTok{\# 4 = not ascertained}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Then we assign prior to all parameters to be estimated. Because we deal with probabilities, the uniform distribution between 0 and 1 will do the job:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# Priors}
\NormalTok{  phiB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  phiNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psiBNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  psiNBB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  pB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  pNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  piB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  betaNB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  betaB }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Now we write the vector of initial state probabilities:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# vector of initial stats probs}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ piB }\CommentTok{\# prob. of being in initial state B}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ piB }\CommentTok{\# prob. of being in initial state NB}
\NormalTok{  delta[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0} \CommentTok{\# prob. of being in initial state dead}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The transition matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psiBNB)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiB }\SpecialCharTok{*}\NormalTok{ psiBNB}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiB}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiNB }\SpecialCharTok{*}\NormalTok{ psiNBB}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiNB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ psiNBB)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiNB}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

And the observation matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of y(t) given z(t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pB             }\CommentTok{\# Pr(alive B t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pB }\SpecialCharTok{*}\NormalTok{ betaB         }\CommentTok{\# Pr(alive B t {-}\textgreater{} detected B t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                  \CommentTok{\# Pr(alive B t {-}\textgreater{} detected NB t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaB)   }\CommentTok{\# Pr(alive B t {-}\textgreater{} detected U t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pNB            }\CommentTok{\# Pr(alive NB t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                  \CommentTok{\# Pr(alive NB t {-}\textgreater{} detected B t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pNB }\SpecialCharTok{*}\NormalTok{ betaNB       }\CommentTok{\# Pr(alive NB t {-}\textgreater{} detected NB t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pNB }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaNB) }\CommentTok{\# Pr(alive NB t {-}\textgreater{} detected U t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}                  \CommentTok{\# Pr(dead t {-}\textgreater{} non{-}detected t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                  \CommentTok{\# Pr(dead t {-}\textgreater{} detected N t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                  \CommentTok{\# Pr(dead t {-}\textgreater{} detected NB t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}                  \CommentTok{\# Pr(dead t {-}\textgreater{} detected U t)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

The observation matrix at first encounter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# probabilities of y(first) given z(first)}
\NormalTok{  omega.init[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive B t = first {-}\textgreater{} non{-}detected t = first)}
\NormalTok{  omega.init[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ betaB      }\CommentTok{\# Pr(alive B t = first {-}\textgreater{} detected B t = first)}
\NormalTok{  omega.init[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive B t = first {-}\textgreater{} detected NB t = first)}
\NormalTok{  omega.init[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaB  }\CommentTok{\# Pr(alive B t = first {-}\textgreater{} detected U t = first)}
\NormalTok{  omega.init[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive NB t = first {-}\textgreater{} non{-}detected t = first)}
\NormalTok{  omega.init[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(alive NB t = first {-}\textgreater{} detected B t = first)}
\NormalTok{  omega.init[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ betaNB     }\CommentTok{\# Pr(alive NB t = first {-}\textgreater{} detected NB t = first)}
\NormalTok{  omega.init[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaNB }\CommentTok{\# Pr(alive NB t = first {-}\textgreater{} detected U t = first)}
\NormalTok{  omega.init[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}          \CommentTok{\# Pr(dead t = first {-}\textgreater{} non{-}detected t = first)}
\NormalTok{  omega.init[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = first {-}\textgreater{} detected N t = first)}
\NormalTok{  omega.init[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = first {-}\textgreater{} detected NB t = first)}
\NormalTok{  omega.init[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# Pr(dead t = first {-}\textgreater{} detected U t = first)}
\NormalTok{...}
\end{Highlighting}
\end{Shaded}

Eventually, we get to the likelihood:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multievent }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
\NormalTok{...}
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \CommentTok{\# latent state at first capture}
\NormalTok{    z[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\NormalTok{    y[i,first[i]] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega.init[z[i,first[i]],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]) }\CommentTok{\# obs at first encounter}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ (first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{K)\{}
      \CommentTok{\# z(t) given z(t{-}1)}
\NormalTok{      z[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(gamma[z[i,t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
      \CommentTok{\# y(t) given z(t)}
\NormalTok{      y[i,t] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dcat}\NormalTok{(omega[z[i,t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

The only change is in the line \texttt{y{[}i,first{[}i{]}{]}\ \textasciitilde{}\ dcat(omega.init{[}z{[}i,first{[}i{]}{]},1:4{]})} where we use the observation matrix at first encounter.

We run NIMBLE and get the following numerical summaries for the model parameters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{MCMCsummary}\NormalTok{(mcmc.multievent, }\AttributeTok{round =} \DecValTok{2}\NormalTok{)}
\DocumentationTok{\#\#        mean   sd 2.5\%  50\% 97.5\% Rhat n.eff}
\DocumentationTok{\#\# betaB  0.19 0.01 0.16 0.19  0.21 1.01   332}
\DocumentationTok{\#\# betaNB 0.76 0.05 0.66 0.76  0.86 1.01    65}
\DocumentationTok{\#\# pB     0.56 0.03 0.51 0.56  0.62 1.06   229}
\DocumentationTok{\#\# pNB    0.60 0.04 0.53 0.60  0.67 1.03   142}
\DocumentationTok{\#\# phiB   0.81 0.02 0.78 0.81  0.85 1.01   312}
\DocumentationTok{\#\# phiNB  0.84 0.02 0.80 0.84  0.87 1.00   354}
\DocumentationTok{\#\# piB    0.71 0.03 0.66 0.71  0.76 1.02   115}
\DocumentationTok{\#\# psiBNB 0.23 0.02 0.18 0.22  0.27 1.00   214}
\DocumentationTok{\#\# psiNBB 0.25 0.04 0.17 0.25  0.34 1.00    95}
\end{Highlighting}
\end{Shaded}

Breeders are difficult to assign to the correct state \(\beta^B\), while non-breeders are relatively well classified as non-breeders \(\beta^{NB}\).

There is again no cost of current reproduction on future reproduction. We no longer detect a cost of breeding on survival.

\subsection{Disease states}\label{diseasemultievent}

Let's have a look to another example. We consider a system of an emerging pathogen \emph{Mycoplasma gallisepticum} Edward and Kanarek and its host the house finch, \emph{Carpodacus mexicanus} MÃ¼ller. The pathogen causes moderate to severe eye swelling (see Figure \ref{fig:pixfinch}).

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/infectedhousefinch} 

}

\caption{A house finch with a heavy infection caused by conjunctivitis. Credit: Jim Mondok.}\label{fig:pixfinch}
\end{figure}

Here we're asking whether the presence of clinical signs of the pathogen influences survival. The objective of the study was also to quantify infection (moving from state healthy to state ill) and recovery (moving from state ill to state healthy) probabilities. The birds were captured via mist nets and marked with individually identifiable color bands over three years. The data were kindly provided by Paul Conn and Evan Cooch. The difficulty was that ascertaining the disease status of birds seen from distance was difficult since determining the presence of the pathogen was only possible when the bird's eyes were clearly visible. In this context, how to study the dynamics of the disease?

First, we think of states and observations. We have:

\begin{itemize}
\tightlist
\item
  3 states

  \begin{itemize}
  \tightlist
  \item
    healthy (H)
  \item
    ill (I)
  \item
    dead (D)
  \end{itemize}
\item
  4 observations

  \begin{itemize}
  \tightlist
  \item
    not seen (1)
  \item
    captured healthy (2)
  \item
    captured ill (3)
  \item
    health status unknown, i.e.~seen at distance (4)
  \end{itemize}
\end{itemize}

How do the states generate observations?

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-371-1.pdf}}

Clearly, this is the same model as in the previous section on titis, Section \ref{breedingmultievent}, in which loosely speaking we replace breeder by healthy and non-breeder by ill.

The vector of initial state probabilities is:

\[\begin{matrix}
& \\
\mathbf{\delta} =
    \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=H & z_t=I & z_t=D \\ \hdashline
\pi^H & 1 - \pi^{H} & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
    \begin{matrix}
    \end{matrix}
\end{matrix}\]

where \(\pi^H\) is the probability that a newly encountered individual is healthy, and \(\pi^{I} = 1 - \pi^H\) is the probability that a newly encountered individual is ill.

The transition matrix is:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=H & z_t=I & z_t=D \\ \hdashline
\phi^H (1-\psi^{HI}) & \phi^H \psi^{HI} & 1 - \phi^H\\
\phi^{I} \psi^{IH} & \phi^{I} (1-\psi^{IH}) & 1 - \phi^{I}\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=H \\ z_{t-1}=I \\ z_{t-1}=D
    \end{matrix}
\end{matrix}\]

where \(\phi^H\) is the survival probability of healthy individuals, \(\phi^I\) the survival probability of ill individuals, \(\psi^{HI}\) the probability of getting ill (infection rate) and \(\psi^{IH}\) the probability of recovering from the disease (recovery rate).

Image you'd like to model the dynamic of an incurable disease, the transition matrix would be modified by having \(\psi^{IH} = 0\), and once a bird gets ill, it remains ill \(\psi^{II} = 1 - \psi^{IH} = 1\). Therefore we would have:

\[\begin{matrix}
& \\
\mathbf{\Gamma} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    z_t=H & z_t=I & z_t=D \\ \hdashline
\phi^H (1-\psi^{HI}) & \phi^H \psi^{HI} & 1 - \phi^H\\
0 & \phi^{I}  & 1 - \phi^{I}\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
    \begin{matrix}
    z_{t-1}=H \\ z_{t-1}=I \\ z_{t-1}=D
    \end{matrix}
\end{matrix}\]

For analysing the house finch data, we allow recovering from the disease. The observation matrix is:

\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=0 & y_t=1 & y_t=2 & y_t=3\\ \hdashline
1-p^H & p^H \beta^H & 0 & p^H (1-\beta^H)\\
1-p^I & 0 & p^{I} \beta^{I} & p^{I} (1-\beta^{I})\\
1 & 0 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
    \begin{matrix}
    z_{t}=H \\ z_{t}=I \\ z_{t}=D
    \end{matrix}
\end{matrix}\]

where \(\beta^H\) is the probability to assign a healthy individual to state H, and \(\beta^{I}\) is the probability to assign a sick individual to state I. \(p^H\) is the detection probability of healthy individuals, \(p^I\) that of sick individuals.

Using the code we developped for the titis example, we get the following results on the finches by running NIMBLE:

\begin{verbatim}
##       mean   sd 2.5%  50% 97.5% Rhat n.eff
## betaH 0.99 0.01 0.97 0.99  1.00 1.01  1421
## betaI 0.05 0.01 0.03 0.05  0.08 1.00  6477
## pH    0.17 0.02 0.13 0.17  0.22 1.01   331
## pI    0.58 0.10 0.41 0.57  0.80 1.04   220
## phiH  0.88 0.02 0.84 0.88  0.92 1.01   360
## phiI  0.99 0.01 0.96 0.99  1.00 1.00  1004
## pi    0.96 0.01 0.93 0.96  0.98 1.00  4190
## psiHI 0.22 0.04 0.16 0.22  0.32 1.02   311
## psiIH 0.46 0.08 0.32 0.45  0.63 1.02   392
\end{verbatim}

Healthy individuals are correctly assigned (\(\beta^H\) is almost 1), while infected individuals are difficult to ascertain (\(\beta^I\) is around 0.05). Unexpectedly, ill birds have a better survival than healthy individuals (compare \(\phi^I\) and \(\phi^H\)). Infection rate (\(\psi^{HI}\)) is 22\%, recovery rate is 46\% (\(\psi^{IH}\)).

\section{Summary}\label{summary-4}

\begin{itemize}
\item
  The AS model is a HMM that extends the CJS model by allowing the estimation of movements between sites (e.g.~geographical locations) or transitions between states (e.g.~breeding status). This flexibility allows addressing all sorts of questions in ecology and evolution.
\item
  Covariates can be considered, and appropriate priors or link functions need to be used when there are more than 2 sites or 2 alive states.
\item
  Model comparison can be achieved with the WAIC and the goodness of fit of the AS model to capture-recapture data can be assessed with classical procedures or posterior predictive checks.
\item
  Importantly, the HMM framework allows to account for uncertainty when assigning states to individuals.
\item
  The models covered in this chapter haven been called multistratum/strata models, multisite models (section \ref{ASmodel}), multistate models (section \ref{states}) and multievent models (section \ref{multievent}). These models are all HMMs.
\end{itemize}

\section{Suggested reading}\label{suggested-reading-4}

\begin{itemize}
\item
  The AS model was introduced in \citet{arnason1972}, \citet{arnason1973} and \citet{SchwarzEtAl1993}. Very soon clever folks realized that sites could be replaced by states as in \citet{NicholsEtAl1992} and \citet{NicholsEtAl1994}. For a review of models with sites and states, see \citet{LebretonEtAl2009}.
\item
  The geese data were analyzed in \citet{hestbeck1991estimates} and \citet{BrownieEtAl1993}, and the titis data in \citet{scofield2001titi}. The house finches data were analyzed in \citet{FaustinoEtAl2004} and \citet{ConnCooch2009} (see also \citet{cooch2012disease}). Check out \citet{santoro2014host}, \citet{MarescotEtAl2018} and \citet{ollivier2023lyme} for other examples in disease ecology.
\item
  Section \ref{localminima} on local minima was inspired by chapter 10 of \citet{cooch2017intromark}.
\item
  Classical goodness of fit tests are reviewed in \citet{pradel2005gof}. See also \citet{pradel2003gof} for tests specifically designed for multisite/multistate models.
\item
  Models with uncertainty were introduced in \citet{pradel_multievent_2005}. \citet{dupuis_bayesian_1995} had a similar idea for the AS model. For a review, see \citet{gimenez_estimating_2012}.
\end{itemize}

\part{Case studies}\label{part-case-studies}

\chapter*{Introduction}\label{introduction-7}


This third part \texttt{Case\ studies} provides real-world case studies from the scientific literature that you can reproduce using material covered in previous chapters. These problems can either i) be used to cement and deepen your understanding of methods and models, ii) be adapted for your own purpose, or iii) serve as teaching projects. For each case study, I recall the ecological question, then build the model step by step and conclude by discussing the results. \textbf{Insist on the fact that I do not always the entire code, but relevant bits. The code and data are available say where}.

\chapter{Dealing with covariates}\label{covariateschapter}

\section{Introduction}\label{introduction-8}

Capture--recapture models often aim not just to estimate demographic parameters, but also to understand what drives their variation as we have seen in Chapter \ref{survival}. In this chapter, we explore how to incorporate covariates -- whether measured at the individual or temporal scale. We cover selecting covariates from a potentially large set, using an extension of standard MCMC algorithms known as reversible jump MCMC (RJMCMC), and handling uncertainty in key covariates, namely age or sex, when these are not perfectly known for all individuals.

\section{Covariate selection with Reversible Jump MCMC}\label{covariate-selection-with-reversible-jump-mcmc}

\subsection{Motivation}\label{motivation}

In Section \ref{waic}, we used WAIC to figure our which model, among several candidates, was best supported by the data. Each model there represented a different ecological hypothesis. Now when you are in a situation where you have several individual or temporal covariates that might explain variation in some demographic parameters, say survival as in Section \ref{covariates}, things can quickly become a bit more tedious.

Let's look at a real-world example: The white stork (\emph{Ciconia ciconia}) population in Baden-WÃ¼rttemberg, Germany. From the 1960s to the 1990s, all Western European stork populations were in decline. One leading hypothesis was that these declines were driven by reduced food availability, itself caused by severe droughts in the storks' wintering grounds in the Sahel region (e.g. \citet{kanya90}, \citet{grosbois_assessing_2008}).

To explore this idea, we'll use rainfall measurements from 10 meteorological stations chosen to represent the storks' wintering area: Diourbel, Gao, Kayes, Kita, Maradi, Mopti, Ouahigouya, SÃ©gou, Tahoua, and Tombouctou. The data we have are the cumulative rainfall amounts (in mm) over June, July, August, and September -- roughly the rainy season in the Sahel.

\begin{verbatim}
##      Diourbel  Gao Kayes  Kita Maradi Mopti Ouahigouya
## [1,]     6770 1900  7130 11350   6970  6580       6220
## [2,]     6420 2690  6180 13690   5880  6470       6260
## [3,]     6890 3530  6480 11820   6100  5230       6730
## [4,]     5700 2710  8440  9590   5210  5670       5670
## [5,]     6760 2090  6910  9850   6020  4200       5630
## [6,]     5432 2100  6300  8960   6890  3900       7180
##      SÃ©gou Tahoua Tombouctou
## [1,]  8380   3920       1390
## [2,]  7530   3200       2210
## [3,]  6340   5170       1760
## [4,]  7490   4370       2580
## [5,]  5920   3120       2340
## [6,]  6461   3891       2020
\end{verbatim}

Our question is: Which combination of these stations best explains variation in survival? If we tried to answer this by testing every possible model, we would have to consider every subset of the 10 stations and compare 1024 models (2\^{}10 combinations). That's far too many for a WAIC-based approach to be practical.

Instead of comparing all possible models one by one, we can use Reversible Jump MCMC (RJMCMC). RJMCMC is an extension of standard Bayesian inference that treats the model itself as an extra (discrete) parameter. In this framework, the posterior distribution spans both the parameter space (e.g.~regression coefficients) and the model space (e.g.~which covariates are included).

To work out the (marginal) posterior probability of each model, we integrate over the parameters using MCMC. Standard MCMC algorithms cannot handle this model-switching scenario, but RJMCMC is designed precisely for it and can explore parameters and models together within a single Markov chain. In other words, RJMCMC can `jump' between models, turning covariates on or off. RJMCMC is like a hiker exploring different trails (models), stopping along each one to take measurements (parameters) before deciding whether to switch to another. From this, we can estimate the posterior probability that each covariate influences survival, as well as model-averaged regression coefficients and survival estimates that account for both parameter and model uncertainty.

We won't dive into the mathematical details here but don't worry, you can still follow the analysis without them. Check out Chapter 7 of \citet{king_bayesian_2009} (see also \citet{gimenez2009winbugs}) if you're interested in.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation}

For our example, we'll work with data from 321 encounter histories of white storks ringed as chicks between 1956 and 1970. We'll fit a Cormack--Jolly--Seber (CJS) model with time-dependent survival and constant detection probability, as described in Section \ref{cjsderivatives}. We model the survival probability \texttt{phi} as a linear function of our covariates stored in \texttt{x}. To handle model selection inside the RJMCMC, we use a clever trick: for each covariate, we introduce an indicator variable \texttt{ksi} that can take the value 1 (covariate is included in the model) or 0 (covariate is excluded). We then combine the regression coefficients \texttt{beta} with these indicators into a single vector \texttt{betaksi}. This means that if \texttt{ksi{[}j{]}\ =\ 0}, the effect of covariate \texttt{j} is zero, effectively removing it from the model. Here's how it works in the NIMBLE code:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{))\{}
  \FunctionTok{logit}\NormalTok{(phi[t]) }\OtherTok{\textless{}{-}}\NormalTok{ intercept }\SpecialCharTok{+} \FunctionTok{inprod}\NormalTok{(betaksi[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], x[t, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[t]      }\CommentTok{\# Pr(alive t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[t]  }\CommentTok{\# Pr(alive t {-}\textgreater{} dead t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{0}           \CommentTok{\# Pr(dead t {-}\textgreater{} alive t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,t] }\OtherTok{\textless{}{-}} \DecValTok{1}           \CommentTok{\# Pr(dead t {-}\textgreater{} dead t+1)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For the indicators and regression coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{  betaksi[j] }\OtherTok{\textless{}{-}}\NormalTok{ beta[j] }\SpecialCharTok{*}\NormalTok{ ksi[j]}
\NormalTok{  ksi[j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbern}\NormalTok{(psi) }\DocumentationTok{\#\# indicator variable associated with betaj}
\NormalTok{\}}
\NormalTok{psi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dbeta}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\DocumentationTok{\#\# hyperprior on inclusion probability}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{  beta[j] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior slopes}
\NormalTok{\}}
\NormalTok{intercept }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.5}\NormalTok{) }\CommentTok{\# prior intercept}
\end{Highlighting}
\end{Shaded}

The prior on \texttt{psi} controls the overall tendency to include covariates in the model. Setting it to \texttt{dbeta(1,\ 1)} makes all inclusion probabilities equally likely, so the data drive the selection. The priors on \texttt{beta} and the \texttt{intercept} (not consider in the selection) are weakly informative.

As we saw in Section \ref{change-sampler}, NIMBLE allows you to change the default sampler. Here, we're going to do something slightly different and use NIMBLE's \texttt{configureRJ()} function to set up RJMCMC for variable selection. The function \texttt{configureRJ()} modifies an existing MCMC configuration so that RJMCMC is used to sample certain parameters, allowing covariates to be turned on or off during the run. It uses a univariate normal proposal distribution, and you can adjust the proposal mean and scale. The main arguments are:\\
- \texttt{targetNodes}: The parameters you want to apply variable selection to (e.g.~\texttt{beta});\\
- \texttt{indicatorNodes}: The indicator variables paired with the target nodes (e.g.~\texttt{ksi});\\
- \texttt{control}: A list for fine-tuning the proposal with \texttt{mean} the mean of the proposal distribution (default is 0), \texttt{scale} the standard deviation of the proposal (default is 1) and \texttt{fixedValue} the value a variable takes when excluded (default is 0).

Here's how we apply it to our white stork example:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# Build the model and configure default MCMC}
\NormalTok{RJsurvival }\OtherTok{\textless{}{-}} \FunctionTok{nimbleModel}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phirjmcmcp, }
                          \AttributeTok{constants =}\NormalTok{ my.constants,}
                          \AttributeTok{data =}\NormalTok{ my.data,}
                          \AttributeTok{inits =} \FunctionTok{initial.values}\NormalTok{())}

\NormalTok{Csurvival }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(RJsurvival)}

\NormalTok{RJsurvivalConf }\OtherTok{\textless{}{-}} \FunctionTok{configureMCMC}\NormalTok{(RJsurvival)}
\NormalTok{RJsurvivalConf}\SpecialCharTok{$}\FunctionTok{addMonitors}\NormalTok{(}\StringTok{\textquotesingle{}ksi\textquotesingle{}}\NormalTok{)}

\CommentTok{\# Switch to reversible jump sampling for the betas}
\FunctionTok{configureRJ}\NormalTok{(}\AttributeTok{conf =}\NormalTok{ RJsurvivalConf,}
            \AttributeTok{targetNodes =} \StringTok{\textquotesingle{}beta\textquotesingle{}}\NormalTok{,}
            \AttributeTok{indicatorNodes =} \StringTok{\textquotesingle{}ksi\textquotesingle{}}\NormalTok{,}
            \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{scale =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We then build, compile, and run the RJMCMC:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{survivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{buildMCMC}\NormalTok{(RJsurvivalConf)}
\NormalTok{CsurvivalMCMC }\OtherTok{\textless{}{-}} \FunctionTok{compileNimble}\NormalTok{(survivalMCMC, }\AttributeTok{project =}\NormalTok{ RJsurvival)}
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{runMCMC}\NormalTok{(}\AttributeTok{mcmc =}\NormalTok{ CsurvivalMCMC, }
                   \AttributeTok{niter =}\NormalTok{ n.iter,}
                   \AttributeTok{nburnin =}\NormalTok{ n.burnin,}
                   \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

When we run this MCMC, \texttt{configureRJ()} takes care of proposing moves that either keep a covariate in the model or drop it out by setting its \texttt{ksi} value to 0. If a covariate is included (\texttt{ksi\ =\ 1}), the sampler updates its regression coefficient \texttt{beta} using the specified normal proposal distribution. If it's excluded (\texttt{ksi\ =\ 0}), the coefficient is fixed at zero (our \texttt{fixedValue}). Over many iterations, the chain `jumps' between different combinations of covariates, building up posterior probabilities for each one's inclusion. This means we can later summarise which covariates are most likely to affect survival (via their inclusion probabilities), and model-averaged estimates of survival that incorporate the uncertainty about which covariates belong in the model. This is what we do in \texttt{R} in the next section.

\subsection{Results}\label{results}

The RJMCMC run gives us the posterior probabilities of different models where each model corresponds to a specific combination of covariates. In our white stork example, the results clearly show that the model including only the rainfall at the Kita station (covariate \#4) is best supported by the data.

To get these results, we first combine the MCMC samples from our two chains:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Gather two chains}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(samples[[}\DecValTok{1}\NormalTok{]], samples[[}\DecValTok{2}\NormalTok{]])}
\end{Highlighting}
\end{Shaded}

Next, we look at the inclusion indicators \texttt{ksi}, which take value 1 if a covariate is in the model and 0 if not. The posterior mean of each \texttt{ksi{[}j{]}} is simply the estimated probability that covariate j is included:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Number of covariates}
\NormalTok{K }\OtherTok{\textless{}{-}} \DecValTok{10}

\CommentTok{\# Extract only ksi columns, in order ksi[1], ..., ksi[10]}
\NormalTok{ksi }\OtherTok{\textless{}{-}}\NormalTok{ out[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"ksi["}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{K, }\StringTok{"]"}\NormalTok{)]}

\CommentTok{\# Posterior inclusion probabilities for each covariate}
\NormalTok{inclusion\_prob }\OtherTok{\textless{}{-}} \FunctionTok{colMeans}\NormalTok{(ksi)}
\NormalTok{inclusion\_prob}
\DocumentationTok{\#\#  ksi[1]  ksi[2]  ksi[3]  ksi[4]  ksi[5]  ksi[6]  ksi[7] }
\DocumentationTok{\#\# 0.02575 0.01720 0.01560 0.96120 0.04430 0.05350 0.02025 }
\DocumentationTok{\#\#  ksi[8]  ksi[9] ksi[10] }
\DocumentationTok{\#\# 0.03360 0.02630 0.02965}
\end{Highlighting}
\end{Shaded}

The posterior inclusion probabilities tell us how likely each covariate is to appear in the `true' model given the data and priors. Here, \texttt{ksi{[}4{]}} -- the Kita station -- stands out, with an inclusion probability around 0.96.

We can also identify the most probable models visited by the RJMCMC. Each MCMC draw corresponds to a model, which we encode as a binary string (e.g., ``0101001001''):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Encode each model draw as a string of 0s and 1s}
\NormalTok{model\_id }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(ksi, }\DecValTok{1}\NormalTok{, paste0, }\AttributeTok{collapse =} \StringTok{""}\NormalTok{)}

\CommentTok{\# Empirical posterior model probabilities}
\NormalTok{post\_model\_prob }\OtherTok{\textless{}{-}} \FunctionTok{sort}\NormalTok{(}\FunctionTok{prop.table}\NormalTok{(}\FunctionTok{table}\NormalTok{(model\_id)), }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Show the top models}
\FunctionTok{head}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{model =} \FunctionTok{names}\NormalTok{(post\_model\_prob), }
                \AttributeTok{prob =} \FunctionTok{as.numeric}\NormalTok{(post\_model\_prob)), }\DecValTok{10}\NormalTok{)}
\DocumentationTok{\#\#         model    prob}
\DocumentationTok{\#\# 1  0001000000 0.75470}
\DocumentationTok{\#\# 2  0001010000 0.03965}
\DocumentationTok{\#\# 3  0001100000 0.02910}
\DocumentationTok{\#\# 4  0000000000 0.02310}
\DocumentationTok{\#\# 5  0001000001 0.02055}
\DocumentationTok{\#\# 6  0001000100 0.01710}
\DocumentationTok{\#\# 7  0001000010 0.01660}
\DocumentationTok{\#\# 8  1001000000 0.01575}
\DocumentationTok{\#\# 9  0001001000 0.01320}
\DocumentationTok{\#\# 10 0101000000 0.01045}
\end{Highlighting}
\end{Shaded}

The posterior model probabilities rank the different combinations of covariates, and in our case, the model containing only the rainfall at Kita is clearly dominant.

In conclusion, a very high inclusion probability for Kita (covariate \#4) and a dominant model containing only Kita indicate that this station's rainfall is strongly linked to survival, while others are not.

Now turning to inference, we'd like to get model-averaged parameters. Each covariate's effect is `on' only when its indicator (\texttt{ksi}) is 1. Multiplying \texttt{beta} by \texttt{ksi} zeros out the effect when that covariate is excluded. Summarising \texttt{betaksi} across draws gives the model-averaged effect on the logit scale:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Pull posterior draws}
\NormalTok{beta  }\OtherTok{\textless{}{-}}\NormalTok{ out[, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"beta["}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{K, }\StringTok{"]"}\NormalTok{)]}

\CommentTok{\# Model{-}averaged coefficients on the logit scale}
\NormalTok{betaksi }\OtherTok{\textless{}{-}}\NormalTok{ beta }\SpecialCharTok{*}\NormalTok{ ksi}

\CommentTok{\# Summaries: mean, 95\% credible intervals, and Pr(effect \textgreater{} 0)}
\NormalTok{df\_betaksi }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{mean  =} \FunctionTok{apply}\NormalTok{(betaksi, }\DecValTok{2}\NormalTok{, mean),}
  \AttributeTok{low  =} \FunctionTok{apply}\NormalTok{(betaksi, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{),}
  \AttributeTok{high  =} \FunctionTok{apply}\NormalTok{(betaksi, }\DecValTok{2}\NormalTok{, quantile, }\AttributeTok{probs =} \FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{),}
  \AttributeTok{P\_gt0 =} \FunctionTok{apply}\NormalTok{(betaksi }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, mean))}

\FunctionTok{round}\NormalTok{(df\_betaksi, }\DecValTok{3}\NormalTok{)}
\DocumentationTok{\#\#            mean    low  high P\_gt0}
\DocumentationTok{\#\# beta[1]   0.003  0.000 0.000 0.023}
\DocumentationTok{\#\# beta[2]   0.000  0.000 0.000 0.009}
\DocumentationTok{\#\# beta[3]   0.000  0.000 0.000 0.010}
\DocumentationTok{\#\# beta[4]   0.348  0.000 0.545 0.961}
\DocumentationTok{\#\# beta[5]  {-}0.006 {-}0.123 0.000 0.003}
\DocumentationTok{\#\# beta[6]   0.007  0.000 0.144 0.051}
\DocumentationTok{\#\# beta[7]  {-}0.002  0.000 0.000 0.004}
\DocumentationTok{\#\# beta[8]   0.005  0.000 0.048 0.028}
\DocumentationTok{\#\# beta[9]   0.002  0.000 0.000 0.021}
\DocumentationTok{\#\# beta[10] {-}0.003 {-}0.020 0.000 0.003}
\end{Highlighting}
\end{Shaded}

A large \texttt{P\_gt0} (e.g., \textgreater{} 0.95) suggests strong evidence that the covariate increases survival (on the logit scale), while values near 0.5 mean the direction is uncertain.

Now let's get model-averaged survival estimates over time. For each MCMC draw, we build the linear predictor with all covariates using \texttt{beta\ *\ ksi}, add the intercept, then inverse-logit to get survival \texttt{phi}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{invlogit }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{x))}

\CommentTok{\# Table of covariate values}
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ rainfall[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{(}\DecValTok{16} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), }\DecValTok{1}\SpecialCharTok{:}\NormalTok{K]}

\CommentTok{\# Intercept MCMC draws}
\NormalTok{intercept }\OtherTok{\textless{}{-}}\NormalTok{ out[, }\StringTok{"intercept"}\NormalTok{]}

\CommentTok{\# Linear predictor for every time t (rows) and every MCMC draw (columns)}
\NormalTok{eta }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(betaksi) }
\NormalTok{eta }\OtherTok{\textless{}{-}} \FunctionTok{sweep}\NormalTok{(eta, }\DecValTok{2}\NormalTok{, intercept, }\StringTok{"+"}\NormalTok{)  }\CommentTok{\# add intercept per draw}

\CommentTok{\# Back{-}transform to survival probabilities}
\NormalTok{phi\_draws }\OtherTok{\textless{}{-}} \FunctionTok{invlogit}\NormalTok{(eta)}

\CommentTok{\# Summaries by time}
\NormalTok{df\_phi }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{mean  =} \FunctionTok{apply}\NormalTok{(phi\_draws, }\DecValTok{1}\NormalTok{, mean),}
  \AttributeTok{low  =} \FunctionTok{apply}\NormalTok{(phi\_draws, }\DecValTok{1}\NormalTok{, quantile, }\AttributeTok{probs =} \FloatTok{2.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{),}
  \AttributeTok{high  =} \FunctionTok{apply}\NormalTok{(phi\_draws, }\DecValTok{1}\NormalTok{, quantile, }\AttributeTok{probs =} \FloatTok{97.5}\SpecialCharTok{/}\DecValTok{100}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(df\_phi, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{1956}\SpecialCharTok{:}\DecValTok{1970}\NormalTok{, }\AttributeTok{y =}\NormalTok{ mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ low, }\AttributeTok{ymax =}\NormalTok{ high), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{size =} \FloatTok{0.9}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{x =} \StringTok{"Years (interval start)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Survival probability"}\NormalTok{,}
    \AttributeTok{title =} \StringTok{"Model{-}averaged survival with 95\% credible intervals"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-385-1.pdf}}

Shaded ribbons show 95\% credible intervals; the solid line is the posterior mean of survival, model-averaged over covariate inclusion.

\section{Uncertainty in age}\label{ageuncertainty}

\subsection{Motivation}\label{motivation-1}

Survival and other demographic parameters often differ between young and older animals, so if we blur age classes we can end up telling the wrong story about population dynamics. In Section \ref{agecov}, we incorporated age as a covariate in capture-recapture models, assuming it was perfectly known. With non-invasive genetics (Figure \ref{fig:marking}), though, age is not observed: a DNA profile can identify who was there, but not how old they were. If we ignore that uncertainty, survival estimates can be biased. As we saw in Section \ref{multievent}, HMM capture--recapture models can handle this kind of problem by treating age as a hidden state and separating it from what we actually observe in the field.

A neat example comes from Apennine brown bears in \citet{Gervasi2017} (see also \citet{gowan2021uncertainty}). Because you cannot age a bear from its DNA, the authors paired genetics with field information to classify detections into two broad age classes, cubs (\textless1 year) and adults (â‰¥1 year). To do so, they used two criteria. First, classify a newly detected bear as a cub in that year if it was sampled at least once on the same date and site as a known adult female and they shared â‰¥1 allele at every locus (mother--offspring consistent). That's the lenient criterion (P1). Second, apply a stricter criterion (P2): make the same mother--offspring call, but require two or more such co-sampling occasions with the same female, again on the same date and site each time.

Why two criteria? Because there's a trade-off. P1 is better at catching real cubs (more sensitive) but risks misclassifying some adults as cubs; P2 is more conservative about calling `cub', so it reduces false cubs but misses some true ones.

Instead of pretending these criteria are perfect, the authors model the misclassification explicitly: age is hidden, `cub/adult by P1/P2' is what we observe, and their model estimates both classification accuracy and age-specific survival at the same time. That way, uncertainty is propagated into the demographic estimates rather than swept under the rug.

Here, I show how to encode that logic in an HMM with NIMBLE: define the hidden age states, write the transition (survival) and detection components, and add a classification step that links hidden age to the observed P1/P2 outcomes. We'll also see how a small subset of known-age individuals (e.g., live-trapped or aged beforehand) anchors the classification step. The payoff is a set of age-specific survival estimates that are robust to imperfect age assignment, which is exactly what we need for sound ecological inference and management.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation-1}

To estimate bear survival, we have at our disposal a 12-year time series of non-invasive genetic sampling data, collected between 2003 and 2014. To build up our model, we first define the states and observations:

\begin{itemize}
\tightlist
\item
  states

  \begin{itemize}
  \tightlist
  \item
    alive as cub (C)
  \item
    alive as adult (A)
  \item
    dead (D)
  \end{itemize}
\item
  observations

  \begin{itemize}
  \tightlist
  \item
    not detected (1)
  \item
    detected and classified as cub by both criteria (2)
  \item
    detected and classified as cub by P1 only (3)
  \item
    detected and classified as adult by both criteria (4)
  \end{itemize}
\end{itemize}

Now we turn to writing our model.

We start with the vector of initial states. Unknown-age individuals enter as a mixture of cub/adult with probabilities \(\pi\) and \(1-\pi\):

\[\begin{matrix}
& \\
\mathbf{\delta} =
  \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=C & z_t=A & z_t=D \\ \hdashline
          \pi & 1 - \pi & 0\\
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
\begin{matrix}
\end{matrix}
\end{matrix}\]

where \(\pi\) is the probability of being alive as a cub, and \(1 - \pi\) is the probability of being alive as an adult. For bears known-age at first detection, we fix the initial state accordingly. In NIMBLE we implement this with an individual-specific \(\delta_i\) using \texttt{equals()} to pin known ages:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Initial state}
\CommentTok{\# age[i] == 0 â†’ unknown â†’ use probabilistic delta: pi, 1 {-} pi}
\CommentTok{\# age[i] == 1 â†’ known cub â†’ force delta to [1, 0, 0]}
\CommentTok{\# age[i] == 2 â†’ known adult â†’ force delta to [0, 1, 0]}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{, i] }\OtherTok{\textless{}{-}} \FunctionTok{equals}\NormalTok{(age[i], }\DecValTok{0}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ pi }\SpecialCharTok{+} \FunctionTok{equals}\NormalTok{(age[i], }\DecValTok{1}\NormalTok{)  }\CommentTok{\# Pr(C)}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{, i] }\OtherTok{\textless{}{-}} \FunctionTok{equals}\NormalTok{(age[i], }\DecValTok{0}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi) }\SpecialCharTok{+} \FunctionTok{equals}\NormalTok{(age[i], }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Pr(A)}
\NormalTok{  delta[}\DecValTok{3}\NormalTok{, i] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We proceed with the transition matrix that contains the survival probabilities. A cub either survives and becomes adult next year (\(\phi_C\)), or dies; adults either survive in the same class (\(\phi_A\)) or die:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=C & z_t=A & z_t=D \\ \hdashline
          0  & \phi_C & 1 - \phi_C\\
          0 & \phi_A & 1 - \phi_A\\
          0 & 0 & 1
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=C \\ z_{t-1}=A \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

Following the paper, adult survival is sex-specific; cub survival is common to both sexes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transition matrix}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phiC}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiC}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}}\NormalTok{ phiA[sex[i]]   }\CommentTok{\# sex{-}specific adult survival (1=f, 2=m)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiA[sex[i]]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,i] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Last step is about the observations, which arise in two steps: detection then classification. For the detection matrix, we need to introduce intermediate observations, say \(y'\) for not detected (1), detected as cub (2) and detected as adult (2):
\[\begin{matrix}
& \\
\mathbf{\Gamma_1} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          y'_t=1 & y'_t=2 & y'_t=3 \\ \hdashline
          1-p_C  & p_C & 0\\
          1-p_A & 0 & 1 - p_A\\
          1 & 0 & 0
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t}=C \\ z_{t}=A \\ z_{t}=D
\end{matrix}
\end{matrix}\]

where \(p_C\) is detection for cubs, and \(p_A\) that for adults. Then the classification matrix is:
\[\begin{matrix}
& \\
\mathbf{\Omega_2} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 & y_t=3 & y_t=4\\ \hdashline
1 & 0 & 0 & 0\\
0 & \beta_{C,CC} & \beta_{C,CA} & 1 - \beta_{C,CC} - \beta_{C,CA}\\
0 & \beta_{A,CC} & \beta_{A,CA} & 1 - \beta_{A,CC} - \beta_{A,CA}\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
    \begin{matrix}
    y'_{t}=1 \\ y'_{t}=2 \\ y'_{t}=3
    \end{matrix}
\end{matrix}\]

where \(\beta_{C,CC}\) is the probability that, being a cub, and individual is classified with both criteria, \(\beta_{C,CA}\) is the probability that, being a cub, an individual is classified as cub with P1 and as adult with P2, \(\beta_{A,CC}\) is the probability that, being an adult, an individual is classified as cub with both criteria, \(\beta_{A,CA}\) is the probability that, being an adult, an individual is classified as cub with P1 and as adult with P2.

The observation matrix is the product of the two detection and classification matrices \(\mathbf{\Omega_1} \mathbf{\Omega_2}\):
\[\begin{matrix}
& \\
\mathbf{\Omega} =
    \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
    y_t=1 & y_t=2 & y_t=3 & y_t=4\\ \hdashline
1-p_C & p_C \beta_{C,CC} & p_C \beta_{C,CA} & p_C (1 - \beta_{C,CC} - \beta_{C,CA})\\
1-p_A & p_A \beta_{A,CC} & p_A \beta_{A,CA} & p_A (1 - \beta_{A,CC} - \beta_{A,CA})\\
1 & 0 & 0 & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
    \begin{matrix}
    z_{t}=1 \\ z_{t}=2 \\ z_{t}=3
    \end{matrix}
\end{matrix}\]

In code, we write \(\mathbf{\Omega_1} \mathbf{\Omega_2}\) directly (faster than doing the matrix multiplication at each iteration). For simplicity we use a single detection probability \(p\) for both ages, but you can let \(p\) vary by age/sex/year or sampling method, as in the paper:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observation matrix}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ betaCCC}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ betaCCA}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaCCC }\SpecialCharTok{{-}}\NormalTok{ betaCCA)}

\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p}
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ betaACC}
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ betaACA}
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ betaACC }\SpecialCharTok{{-}}\NormalTok{ betaACA)}

\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

At first detection, the event `not detected' is impossible by construction (we condition on the first detection). We therefore use an initial observation matrix \(\mathbf{\Omega}_{\text{init}}\) with \(p\) set to 1 (not shown).

The likelihood and priors are handled as usual.

\subsection{Results}\label{results-1}

Here are the raw results:

\begin{verbatim}
##         mean   sd 2.5%  50% 97.5% Rhat n.eff
## betaACA 0.05 0.02 0.02 0.04  0.08 1.00  4114
## betaACC 0.02 0.01 0.00 0.02  0.05 1.00  4211
## betaCCA 0.57 0.12 0.33 0.58  0.79 1.00   966
## betaCCC 0.30 0.10 0.13 0.29  0.50 1.00   998
## p       0.71 0.04 0.63 0.71  0.79 1.00  3329
## phiA[1] 0.88 0.03 0.80 0.88  0.94 1.00  4195
## phiA[2] 0.82 0.06 0.69 0.82  0.92 1.00  3643
## phiC    0.53 0.13 0.28 0.53  0.78 1.00  3563
## pi      0.27 0.09 0.12 0.26  0.49 1.02  1416
\end{verbatim}

which we can arrange in a table to compare them with the results obtained by \citet{Gervasi2017}:
\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:unnamed-chunk-390}Comparison of parameter estimates for the same HMM to account for age uncertainty: our NIMBLE fit vs.~Gervasi et al.~(2017, Table 3). Intervals are 95\% credible intervals for NIMBLE and confidence intervals for Gervasi et al.. Detection in Gervasi et al.~varies by design/year; we report their overall average.\}
\centering

\begin{tabular}[t]{l|c|c}
\hline
Quantity & Our (NIMBLE) & Gervasi et al. (2017)\\
\hline
$\phi_C$ (cub survival) & 0.53 (0.28â€“0.78) & 0.51 (0.22â€“0.79)\\
\hline
$\phi_{A,f}$ (adult female) & 0.88 (0.80â€“0.94) & 0.92 (0.87â€“0.95)\\
\hline
$\phi_{A,m}$ (adult male) & 0.82 (0.69â€“0.92) & 0.85 (0.76â€“0.91)\\
\hline
$p$ (detection) & 0.71 (0.63â€“0.79) & â‰ˆ0.71 (0.62â€“0.79)\\
\hline
$\pi$ (cub at first detection) & 0.27 (0.12â€“0.49) & 0.062 (0.007â€“0.396)\\
\hline
$C_{c,cc}$ (cub by both) & 0.30 (0.13â€“0.50) & 0.40 (0.15â€“0.72)\\
\hline
$C_{c,ca}$ (cub by P1 only) & 0.57 (0.33â€“0.79) & 0.60 (0.28â€“0.85)\\
\hline
$C_{a,cc}$ (adultâ†’cub by both) & 0.02 (0.00â€“0.05) & 0.067 (0.024â€“0.174)\\
\hline
$C_{a,ca}$ (adultâ†’cub by P1 only) & 0.05 (0.02â€“0.08) & 0.191 (0.110â€“0.309)\\
\hline
\end{tabular}

\textbackslash end\{table\}

Our NIMBLE fit reproduces the main biological signals reported for the Apennine brown bear. Survival shows the expected ordering, that cubs \textless\textless{} adults, and adult females \textgreater{} adult males -- with estimates that closely match the published analysis. The credible/confidence intervals overlap broadly, indicating good agreement.

Our detection estimate aligns with the study's average detection, even though their analysis lets detection vary by design and year rather than assuming a single constant value.

For age classification (linking the hidden age to P1/P2 outcomes), mapping our parameters to theirs, we found lower adult-as-cub misclassification than the paper (and thus a higher probability that detected adults are correctly called `adult by both'), which still matches their qualitative take: the stricter criterion (P2) performs very well for adults.

The main divergence is in the initial state mix. Intervals overlap, but the means differ. A likely explanation is model structure: we used a single, constant detection probability, while the published analysis allows detection to vary by sampling design and year -- and detectability does vary meaningfully across methods and years. Collapsing this heterogeneity into one \(p\) can nudge the mixture at first detection toward a higher cub fraction to explain patterns that the richer detection model attributes to design/year effects.

In conclusion, we managed to recover the core findings of the published study (age- and sex-specific survival and average detection). Small discrepancies appear mainly in adult misclassification rates and \(\pi\). To mirror \citet{Gervasi2017} even more closely, the natural next step is to let \(p\) vary by sampling design/year (an information I did not have), as in their best supported model.

\section{Uncertainty in sex}\label{uncertainty-in-sex}

\subsection{Motivation}\label{motivation-2}

Many species show sex differences in demographic parameters, so mislabeling sex or discarding uncertain cases can bias the very patterns we want to estimate. In monomorphic birds for example, field sexing relies on behaviour (e.g., copulation posture, courtship displays, relative body size), and those clues vary in reliability; in the Audouin's gull (\emph{Larus audouinii}) study that motivates this section, \textasciitilde80\% of individuals were never sexed in the field, making ``known-sex only'' analyses both wasteful and biased toward higher survival.

\citet{pradel2008sex} (see also \citet{genovart_exploiting_2012}) turned this problem into a HMM inference task: treat sex as a hidden state (male/female/dead) and model the sex-assignment process itself. The model developed by Pradel, Genovart and colleagues separates (i) the probability of attempting a sex judgment (with a time trend, because sexing effort increased) from (ii) criterion-specific accuracy for four behavioural clues (copulation, courtship feeding, begging for food, body size).

In brief, the message is: when field classifications are uncertain, exploit them rather than filter them out---use HMM to couple biological states to imperfect observations and propagate classification uncertainty into survival.

That philosophy mirrors the age-uncertainty Section \ref{ageuncertainty} we just completed: there, age was hidden and P1/P2 provided noisy age labels; here, sex is hidden and multiple behavioural criteria provide noisy sex labels.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation-2}

To estimate Audouin's gull survival, we have at our disposal a dataset with 4093 marked birds from a colony at the Ebro Delta (Spain), collected over 10 years. To build up our model, we first define the states and observations:

\begin{itemize}
\tightlist
\item
  states

  \begin{itemize}
  \tightlist
  \item
    alive as male (M)
  \item
    alive as female (F)
  \item
    dead (D)
  \end{itemize}
\item
  observations

  \begin{itemize}
  \tightlist
  \item
    not seen (1)
  \item
    judged from copulation to be male (2)
  \item
    judged from begging food to be male (3)
  \item
    judged from coutship feeding to be male (4)
  \item
    judged from body size to be male (5)
  \item
    judged from copulation to be female (6)
  \item
    judged from begging food to be female (7)
  \item
    judged from coutship feeding to be female (8)
  \item
    judged from body size to be female (9)
  \item
    not judged (10).
  \end{itemize}
\end{itemize}

Now we turn to writing our model.

We start with the vector of initial states. Unknown-sex individuals enter as a mixture of male/female with probabilities \(\pi\) and \(1-\pi\):
\[\begin{matrix}
& \\
\mathbf{\delta} =
  \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=M & z_t=F & z_t=D \\ \hdashline
          \pi & 1 - \pi & 0\\
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
\begin{matrix}
\end{matrix}
\end{matrix}\]

We proceed with the transition matrix that contains the survival probabilities. A male either survives next year as a male (\(\phi_M\)), or dies; the same happens for females (with probability \(\phi_F\)):
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=M & z_t=F & z_t=D \\ \hdashline
          \phi_M  & 0 & 1 - \phi_M\\
          0 & \phi_F & 1 - \phi_F\\
          0 & 0 & 1
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=M \\ z_{t-1}=F \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

The vector of initial states and the matrix of transition probabilities are written as usual in NIMBLE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pi}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi}
\NormalTok{  delta[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
  
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiM            }
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}               
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiM        }
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}               
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phiF            }
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phiF        }
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}               
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}               
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}               
\end{Highlighting}
\end{Shaded}

Last step is the observation matrix, which we write as follows (its transposed for convenience):
\[\begin{matrix}
& \\
\mathbf{\Gamma'} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=M & z_t=F & z_t=D \\ \hdashline
          1 - p  & 1 - p & 1\\
          p e (1-m_4) m_1 x_1 & p e (1-m_4) m_1 (1-x_1) & 0\\
          p e (1-m_4) m_2 x_2 & p e (1-m_4) m_2 (1-x_2) & 0\\
          p e (1-m_4) m_3 x_3 & p e (1-m_4) m_3 (1-x_3) & 0\\
          p e m_4 x_4 & p e m_4 (1-x_4) & 0\\
          p e (1-m_4) m_1 (1-x_1) & p e (1-m_4) m_1 x_1 & 0\\
          p e (1-m_4) m_2 (1-x_2) & p e (1-m_4) m_2 x_2 & 0\\
          p e (1-m_4) m_3 (1-x_3) & p e (1-m_4) m_3 x_3 & 0\\
          p e m_4 (1-x_4) & p e m_4 x_4 & 0\\
          p (1-e) & p (1-e) & 0\\
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right )
\begin{matrix}
y_{t}=1 \\ y_{t}=2 \\ y_{t}=3 \\ y_{t}=4 \\ y_{t}=5 \\ y_{t}=6 \\ y_{t}=7 \\ y_{t}=8 \\ y_{t}=9 \\ y_{t}=10
\end{matrix}
\end{matrix}\]

This matrix is a bit more complex than what we have encountered before, and needs some explanations. When an individual is encountered (with probability \(p\)), there is some chance that it will be sexed (\(e\)) based on its general appearance (body size: probability \(m_4\)) or its behaviour (probability \(1 - m_4\)). When behaviour is used, there are three different criteria (copulation, begging for food and courtship feeding), which occur with probabilities \(m_1\), \(m_2\), and \(m_3\), respectively. Finally, each criterion has its own reliability (probabilities \(x_1\), \(x_2\), \(x_3\), \(x_4\), where \(x_4\) is the reliability of the body-size criterion).

I admit that this matrix representation can be difficult to grasp at once. It can be easier to split the observation matrix in several steps, like encountering, sexing, body size sexing, sex determination and correctness, then to sketch a decision treeMaybe a binary tree like in Figure 2 in \citet{genovart_exploiting_2012} (see also appendix S1 in \citet{genovart_exploiting_2012}).

In code, this gives:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p                                  }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{]      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{2}\NormalTok{]      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{3}\NormalTok{]      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{4}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{4}\NormalTok{]      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{6}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{1}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{2}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{8}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{3}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{9}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{4}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{4}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{10}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ e)      }

\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p                                  }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{1}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{2}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{3}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{4}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ x[}\DecValTok{4}\NormalTok{])      }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{6}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{1}\NormalTok{]      }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{7}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{2}\NormalTok{]     }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ m[}\DecValTok{4}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{3}\NormalTok{]   }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{9}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ e }\SpecialCharTok{*}\NormalTok{ m[}\DecValTok{4}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ x[}\DecValTok{4}\NormalTok{]   }
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{10}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ e)      }

\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}                                  
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{6}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{7}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{8}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{9}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{10}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}     
\end{Highlighting}
\end{Shaded}

At first detection, the observation `not detected' is impossible by construction (we condition on the first detection). We therefore use an initial observation matrix \(\mathbf{\Omega}_{\text{init}}\) with \(p\) set to 1 (not shown).

The likelihood and priors are handled as usual.

\subsection{Results}\label{results-2}

Here are the raw results:

\begin{verbatim}
##      mean   sd 2.5%  50% 97.5% Rhat n.eff
## e    0.07 0.00 0.07 0.07  0.07 1.01   589
## m[1] 0.29 0.02 0.26 0.29  0.32 1.00   913
## m[2] 0.60 0.02 0.56 0.60  0.63 1.00   892
## m[3] 0.11 0.01 0.09 0.11  0.14 1.00   817
## m[4] 0.17 0.01 0.14 0.17  0.19 1.01   949
## p    0.65 0.00 0.64 0.65  0.66 1.00   394
## phiF 0.90 0.01 0.89 0.90  0.91 1.01   205
## phiM 0.89 0.01 0.88 0.89  0.90 1.01   279
## pi   0.52 0.01 0.50 0.52  0.55 1.00   291
## x[1] 0.52 0.03 0.45 0.52  0.58 1.01   915
## x[2] 0.55 0.02 0.50 0.55  0.60 1.01   725
## x[3] 0.54 0.05 0.44 0.55  0.65 1.00   927
## x[4] 0.58 0.04 0.50 0.58  0.67 1.00   753
\end{verbatim}

which we can arrange in a table to compare them with the results obtained by \citet{pradel2008sex}:

\begin{table}

\caption{\label{tab:unnamed-chunk-394}Comparison with Pradel et al. (2007) Table 5, Model B (no genetically sexed anchors). Our error rates are 1 âˆ’ x[i]. Pradel's m4 is time-varying; the estimates provided by the main author for m4(t) were for t=1..10: 0.001, 0.002, 0.004, 0.007, 0.014, 0.026, 0.047, 0.084, 0.146, 0.242.}
\centering
\begin{tabular}[t]{l|l|c|c}
\hline
  & Quantity & Our (NIMBLE) & Pradel et al. (2007) Table 5, Model B\\
\hline
pi & $\mu$ (proportion male) & 0.52 (0.50â€“0.55) & 0.5325 (SE 0.0286)\\
\hline
phiF & $\phi_F$ (female survival) & 0.90 (0.89â€“0.91) & 0.9122 (SE 0.0142)\\
\hline
phiM & $\phi_M$ (male survival) & 0.89 (0.88â€“0.90) & 0.8597 (SE 0.0141)\\
\hline
m[1] & $m_1$ (Copulation) & 0.29 (0.26â€“0.32) & 0.2904 (const.)\\
\hline
m[2] & $m_2$ (Begging) & 0.60 (0.56â€“0.63) & 0.5973 (const.)\\
\hline
m[3] & $m_3$ (Courtship feeding) & 0.11 (0.09â€“0.14) & 0.1123 (const.)\\
\hline
m[4] & $m_4$ (Body size) & 0.17 (0.14â€“0.19) & time-varying (see caption)\\
\hline
x[1] & Error (Copulation) & 0.48 (0.42â€“0.55) & 0.0577 (SE 0.0407)\\
\hline
x[2] & Error (Begging) & 0.45 (0.40â€“0.50) & 0.0561 (SE 0.0308)\\
\hline
x[3] & Error (Courtship feeding) & 0.46 (0.35â€“0.56) & 0.0000 (SE 0.1553)\\
\hline
x[4] & Error (Body size) & 0.42 (0.33â€“0.50) & 0.0928 (SE 0.0741)\\
\hline
\end{tabular}
\end{table}

Our NIMBLE fit recovers the main biological signals reported for the Audouin's gulls analysis. Survival shows the expected ordering -- adult females â‰¥ adult males -- with broadly overlapping intervals and means that are close to the published model. In our run, the female--male gap is a bit smaller (â‰ˆ0.90 vs 0.89), but the direction matches the paper's message (higher female survival).

We used a single, constant detection probability and obtained \(pâ‰ˆ0.65\), which sits near the middle of the yearâ€specific values reported in the paper.

The estimated probabilities of which clue is used align closely with the published analysis for the three `behavioural' criteria: \(m_1 \approx 0.29\) (copulation), \(m_2 \approx 0.60\) (begging), \(m_3 \approx 0.11\) (courtship feeding). The paper lets body size use vary over time; our constant estimate (\(m_4 \approx 0.17\)) should be read as a pooled average across years---consistent with their finding of a low but increasing use of this criterion.

The main tension with lies in the error rates by criterion. The published analysis finds low misclassification (roughly 5--10\% for most criteria, with a very imprecise estimate for courtship feeding). Our fit suggests substantially higher error across criteria. That discrepancy is where we expect sensitivity when there are no knownâ€sex anchors (and some components are simplified). Without anchors, the model can admit dual solutions (swapping sex labels and flipping error probabilities) that fit nearly equally well; small modelling choices or initial values can tip the chain toward one mode, see \citet{pradel2008sex}.

Our estimate of the proportion of males at first encounter (\(\mu \approx 0.52\)) agrees closely with the published result, reinforcing that the state mix is well constrained by the data even when sex is uncertain.

To conclude, the sexâ€uncertainty HMM retrieves the core demographic pattern (female â‰¥ male survival) and matches the paper on overall detectability and the use of criteria. To mirror the published results more closely, one could add a small anchor set of genetically sexed individuals (an information used in \citet{pradel2008sex} but that I did not have).

\chapter{Addressing model lack of fit}\label{lackoffit}

\section{Introduction}\label{introduction-9}

Capture--recapture models rely on assumptions that must hold for inference to be reliable. In Section \ref{gof}, we saw what can go wrong and how to diagnose those issues. In this chapter, we turn to remedies and show how to fix them by building models that explicitly accommodate lack of fit. We focus on three themes: trap-dependence (detection today affects detection tomorrow), memory in the state process (the Markov assumption is too short -- transitions depend on more than the current state), and individual heterogeneity (persistent differences among animals).

\section{Accounting for trap-dependence}\label{trapdep}

\subsection{Motivation}\label{motivation-3}

Capture--recapture inference can go sideways when detection depends on past capture. If animals become trapâ€shy (avoid traps) or trapâ€happy (seek them) after being caught, the independence assumption breaks, and survival or detection can be biased if we ignore it. Tests for this trapâ€dependence exist, see Section \ref{gof}, and surveys show it's common across taxa -- roughly 70\% of reviewed studies reported evidence for it \citet{pradeltrapdep2012}.

The fix is to encode the behavioural response. Following \citet{pradeltrapdep2012}, we treat trapâ€awareness as a (possibly temporary) state: right after a capture, individuals move to a ``trapâ€aware'' state with its own detection probability, so that detection at time \(t+1\) can differ for animals caught vs.~not caught at \(t\). This HMM formulation avoids awkward data splitting, plays nicely with age or covariates, and is equivalent to the classic approach when both are fitted to the same data. Importantly, in the authors' example, ignoring trapâ€dependence underestimated survival, illustrating why modelling trap-dependence matters.

Here, we adopt that logic in NIMBLE: define a trapâ€aware state, let detection depend on it, and keep the rest of the HMM machinery unchanged.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation-3}

For our example, we'll work with data from 519 encounter histories of Cory's shearwaters (\emph{Calonectris diomedea}) marked as adults between 2001 and 2008 at Pantaleu Islet (Balearic Archipelago, Spain). If you perform goodness-of-fit tests on this dataset, as in Section \ref{gof}, you will end up with a significant trap-dependence effect (as well as a transience effect). To build up our model and explicitely account for this lack of fit, we first define the states and observations:

\begin{itemize}
\item
  states\\
  - A `trap aware' is the original state of an individual when it is first released after marking\\
  - U `trap unaware' which follows any occasion where it is not captured\\
  - D is for dead
\item
  observations\\
  - not detected (1)\\
  - detected (2)
\end{itemize}

Now we turn to writing our model.

We start with the vector of initial states. Individuals enter as trap-aware with probability 1:
\[\begin{matrix}
& \\
\mathbf{\delta} =
\left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=A & z_t=U & z_t=D \\ \hdashline
1 & 0 & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
\begin{matrix}
\end{matrix}
\end{matrix}\]

We proceed with the transition matrix:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=A & z_t=U & z_t=D \\ \hdashline
\phi p' & \phi (1-p') & 1 - \phi\\
\phi p & \phi (1-p) & 1 - \phi\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=A \\ z_{t-1}=U \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

You might have noticed something ususual in that \(\mathbf{\Gamma}\) contains both the survival and detection probabilities. Why is that? We track three hidden states from the end of session \(t\) to \(t+1\): \(A =\) trap-aware (the bird was just caught at \(t\)), \(U =\) trap-unaware (not caught at \(t\)), and \(D =\) dead. Between sessions, everyone either survives with probability \(\phi\) or dies with \(1 - \phi\) (which moves them to \(D\), an absorbing state). If alive at \(t+1\), the awareness flag for the next session is set by whether the bird gets caught at \(t+1\): those caught will be \(A\) next time, those not caught will be \(U\). Because capture probabilities can differ by current awareness, we allow \(p'\) for birds that were \(A\) at \(t\) and \(p\) for birds that were \(U\) at \(t\). If \(p'>p\) you have trap-happy behaviour (recently caught birds are easier to catch next time); if \(p'<p\), it's trap-shy. This `survive â†’ update awareness via capture' factorization is what the transition matrix formalizes.

Following the paper, adult survival is age-specific to account for a transient effect, see Section \ref{gof}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transition matrix}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in}\NormalTok{ first[i]}\SpecialCharTok{:}\NormalTok{(K}\DecValTok{{-}1}\NormalTok{))\{}
\NormalTok{      phi[i,t] }\OtherTok{\textless{}{-}}\NormalTok{ beta[age[i,t]] }\CommentTok{\# beta1 = phi1, beta2 = phi2+}
\NormalTok{      gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t] }\SpecialCharTok{*}\NormalTok{ pprime}
\NormalTok{      gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pprime)}
\NormalTok{      gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i,t]}
\NormalTok{      gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t] }\SpecialCharTok{*}\NormalTok{ p}
\NormalTok{      gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}}\NormalTok{ phi[i,t] }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p)}
\NormalTok{      gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi[i,t] }
\NormalTok{      gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{      gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{0} 
\NormalTok{      gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{,i,t] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{    \}}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

In the code above, age is passed in the data and created as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Age effect via an individual x time covariate and use of nested indexing }
\CommentTok{\# to distinguish survival over the interval after first detection from survival afterwards: }
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, }\AttributeTok{nrow =} \FunctionTok{nrow}\NormalTok{(y), }\AttributeTok{ncol =} \FunctionTok{ncol}\NormalTok{(y) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(age))\{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(age))\{}
    \ControlFlowTok{if}\NormalTok{ (j }\SpecialCharTok{==}\NormalTok{ first[i]) age[i,j] }\OtherTok{\textless{}{-}} \DecValTok{1} \CommentTok{\# age = 1}
    \ControlFlowTok{if}\NormalTok{ (j }\SpecialCharTok{\textgreater{}}\NormalTok{ first[i]) age[i,j] }\OtherTok{\textless{}{-}} \DecValTok{2}  \CommentTok{\# age \textgreater{} 1}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Last step is the observation matrix:
\[\begin{matrix}
& \\
\mathbf{\Omega} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
y_t=1 & y_t=2 \\ \hdashline
0 & 1 \\
1 & 0 \\
1 & 0 
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t}=A \\ z_{t}=U \\ z_{t}=D
\end{matrix}
\end{matrix}\]

If an animal is trap-aware at \(t\), that means that it has just been captured. If it is trap unaware or dead, it has not been captured during this session. In code, we write:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observation matrix}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

The likelihood and priors are handled as usual.

\subsection{Results}\label{results-3}

Here are the raw results with trap-dependence:

\begin{verbatim}
##        mean   sd 2.5%  50% 97.5% Rhat n.eff
## p      0.45 0.06 0.34 0.45  0.56 1.00   126
## phi1   0.76 0.02 0.71 0.76  0.80 1.00   395
## phi2   0.87 0.02 0.84 0.87  0.91 1.01   173
## pprime 0.79 0.02 0.75 0.79  0.82 1.00   183
\end{verbatim}

Our estimates are very (very) similar to those obtained by \citet{pradeltrapdep2012}, check out their Table 1. Interestingly, when we fit the same model by ignoring trap-dependence, we get the following results:

\begin{verbatim}
##      mean   sd 2.5%  50% 97.5% Rhat n.eff
## p    0.78 0.02 0.75 0.78  0.81 1.00   489
## phi1 0.74 0.02 0.70 0.74  0.79 1.00   615
## phi2 0.84 0.01 0.82 0.84  0.87 1.01   569
\end{verbatim}

When compared the our previous estimates, we see that ignoring trap-dependence leads to an underestimation of survival, either \(\phi_1\) the transient survival or \(\phi_{2}\) the resident survival.

The approach described here can be conveniently extended to several sites or states, see \citet{pradeltrapdep2012}.

\section{Allowing your Markov models to remember}\label{memorymodel}

\subsection{Motivation}\label{motivation-4}

How do we make our models remember? So far, the dynamics of our states have been first-order Markov: the state an animal moves to next depends only on the state (site) it occupies now. But if we think of states as sites, transitions are dispersal or migration, and many species show site fidelity or directional return that reflects where they were two steps back.

To relax the first-order assumption, we move to a second-order Markov description, and what happens at \(t+1\) depends on the site at \(t\) and at \(tâˆ’1\). This idea, often called a memory model, was introduced in multisite capture--recapture by \citet{hestbeck1991estimates} and \citet{BrownieEtAl1993}, then formulated cleanly within the HMM framework by \citet{rouan2009memory} and \citet{pradel_multievent_2005}.

To illustrate the memory model, we will use the geese data introduced in Chapter \ref{dispersal}. As we saw in Section \ref{gofas}, there is a strong positive association between the site where an animal was last seen and the site where it will next be seen. This suggests that a first-order model is too simple and that transitions at \(t+1\) should not only depend on \(t\) but also \(t-1\).

HMMs make this extension relativement straightforward: we redefine the latent states as pairs of sites (e.g., AA, AB, BA, BB plus dead), which encodes the second-order dependency while keeping the coding and model structure familiar. Let's get to it.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation-4}

As a reminder, the two main ingredients for a HMM capturing dispersal between 2 sites are a transition matrix:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=A & z_t=B & z_t=D \\ \hdashline
\phi^A (1-\psi^{AB}) & \phi^A \psi^{AB} & 1 - \phi^A\\
\phi^B \psi^{BA} & \phi^B (1-\psi^{BA}) & 1 - \phi^B\\
0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=A \\ z_{t-1}=B \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

and an observation matrix:
\[\begin{matrix}
& \\
\mathbf{\Omega} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
y_t=1 & y_t=2 & y_t=3 \\ \hdashline
1 - p^A & p^A & 0\\
1 - p^B & 0 & p^B\\
1 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t}=A \\ z_{t}=B \\ z_{t}=D
\end{matrix}
\end{matrix}\]

From here, how to come up with a HMM formulation of the memory model? We need to keep track of the sites previously visited, and to do so, the trick is to consider states as being pairs of sites occupied:

\begin{itemize}
\tightlist
\item
  States

  \begin{itemize}
  \tightlist
  \item
    AA is for alive in site A at \(t\) and alive in site A at \(t-1\)\\
  \item
    AB is for alive in site A at \(t\) and alive in site B at \(t-1\)\\
  \item
    BA is for alive in site B at \(t\) and alive in site A at \(t-1\)\\
  \item
    BB is for alive in site B at \(t\) and alive in site B at \(t-1\)\\
  \item
    D is for dead
  \end{itemize}
\item
  Observations

  \begin{itemize}
  \tightlist
  \item
    1 not captured\\
  \item
    2 captured at site A\\
  \item
    3 captured at site B
  \end{itemize}
\end{itemize}

Now we turn to writing our model.

We start with the vector of initial states. Individuals enter the study with probabilities:
\[\begin{matrix}
& \\
\mathbf{\delta} =
\left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=AA & z_t=AB & z_t=BA & z_t=BB &z_t=D \\ \hdashline
\pi^{AA} & \pi^{AB} & \pi^{BA} & \pi^{BB} & 0\\
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
\begin{matrix}
\end{matrix}
\end{matrix}\]

where \(\pi^{ij}\) is the probability of being alive at site \(j\) when first captured and site \(i\) before, and \(\pi^{BB} = 1 - (\pi^{AA} + \pi^{AB} + \pi^{BA})\).

We proceed with the transition matrix that contains the survival and movement probabilities:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=AA & z_t=AB & z_t=BA & z_t=BB & z_t=D \\ \hdashline
\phi^{AAA} & \phi^{AAB} & 0 & 0 & 1 - \phi^{AAA} - \phi^{AAB}\\
0 & 0 & \phi^{ABA} & \phi^{ABB} & 1 - \phi^{ABA} - \phi^{ABB}\\
\phi^{BAA} & \phi^{BAB} & 0 & 0 & 1 - \phi^{BAA} - \phi^{BAB}\\
0 & 0 & \phi^{BBA} & \phi^{BBB} & 1 - \phi^{BBA} - \phi^{BBB}\\
0 & 0 & 0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right )
\begin{matrix}
z_{t-1}=AA \\ z_{t-1}=AB \\ z_{t-1}=BA \\ z_{t-1}=BB \\ z_{t-1}=D 
\end{matrix}
\end{matrix}\]

where \(\phi^{ijk}\) is the probability to be in site \(k\) at time \(t + 1\) for an individual
present in site \(j\) at \(t\) and in site \(i\) at \(t - 1\).

An alternate parameterization for \(\mathbf{\Gamma}\) is:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
z_t=AA & z_t=AB & z_t=BA & z_t=BB & z_t=D \\ \hdashline
\phi \psi^{AAA} & \phi (1 - \psi^{AAA}) & 0 & 0 & 1 - \phi\\
0 & 0 & \phi (1 - \psi^{ABB}) & \phi \psi^{ABB} & 1 - \phi\\
\phi \psi^{BAA} & \phi (1 - \psi^{BAA}) & 0 & 0 & 1 - \phi\\
0 & 0 & \phi (1-\psi^{BBB}) & \phi \psi^{BBB} & 1 - \phi\\
0 & 0 & 0 & 0 & 1
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right )
\begin{matrix}
z_{t-1}=AA \\ z_{t-1}=AB \\ z_{t-1}=BA \\ z_{t-1}=BB \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

in which \(\phi\) is the probability of surviving from one occasion to the next, and \(\psi_{ijj}\) is the probability an animal stays at the same site \(j\) given that it was at site \(i\) on the previous occasion.

Last step is the observation matrix, which we write as follows:
\[\begin{matrix}
& \\
\mathbf{\Omega} =
\left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right .
\end{matrix}
\hspace{-1.2em}
\begin{matrix}
y_t=1 & y_t=2 & y_t=3 \\ \hdashline
1 - p^A & p^A & 0\\
1 - p^B & 0 & p^B\\
1 - p^A & p^A & 0\\
1 - p^B & 0 & p^B\\
1 & 0 & 0
\end{matrix}
\hspace{-0.2em}
\begin{matrix}
& \\
\left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \\ 12 \\ 12\end{matrix} } \right )
\begin{matrix}
z_t=AA \\ z_t=AB \\ z_t=BA \\ z_t=BB \\ z_t=D
\end{matrix}
\end{matrix}\]

To implement this model in NIMBLE, we're going to use \texttt{nimbleEcology}, which makes our life much easier as we do not have to initialize the latent states \(z\) -- remember the likelihood is marginalized, see Sections \ref{marginalization} and \ref{nimbleecologyintro}. Specifically, we use the function \texttt{dHMM()} that implements the distribution of a HMM with constant parameters. We also use a Dirichlet prior to ensure that the initial state probabilities and the survival-movement probabilities sum up to 1 and lie between 0 and 1, see Section \ref{dirichletprior}. The code is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{multisite.marginalized }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# phi111: survival{-}mov probability from state 11 to state 11}
  \CommentTok{\# phi112: survival{-}mov probability from state 11 to state 12}
  \CommentTok{\# phi121: survival{-}mov probability from state 12 to state 21}
  \CommentTok{\# phi122: survival{-}mov probability from state 12 to state 22}
  \CommentTok{\# phi211: survival{-}mov probability from state 21 to state 11}
  \CommentTok{\# phi212: survival{-}mov probability from state 21 to state 12}
  \CommentTok{\# phi221: survival{-}mov probability from state 22 to state 21}
  \CommentTok{\# phi222: survival{-}mov probability from state 22 to state 22}
  \CommentTok{\# det1: detection probability site 1}
  \CommentTok{\# det2: detection probability site 2}
  \CommentTok{\# pi11: init stat prob 11}
  \CommentTok{\# pi12: init stat prob 12}
  \CommentTok{\# pi21: init stat prob 21}
  \CommentTok{\# pi22: init stat prob 22}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (S):}
  \CommentTok{\# 1 alive 11}
  \CommentTok{\# 2 alive 12}
  \CommentTok{\# 3 alive 21}
  \CommentTok{\# 4 alive 22}
  \CommentTok{\# 5 dead}
  \CommentTok{\# Observations (O):  }
  \CommentTok{\# 1 not seen}
  \CommentTok{\# 2 seen at site 1 }
  \CommentTok{\# 3 seen at site 2}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  
  \CommentTok{\# priors}
\NormalTok{  det1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  det2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  phi11[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# phi111, phi112, 1{-}sum}
\NormalTok{  phi12[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# phi121, phi122, 1{-}sum}
\NormalTok{  phi21[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# phi211, phi212, 1{-}sum}
\NormalTok{  phi22[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{ddirch}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# phi221, phi222, 1{-}sum}
  
  \CommentTok{\# probabilities of state z(t+1) given z(t)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi11[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi11[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi11[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi12[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi12[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi12[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi21[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi21[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi21[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi22[}\DecValTok{1}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi22[}\DecValTok{2}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi22[}\DecValTok{3}\NormalTok{]}
\NormalTok{  gamma[}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  gamma[}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
  
  \CommentTok{\# probabilities of y(t) given z(t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ det1}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ det1}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ det2}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ det2}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ det1}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ det1}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ det2}
\NormalTok{  omega[}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ det2}
\NormalTok{  omega[}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  omega[}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{  omega[}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
  
  \CommentTok{\# initial state probs}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
\NormalTok{    init[i, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ gamma[ y[i, first[i] ] }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{ ] }\CommentTok{\# first state propagation}
\NormalTok{  \}}
  
  \CommentTok{\# likelihood }
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N)\{}
\NormalTok{    y[i,(first[i]}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{K] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dHMM}\NormalTok{(}\AttributeTok{init =}\NormalTok{ init[i,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],           }\CommentTok{\# count data from first[i] + 1}
                               \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],     }\CommentTok{\# observation matrix}
                               \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{],   }\CommentTok{\# transition matrix}
                               \AttributeTok{len =}\NormalTok{ K }\SpecialCharTok{{-}}\NormalTok{ first[i],           }\CommentTok{\# nb of occasions}
                               \AttributeTok{checkRowSums =} \DecValTok{0}\NormalTok{)             }\CommentTok{\# do not check whether elements in a row sum tp 1}
\NormalTok{  \}}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\subsection{Results}\label{results-4}

The raw results are:

\begin{verbatim}
##          mean   sd 2.5%  50% 97.5% Rhat n.eff
## det1     0.45 0.01 0.43 0.45  0.47 1.01   296
## det2     0.41 0.01 0.39 0.41  0.42 1.05   244
## phi11[1] 0.50 0.01 0.49 0.50  0.52 1.00   478
## phi11[2] 0.16 0.01 0.15 0.16  0.17 1.01   834
## phi11[3] 0.34 0.01 0.32 0.34  0.35 1.00   565
## phi12[1] 0.10 0.00 0.09 0.10  0.11 1.00   496
## phi12[2] 0.57 0.01 0.55 0.57  0.59 1.02   382
## phi12[3] 0.33 0.01 0.32 0.34  0.35 1.02   421
## phi21[1] 0.36 0.03 0.31 0.36  0.42 1.00   900
## phi21[2] 0.33 0.03 0.27 0.33  0.38 1.01  1149
## phi21[3] 0.31 0.03 0.24 0.31  0.37 1.01   932
## phi22[1] 0.06 0.00 0.05 0.06  0.07 1.00   676
## phi22[2] 0.63 0.01 0.61 0.63  0.64 1.01  1140
## phi22[3] 0.31 0.01 0.30 0.31  0.33 1.00   965
\end{verbatim}

which we can compare to the results obtained by \citet{pradel_multievent_2005} in his Table 1:
\textbackslash begin\{table\}

\textbackslash caption\{\label{tab:unnamed-chunk-403}Second-order (memory) estimates of transition probabilities. The first column gives the Transition made in \(t\) to \(t+1\) where M is for mid--Atlantic and C for Chesapeake. The second column gives the Condition that is whether the location at \(t+1\) is Equal or Not equal to the location at \(t-1\). The Pradel (2005) time-averaged estimates are given in the third column. Our NIMBLE estimates (mean and 95\% credible interval) are given in the fourth column. \}
\centering

\begin{tabular}[t]{l|l|c|c}
\hline
Transition & Condition & Pradel (avg 1985â€“88) & Ours\\
\hline
MM & Equal to t-1 & 0.57 & 0.50 (0.49â€“0.52)\\
\hline
MM & Not equal to t-1 & 0.33 & 0.36 (0.31â€“0.42)\\
\hline
MC & Equal to t-1 & 0.27 & 0.33 (0.27â€“0.38)\\
\hline
MC & Not equal to t-1 & 0.09 & 0.16 (0.15â€“0.17)\\
\hline
CM & Equal to t-1 & 0.21 & 0.10 (0.09â€“0.11)\\
\hline
CM & Not equal to t-1 & 0.05 & 0.06 (0.05â€“0.07)\\
\hline
CC & Equal to t-1 & 0.63 & 0.63 (0.61â€“0.64)\\
\hline
CC & Not equal to t-1 & 0.48 & 0.57 (0.55â€“0.59)\\
\hline
\end{tabular}

\textbackslash end\{table\}

In both analyses, memory is real: for each transition, the probability is higher when the destination at \(t+1\) matches where the bird was at \(tâˆ’1\) (`Equal to \(t-1\)' rows) than when it doesn't (`Not equal to \(t-1\)' rows). That's site fidelity or directional return. Our fit mirrors Pradel (2005) especially well for staying in Chesapeake (CC, equal: 0.63 in both) and still shows a clear memory signal for staying in mid-Atlantic (MM, 0.50 vs 0.57 when equal). Moves also carry memory: Mâ†’C is more likely when the bird was in C two steps back (0.33 vs 0.16), and Câ†’M shows a weaker but similar pattern (0.10 vs 0.06).

Where we differ is in the directional balance. Relative to Pradel's averages, our model leans more toward Chesapeake: we estimate higher Mâ†’C probabilities (both equal and not equal; e.g., 0.33 vs 0.27 and 0.16 vs 0.09) and lower Câ†’M (equal) (0.10 vs 0.21). We also find a higher chance of staying in C (CC, not equal: 0.57 vs 0.48). Several of these differences are well outside our 95\% credible intervals (e.g., CM equal 0.09--0.11 vs 0.21, MC not-equal 0.15--0.17 vs 0.09). The big picture remains the same, though: movements are second--order and that memory is asymmetric, being strongest for persistence in Chesapeake and for moves toward it.

\section{Accomodating individual heterogeneity}\label{accomodating-individual-heterogeneity}

\subsection{Motivation}\label{motivation-5}

I've worked with large carnivores for almost two decades, and they're what pulled me into HMMs -- especially gray wolves. Wolves are social, live in hierarchical packs, and that structure shows up in demography. It also shows up in how we detect them: dominant individuals use trails and roads more often, and that's pretty much where we search for scats for DNA-based identification. The result is individual heterogeneity in detection, with some wolves being more detectable (via DNA) than others.

Shirley Pledger in a series of papers developed so--called mixture models in which individuals are assigned in two or more classes with class-specific survival/detection probabilities. If we ignore that variation, estimates can be biased and lack-of-fit tests light up. In \citet{cubaynes_importance_2010}, we used an HMM formulation to capture heterogeneity in detection linked to social status (see also \citet{pradel2009}).

In what follows, I'll show how to build and fit these finite--mixture HMM using simulated data so we control the truth. This gives a practical template for dealing with heterogeneity whenever behavior or status makes some individuals easier (or more difficult) to find than others.

\subsection{Model and NIMBLE implementation}\label{model-and-nimble-implementation-5}

To build up our model, we first define the states and observations:

\begin{itemize}
\tightlist
\item
  States

  \begin{itemize}
  \tightlist
  \item
    alive in class 1 (A1)
  \item
    alive in class 2 (A2)
  \item
    dead (D)
  \end{itemize}
\item
  Observations

  \begin{itemize}
  \tightlist
  \item
    not captured (1)
  \item
    captured (2)
  \end{itemize}
\end{itemize}

Now we turn to writing our model.

We start with the vector of initial states. Individuals enter as a mixture of A1/A2 with probabilities \(\pi\) and \(1-\pi\):
\[\begin{matrix}
& \\
\mathbf{\delta} =
  \left ( \vphantom{ \begin{matrix} 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=A1 & z_t=A2 & z_t=D \\ \hdashline
          \pi & 1 - \pi & 0\\
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \end{matrix} } \right )
\begin{matrix}
\end{matrix}
\end{matrix}\]

where \(\pi\) is the probability of being alive in A1, and \(1 - \pi\) is the probability of being in A2.

We proceed with the transition matrix that contains the survival probabilities:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=A1 & z_t=A2 & z_t=D \\ \hdashline
          \phi  & 0 & 1 - \phi\\
          0 & \phi & 1 - \phi\\
          0 & 0 & 1
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=A1 \\ z_{t-1}=A2 \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

where \(\phi\) is the survival probability. In NIMBLE, we write:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Transition matrix}
\NormalTok{gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# A1(t){-}\textgreater{}A1(t+1)}
\NormalTok{gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A1(t){-}\textgreater{}A2(t+1)}
\NormalTok{gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# A1(t){-}\textgreater{}D(t+1)}
\NormalTok{gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A2(t){-}\textgreater{}A1(t+1)}
\NormalTok{gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# A2(t){-}\textgreater{}A2(t+1)}
\NormalTok{gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# A2(t){-}\textgreater{}D(t+1)}
\NormalTok{gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# D(t){-}\textgreater{}A1(t+1)}
\NormalTok{gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# D(t){-}\textgreater{}A2(t+1)}
\NormalTok{gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# D(t){-}\textgreater{}D(t+1)}
\end{Highlighting}
\end{Shaded}

Survival could be made heterogeneous; to do so, we'd need to amend the transition matrix as follows:
\[\begin{matrix}
& \\
\mathbf{\Gamma} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          z_t=A1 & z_t=A2 & z_t=D \\ \hdashline
          \phi (1-\psi^{12}) & \phi \psi^{12} & 1 - \phi\\
          \phi \psi^{21} & \phi (1-\psi^{21}) & 1 - \phi\\
          0 & 0 & 1
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12 \end{matrix} } \right )
\begin{matrix}
z_{t-1}=A1 \\ z_{t-1}=A2 \\ z_{t-1}=D
\end{matrix}
\end{matrix}\]

where \(\psi^{12}\) is the probability for an individual to change class of heterogeneity, from A1 to A2, and \(\psi^{21}\) is the probability for an individual to change class of heterogeneity, from A2 to A1.

Last step is the observation matrix, which we write as follows:
\[\begin{matrix}
& \\
\mathbf{\Omega} =
  \left ( \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right .
          \end{matrix}
          \hspace{-1.2em}
          \begin{matrix}
          y_t=1 & y_t=2\\ \hdashline
          1 - p^1 & p^1\\
          1 - p^2 & p^2\\
          1 & 0
          \end{matrix}
          \hspace{-0.2em}
          \begin{matrix}
          & \\
          \left . \vphantom{ \begin{matrix} 12 \\ 12 \\ 12\end{matrix} } \right )
\begin{matrix}
z_{t}=A1 \\ z_{t}=A2 \\ z_{t}=D
\end{matrix}
\end{matrix}\]

where \(p^1\) is detection for individuals in A1, and \(p^2\) that of individuals in A2. In NIMBLE, this translates into:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Observation matrix}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p1   }\CommentTok{\# A1(t){-}\textgreater{}0(t)}
\NormalTok{omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p1       }\CommentTok{\# A1(t){-}\textgreater{}1(t)}
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p2   }\CommentTok{\# A2(t){-}\textgreater{}0(t)}
\NormalTok{omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ p2       }\CommentTok{\# A2(t){-}\textgreater{}1(t)}
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}         \CommentTok{\# D(t){-}\textgreater{}0(t)}
\NormalTok{omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# D(t){-}\textgreater{}1(t)}
\end{Highlighting}
\end{Shaded}

At first detection, the observation `not detected' is impossible by construction (we condition on the first detection). We therefore use an initial observation matrix \(\mathbf{\Omega}_{\text{init}}\) with \(p_1 = p_2\) set to 1 (not shown).

Instead of real data, we simulate data, which is useful to identify problems in our analysis as we will illustrate below. We simulate a finite--mixture scenario where individuals differ in detection but share the same survival. Each animal is assigned to one of two latent classes: a small, highly detectable group (class A1; proportion \(\pi = 0.2\), \(p_1=0.8\)) and a larger, less detectable group (class A2; proportion \(1-\pi = 0.8\), \(p_2=0.3\)). Everyone starts captured at the first occasion (this mimics the CJS conditioning on first capture), then the latent alive state \(z_{i,t}\) evolves with constant survival \(\phi = 0.7\). If an individual is alive at time \(t\), it's observed with its own detection probability \(p_i\) (either \(p_1\) or \(p_2\) depending on class). The result is a matrix of 0/1 encounter histories \(y\) with built-in individual heterogeneity in detection, plus bookkeeping objects: \texttt{which\_mixture} (true class), \texttt{detection} (each animal's \(p_i\)), and \texttt{z} (true alive states). We simulate the fate of 400 animals over 10 years.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Simulate encounter histories with individual heterogeneity (finite mixture)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)           }\CommentTok{\# for reproducibility}

\CommentTok{\# {-}{-}{-} Parameters}
\NormalTok{phi }\OtherTok{\textless{}{-}} \FloatTok{0.7}               \CommentTok{\# apparent survival (same for all)}
\NormalTok{prop\_class1 }\OtherTok{\textless{}{-}} \FloatTok{0.2}       \CommentTok{\# mixture proportion pi: Pr(class 1)}
\NormalTok{p\_class1 }\OtherTok{\textless{}{-}} \FloatTok{0.8}          \CommentTok{\# detection if in class 1 (high detectability)}
\NormalTok{p\_class2 }\OtherTok{\textless{}{-}} \FloatTok{0.3}          \CommentTok{\# detection if in class 2 (low detectability)}

\NormalTok{nind }\OtherTok{\textless{}{-}} \DecValTok{400}              \CommentTok{\# number of individuals}
\NormalTok{nyear }\OtherTok{\textless{}{-}} \DecValTok{10}              \CommentTok{\# number of occasions}

\CommentTok{\# {-}{-}{-} Storage}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, nind, nyear)    }\CommentTok{\# latent alive state}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, nind, nyear)    }\CommentTok{\# observed detections (0/1, with NAs before first)}
\NormalTok{y }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{, nind, nyear)    }\CommentTok{\# final encounter histories (0/1)}
\NormalTok{first }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, nind)           }\CommentTok{\# first{-}capture occasion (here: all start at 1)}
\NormalTok{detection }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, nind)      }\CommentTok{\# each individualâ€™s p\_i}
\NormalTok{which\_mixture }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\ConstantTok{NA}\NormalTok{, nind)  }\CommentTok{\# latent class: 1 or 0 (class 2)}

\CommentTok{\# {-}{-}{-} Assign individuals to classes, then give them class{-}specific detection}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nind) \{}
\NormalTok{  which\_mixture[i] }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, prop\_class1) }\CommentTok{\# 1 with prob pi, else 0}
\NormalTok{  detection[i] }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (which\_mixture[i] }\SpecialCharTok{==} \DecValTok{1}\NormalTok{) p\_class1 }\ControlFlowTok{else}\NormalTok{ p\_class2}
\NormalTok{\}}

\CommentTok{\# {-}{-}{-} Generate latent states and observations}
\CommentTok{\# Everyone is alive and detected at first capture (CJS conditioning)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{nind) \{}
\NormalTok{  z[i, first[i]] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{  x[i, first[i]] }\OtherTok{\textless{}{-}} \DecValTok{1}

  \CommentTok{\# From the second occasion on:}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ (first[i] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nyear) \{}
    \CommentTok{\# Alive state: survive from previous if alive; else remain 0}
\NormalTok{    z[i, j] }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, phi }\SpecialCharTok{*}\NormalTok{ z[i, j }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{])}

    \CommentTok{\# Observation: if alive, detect with p\_i; if dead, 0}
\NormalTok{    x[i, j] }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, z[i, j] }\SpecialCharTok{*}\NormalTok{ detection[i])}
\NormalTok{  \}}
\NormalTok{\}}

\CommentTok{\# {-}{-}{-} Final encounter histories: replace NAs (before first) with 0}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ x}
\NormalTok{y[}\FunctionTok{is.na}\NormalTok{(y)] }\OtherTok{\textless{}{-}} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

The simulated data look like:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(y)}
\DocumentationTok{\#\#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]}
\DocumentationTok{\#\# [1,]    1    0    0    0    0    0    0    0    0     0}
\DocumentationTok{\#\# [2,]    1    0    0    0    0    0    0    0    0     0}
\DocumentationTok{\#\# [3,]    1    0    0    0    0    0    0    0    0     0}
\DocumentationTok{\#\# [4,]    1    0    1    0    0    0    0    0    0     0}
\DocumentationTok{\#\# [5,]    1    0    0    0    0    0    0    0    0     0}
\DocumentationTok{\#\# [6,]    1    1    0    0    0    0    0    0    0     0}
\end{Highlighting}
\end{Shaded}

Now when I fit the model above to these data, I get the following results:

\begin{verbatim}
##     mean   sd 2.5%  50% 97.5% Rhat n.eff
## phi 0.70 0.01 0.67 0.70  0.73 1.00  1074
## pi  0.70 0.03 0.63 0.70  0.76 1.01   722
## pp1 0.45 0.02 0.40 0.45  0.49 1.00   492
## pp2 0.49 0.03 0.43 0.48  0.55 1.01   632
\end{verbatim}

These estimates diverge markedly from the values used to simulate the data:

\begin{table}

\caption{\label{tab:unnamed-chunk-409}Comparison of posterior estimates from NIMBLE with the data-generating values for a finite--mixture HMM.}
\centering
\begin{tabular}[t]{l|c|c}
\hline
Parameter & True value & Posterior mean (95\% credible interval)\\
\hline
phi & 0.7 & 0.70 (0.67â€“0.73)\\
\hline
pi & 0.2 & 0.70 (0.63â€“0.76)\\
\hline
pp1 & 0.8 & 0.45 (0.40â€“0.49)\\
\hline
pp2 & 0.3 & 0.49 (0.43â€“0.55)\\
\hline
\end{tabular}
\end{table}

Why is that? The HMM formulation we used for capturing heterogeneity creates a limitation: individuals are not allowed to switch between classes over time (e.g., from A1 to A2), because the transition matrix does not permit it. In other words, any individual seen \textgreater{} 1 time (it's detected after the first observation occasion) can \emph{never} change class assignments away from their initial value class assignment. Why this formulation fails? The problem is subtle but fundamental: in a HMM, transitions between states are governed by a Markov process. If the transition matrix says an individual in A1 today must stay in A1 (or die), then the individual is permanently locked in that class. As a result, once an individual is assigned to a class through its initial latent state (e.g., A1), it can never change class during sampling. Even worse, any individual seen on multiple occasions (say, years 1 and 4) must stay alive during those years, the latent state cannot `jump' between classes without violating the transition constraints, therefore, the initial class assignment is fixed forever, and the MCMC sampler cannot explore the alternative class, no matter how much data supports it.

I must confess, I got stuck in that trap, and it was only because I used simulations that I could identify the problem. I then asked the NIMBLE team for help, and Daniel Turek came up with the explanation -- thanks, Daniel!

Fortunately, there are several solutions to this problem:

\begin{itemize}
\item
  Marginalize over the latent states: Instead of sampling the latent states \(z\), you can marginalize them out, which avoids the class-switching problem entirely. This is the principle behind the \texttt{nimbleEcology} package, see Section \ref{nimbleecologyintro}.
\item
  Use a class-level latent variable: Rather than coding classes as HMM states, define a latent class assignment (e.g., \texttt{g{[}i{]}\ \textasciitilde{}\ dcat(pi{[}1:2{]})}) for each individual. The HMM then conditions on that fixed class, allowing for proper inference and switching during MCMC.
\item
  Custom block sampler: Design a custom MCMC sampler that updates the entire latent history of an individual (\texttt{z{[}i,\ 1:K{]}}) at once. This allows for class-switching in a consistent way, though it's more advanced and computationally heavier.
\end{itemize}

Here I will adopt the first solution and use some material we saw in Section \ref{marginalization}. In NIMBLE, we start by writing our own functions for the HMM likelihood, using the backward algorithm:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Goal: integrate out latent states z using a custom density (forward algorithm),}
\CommentTok{\# and speed things up by pooling identical encounter histories with a weight "size".}

\CommentTok{\# Get rid of individuals for which first==K}
\NormalTok{mask }\OtherTok{\textless{}{-}} \FunctionTok{which}\NormalTok{(first}\SpecialCharTok{!=}\FunctionTok{ncol}\NormalTok{(y)) }\CommentTok{\# individuals that are not first encountered at last occasion}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y[mask, ]                }\CommentTok{\# keep only these}
\NormalTok{first }\OtherTok{\textless{}{-}}\NormalTok{ first[mask]}
\CommentTok{\# Rationale: if first == K (first seen at last occasion), there is no post{-}capture interval}
\CommentTok{\# to inform phi/p; removing them simplifies the forward recursion and avoids degenerate len=1 cases.}

\CommentTok{\# Pool encounter histories}
\NormalTok{y\_weighted }\OtherTok{\textless{}{-}}\NormalTok{ y }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by\_all}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                 \CommentTok{\# group by entire encounter history (all columns)}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{size =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}          \CommentTok{\# count how many individuals share that history}
  \FunctionTok{relocate}\NormalTok{(size) }\SpecialCharTok{\%\textgreater{}\%}                 \CommentTok{\# move the "size" column to the front}
  \FunctionTok{as.matrix}\NormalTok{()}
\FunctionTok{head}\NormalTok{(y\_weighted)}
\NormalTok{size }\OtherTok{\textless{}{-}}\NormalTok{ y\_weighted[,}\DecValTok{1}\NormalTok{] }\CommentTok{\# nb of individuals w/ a particular encounter history}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ y\_weighted[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]   }\CommentTok{\# pooled data: one row per unique history}

\CommentTok{\# Assemble data and constants}
\CommentTok{\# +1 because the custom distribution expects categories 1..2 (not 0/1)}
\NormalTok{my.data }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{y =}\NormalTok{ y }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}
\NormalTok{my.constants }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}\AttributeTok{N =} \FunctionTok{nrow}\NormalTok{(y), }
                     \AttributeTok{K =} \FunctionTok{ncol}\NormalTok{(y), }
                     \AttributeTok{first =}\NormalTok{ first,}
                     \AttributeTok{size =}\NormalTok{ size,}
                     \AttributeTok{one =} \DecValTok{1}\NormalTok{)}
\CommentTok{\# "one" is a dummy constant used in a dconstraint (to prevent label switching).}

\CommentTok{\# NIMBLE functions}
\CommentTok{\# Custom HMM *density* for pooled histories via forward algorithm (alpha recursion)}
\NormalTok{dwolfHMM }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{x =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{), }
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{), }\CommentTok{\# vector of initial states (delta)}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),  }\CommentTok{\# observation matrix (omega)}
                 \AttributeTok{probObse =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{), }\CommentTok{\# observation matrix used at first occasion (conditioning)}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}\CommentTok{\# transition matrix (gamma)}
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),     }\CommentTok{\# multiplicity of this history}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{), }\CommentTok{\# number of sampling occasions}
                 \AttributeTok{log =} \FunctionTok{integer}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) \{}
    \CommentTok{\# Initial forward probs:}
    \CommentTok{\# multiply initial state probs by the observation prob at the first datum x[1]}
    \CommentTok{\# (probObs at t=first is effectively 1 due to CJS conditioning; handled via \textquotesingle{}probObse\textquotesingle{}).}
\NormalTok{    alpha }\OtherTok{\textless{}{-}}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ probObse[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,x[}\DecValTok{1}\NormalTok{]]}\CommentTok{\# * probObs[1:3,x[1]] == 1 due to conditioning on first detection}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len) \{}
      \CommentTok{\# standard forward step: alpha\_t = (alpha\_\{t{-}1\} * Gamma) .* Omega(:, y\_t)}
\NormalTok{      alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ (alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{] }\SpecialCharTok{\%*\%}\NormalTok{ probTrans[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ probObs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,x[t]]}
\NormalTok{    \}}
    \CommentTok{\# Pooling: multiply the log{-}likelihood by \textquotesingle{}size\textquotesingle{}}
\NormalTok{    logL }\OtherTok{\textless{}{-}}\NormalTok{ size }\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(alpha[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]))}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{))}
    \ControlFlowTok{if}\NormalTok{ (log) }\FunctionTok{return}\NormalTok{(logL)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{exp}\NormalTok{(logL))}
\NormalTok{  \}}
\NormalTok{)}

\CommentTok{\# Matching *random generator* (required by NIMBLE for custom distributions)}
\CommentTok{\# Generates one synthetic history consistent with the HMM; not used in MCMC, but needed to register the dist.}
\NormalTok{rwolfHMM }\OtherTok{\textless{}{-}} \FunctionTok{nimbleFunction}\NormalTok{(}
  \AttributeTok{run =} \ControlFlowTok{function}\NormalTok{(}\AttributeTok{n =} \FunctionTok{integer}\NormalTok{(),}
                 \AttributeTok{probInit =} \FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{),}
                 \AttributeTok{probObs =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{probObse =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{probTrans =} \FunctionTok{double}\NormalTok{(}\DecValTok{2}\NormalTok{),}
                 \AttributeTok{size =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{),}
                 \AttributeTok{len =} \FunctionTok{double}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{default =} \DecValTok{0}\NormalTok{)) \{}
    \FunctionTok{returnType}\NormalTok{(}\FunctionTok{double}\NormalTok{(}\DecValTok{1}\NormalTok{))}
\NormalTok{    z }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(len) }\CommentTok{\# latent states}
\NormalTok{    z[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probInit[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\CommentTok{\# all individuals alive at t = 0 (by construction)}
\NormalTok{    y }\OtherTok{\textless{}{-}}\NormalTok{ z}
\NormalTok{    y[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{2} \CommentTok{\# all individuals are detected at t = 0 (CJS conditioning)}
    \ControlFlowTok{for}\NormalTok{ (t }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{len)\{}
      \CommentTok{\# state at t given state at t{-}1}
\NormalTok{      z[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probTrans[z[t}\DecValTok{{-}1}\NormalTok{],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }
      \CommentTok{\# observation at t given state at t}
\NormalTok{      y[t] }\OtherTok{\textless{}{-}} \FunctionTok{rcat}\NormalTok{(}\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ probObs[z[t],}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\NormalTok{    \}}
    \FunctionTok{return}\NormalTok{(y)}
\NormalTok{  \})}

\CommentTok{\# Register the custom density/generator in the global environment}
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}dwolfHMM\textquotesingle{}}\NormalTok{, dwolfHMM, .GlobalEnv)}
\FunctionTok{assign}\NormalTok{(}\StringTok{\textquotesingle{}rwolfHMM\textquotesingle{}}\NormalTok{, rwolfHMM, .GlobalEnv)}
\end{Highlighting}
\end{Shaded}

Now the NIMBLE code for the model, at last:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Model code}
\NormalTok{hmm.phipmix }\OtherTok{\textless{}{-}} \FunctionTok{nimbleCode}\NormalTok{(\{}
  
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# Parameters:}
  \CommentTok{\# pi: initial state probability C1   (mixture weight for class 1)}
  \CommentTok{\# phi: survival probability          (shared across classes)}
  \CommentTok{\# pp1: recapture probability C1      (detection for class 1)}
  \CommentTok{\# pp2: recapture probability C2      (detection for class 2)}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  \CommentTok{\# States (S):}
  \CommentTok{\# 1 alive (A1)  {-}\textgreater{} class 1}
  \CommentTok{\# 2 alive (A2)  {-}\textgreater{} class 2}
  \CommentTok{\# 5 dead (D)}
  \CommentTok{\# Observations (O):}
  \CommentTok{\# 1 neither seen nor recovered (0)}
  \CommentTok{\# 2 seen alive (1)}
  \CommentTok{\# {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
  
  \CommentTok{\# priors}
\NormalTok{  phi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior survival}
\NormalTok{  one }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dconstraint}\NormalTok{(pp1 }\SpecialCharTok{\textless{}}\NormalTok{ pp2) }\CommentTok{\# to avoid label switching (enforces an ordering)}
\NormalTok{  pp1 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection (class 1)}
\NormalTok{  pp2 }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{) }\CommentTok{\# prior detection (class 2)}
\NormalTok{  pi }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dunif}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# prob init state 1 (mixture proportion for class 1)}
  
  \CommentTok{\# transition matrix (classes are persistent; no switching between A1 and A2)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# A1(t){-}\textgreater{}A1(t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A1(t){-}\textgreater{}A2(t+1)}
\NormalTok{  gamma[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# A1(t){-}\textgreater{}D(t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A2(t){-}\textgreater{}A1(t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ phi      }\CommentTok{\# A2(t){-}\textgreater{}A2(t+1)}
\NormalTok{  gamma[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ phi  }\CommentTok{\# A2(t){-}\textgreater{}D(t+1)}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# D(t){-}\textgreater{}A1(t+1)}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# D(t){-}\textgreater{}A2(t+1)}
\NormalTok{  gamma[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# D(t){-}\textgreater{}D(t+1)}
  
  \CommentTok{\# vector of initial state probs (mixture at first capture)}
\NormalTok{  delta[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pi         }\CommentTok{\# A1(first)}
\NormalTok{  delta[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pi     }\CommentTok{\# A2(first)}
\NormalTok{  delta[}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}          \CommentTok{\# D(first)}
  
  \CommentTok{\# observation matrix (detection differs by class; dead are never seen)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pp1   }\CommentTok{\# A1(t){-}\textgreater{}0(t)}
\NormalTok{  omega[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pp1       }\CommentTok{\# A1(t){-}\textgreater{}1(t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ pp2   }\CommentTok{\# A2(t){-}\textgreater{}0(t)}
\NormalTok{  omega[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ pp2       }\CommentTok{\# A2(t){-}\textgreater{}1(t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}         \CommentTok{\# D(t){-}\textgreater{}0(t)}
\NormalTok{  omega[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}         \CommentTok{\# D(t){-}\textgreater{}1(t)}
  
  \CommentTok{\# \textquotesingle{}omegae\textquotesingle{} is used only at the first datum inside the custom density}
  \CommentTok{\# to implement CJS conditioning (everyone is detected at first capture)}
\NormalTok{  omegae[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A1(t){-}\textgreater{}0(t)}
\NormalTok{  omegae[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# A1(t){-}\textgreater{}1(t)}
\NormalTok{  omegae[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# A2(t){-}\textgreater{}0(t)}
\NormalTok{  omegae[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# A2(t){-}\textgreater{}1(t)}
\NormalTok{  omegae[}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}        \CommentTok{\# D(t){-}\textgreater{}0(t)}
\NormalTok{  omegae[}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}        \CommentTok{\# D(t){-}\textgreater{}1(t)}
  
  \CommentTok{\# likelihood}
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{N) \{}
    \CommentTok{\# One weighted likelihood contribution per *unique* encounter history,}
    \CommentTok{\# using the forward algorithm implemented in dwolfHMM.}
\NormalTok{    y[i,first[i]}\SpecialCharTok{:}\NormalTok{K] }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dwolfHMM}\NormalTok{(}\AttributeTok{probInit =}\NormalTok{ delta[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{],      }\CommentTok{\# initial state probs}
                               \AttributeTok{probObs =}\NormalTok{ omega[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{],   }\CommentTok{\# observation matrix}
                               \AttributeTok{probObse =}\NormalTok{ omegae[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{], }\CommentTok{\# obs matrix for first occasion (conditioning)}
                               \AttributeTok{probTrans =}\NormalTok{ gamma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\CommentTok{\# transition matrix}
                               \AttributeTok{size =}\NormalTok{ size[i],             }\CommentTok{\# weight: how many individuals share this history}
                               \AttributeTok{len =}\NormalTok{ K }\SpecialCharTok{{-}}\NormalTok{ first[i] }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)     }\CommentTok{\# number of occasions for this history}
\NormalTok{  \}}
\NormalTok{\})}

\CommentTok{\# Initial values (cool thing, we do not need inits for the latent states anymore!!)}
\NormalTok{initial.values }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{() }\FunctionTok{list}\NormalTok{(}\AttributeTok{phi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{),}
                                  \AttributeTok{pp1 =} \FloatTok{0.3}\NormalTok{,}
                                  \AttributeTok{pp2 =} \FloatTok{0.8}\NormalTok{,}
                                  \AttributeTok{pi =} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\# Note: \textquotesingle{}one\textquotesingle{} is provided in constants; no init needed. The pp1\textless{}pp2 constraint}
\CommentTok{\# prevents label switching by fixing the class ordering.}

\CommentTok{\# Parameters to be monitored}
\NormalTok{parameters.to.save }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"phi"}\NormalTok{, }\StringTok{"pp1"}\NormalTok{, }\StringTok{"pp2"}\NormalTok{, }\StringTok{"pi"}\NormalTok{)}

\CommentTok{\# Run NIMBLE}
\NormalTok{out }\OtherTok{\textless{}{-}} \FunctionTok{nimbleMCMC}\NormalTok{(}\AttributeTok{code =}\NormalTok{ hmm.phipmix, }
                  \AttributeTok{constants =}\NormalTok{ my.constants,}
                  \AttributeTok{data =}\NormalTok{ my.data,              }
                  \AttributeTok{inits =}\NormalTok{ initial.values,}
                  \AttributeTok{monitors =}\NormalTok{ parameters.to.save,}
                  \AttributeTok{niter =}\NormalTok{ n.iter,}
                  \AttributeTok{nburnin =}\NormalTok{ n.burnin, }
                  \AttributeTok{nchains =}\NormalTok{ n.chains)}
\end{Highlighting}
\end{Shaded}

The results look fine now:

\begin{table}

\caption{\label{tab:unnamed-chunk-412}Posterior estimates vs. data-generating values for a finite--mixture HMM, when a marginalized likelihood is used.}
\centering
\begin{tabular}[t]{l|c|c}
\hline
Parameter & True value & Posterior mean (95\% credible interval)\\
\hline
phi & 0.7 & 0.72 (0.69â€“0.75)\\
\hline
pi & 0.8 & 0.72 (0.54â€“0.87)\\
\hline
pp1 & 0.3 & 0.27 (0.18â€“0.35)\\
\hline
pp2 & 0.8 & 0.79 (0.65â€“0.92)\\
\hline
\end{tabular}
\end{table}

In summary, when modeling unobservable individual heterogeneity (e.g., detection classes) in an HMM, avoid encoding class identity as a dynamic state. Instead, treat it as a fixed latent variable, or marginalize it out. Otherwise, the model is unable to explore the full posterior and will yield biased or unreliable estimates.

A question that remains is the number of classes we should use. In other words, why 2 classes and not 3 or 4? One option is to fit models with more classes and select among them (see \citet{cubaynes2012}). Alternatively, you can take a non-parametric route and let the data decide how many classes are needed; this is relatively easy in NIMBLE (see \citet{turek_bayesian_2021}).

For broader context on individual heterogeneity, see \citet{gimenez2018ih}. Finally, remember that these hidden classes were introduced by Pledger and colleagues primarily to correct bias in survival or abundance when heterogeneity is ignored; interpreting the classes biologically after fitting should be done with caution.

\chapter{Quantifying life history traits}\label{tradeoffs}

WORK IN PROGRESS

See \url{https://www.oxfordbibliographies.com/display/document/obo-9780199830060/obo-9780199830060-0016.xml} and \url{https://en.wikipedia.org/wiki/Life_history_theory} for a definition. I think that all case studies below fall in the LH category. I might consider moving the disease ecology case study in the main Sites and states chapter. See however \url{https://onlinelibrary.wiley.com/doi/epdf/10.1111/ele.13681} for a link between disease ecology and life history theory.

\section{Age mortalities}\label{age-mortalities}

Combine dead recoveries with live recaptures. Paper by \citet{koons2014}. Roe deer data. Cite recent paper by Michael on the combination.

See also nice app by \citet{fernandez-chacon_causes_2016}, \citet{peron_evidence_2016} and \citet{marzo2011}. Extension to semi-markov in \citet{choquet_semi-markov_2011}.

Results

\begin{verbatim}
##                mean   sd  2.5%   50% 97.5% Rhat n.eff
## alpha_H_01    -1.45 0.53 -2.46 -1.41 -0.49 1.10    80
## alpha_H_ge1   -2.81 0.46 -3.78 -2.74 -2.07 1.05    55
## alpha_NH_01    0.17 0.17 -0.21  0.19  0.42 1.18   156
## alpha_NH_12   -1.99 0.39 -2.94 -1.93 -1.43 1.09   337
## beta0_NH      -2.77 0.47 -3.90 -2.68 -2.11 1.11    95
## beta1_NH       2.11 0.50  1.33  2.05  3.26 1.06   143
## ll             0.40 0.20  0.15  0.35  0.91 1.05    54
## lp_ageGT1[1]   0.02 2.00 -3.92  0.02  3.87 1.00  3335
## lp_ageGT1[2]  -0.66 0.82 -2.33 -0.64  0.91 1.00  3421
## lp_ageGT1[3]  -0.84 0.66 -2.21 -0.81  0.41 1.00  3325
## lp_ageGT1[4]   0.20 0.46 -0.71  0.19  1.13 1.00  3350
## lp_ageGT1[5]   0.41 0.41 -0.37  0.40  1.22 1.00  3492
## lp_ageGT1[6]   0.12 0.37 -0.59  0.12  0.84 1.00  3245
## lp_ageGT1[7]  -0.12 0.36 -0.82 -0.12  0.56 1.00  3358
## lp_ageGT1[8]  -0.22 0.34 -0.90 -0.21  0.42 1.00  3244
## lp_ageGT1[9]  -0.01 0.32 -0.64 -0.01  0.63 1.00  3660
## lp_ageGT1[10]  0.66 0.33  0.02  0.66  1.31 1.00  3662
## lp_ageGT1[11]  0.26 0.32 -0.37  0.26  0.88 1.00  3357
## lp_ageGT1[12] -0.35 0.32 -0.97 -0.35  0.29 1.00  3441
## lp_ageGT1[13] -0.16 0.32 -0.82 -0.16  0.47 1.00  3625
## lp_ageGT1[14]  0.14 0.31 -0.47  0.14  0.78 1.00  3502
## lp_ageGT1[15] -1.22 0.37 -1.97 -1.20 -0.51 1.00  3537
## lp_ageGT1[16] -0.08 0.30 -0.68 -0.08  0.50 1.00  3393
## lp_ageGT1[17] -0.60 0.30 -1.20 -0.59  0.00 1.00  3391
## lp_ageGT1[18] -0.57 0.29 -1.14 -0.56  0.01 1.00  3571
## lp_ageGT1[19] -0.12 0.29 -0.69 -0.12  0.44 1.00  3228
## lp_ageGT1[20]  0.31 0.28 -0.25  0.31  0.86 1.00  3451
## lp_ageGT1[21]  0.13 0.29 -0.44  0.13  0.72 1.00  3576
## lp_ageGT1[22] -0.33 0.31 -0.94 -0.33  0.26 1.00  3970
## lp_ageGT1[23]  0.23 0.31 -0.37  0.23  0.85 1.00  3339
## lp_ageGT1[24]  0.30 0.32 -0.33  0.30  0.91 1.00  3422
## lp_ageGT1[25]  0.51 0.33 -0.13  0.50  1.16 1.00  3409
## lp_ageGT1[26]  0.74 0.35  0.06  0.74  1.44 1.00  3343
## lp_ageGT1[27] -0.06 0.34 -0.74 -0.06  0.60 1.00  3375
## lp_ageGT1[28]  1.92 0.85  0.66  1.77  4.05 1.00  1473
## lp_ageGT1[29]  0.00 2.01 -3.90  0.02  3.93 1.00 14744
## lp_ageLE1[1]   2.89 1.16  0.87  2.81  5.41 1.00  3267
## lp_ageLE1[2]   2.40 1.21  0.28  2.31  4.93 1.00  3215
## lp_ageLE1[3]   0.02 0.62 -1.19  0.02  1.24 1.00  3475
## lp_ageLE1[4]   0.00 0.63 -1.23 -0.01  1.27 1.00  3409
## lp_ageLE1[5]   1.61 0.94 -0.05  1.53  3.65 1.00  3066
## lp_ageLE1[6]  -0.29 0.77 -1.85 -0.28  1.23 1.01  3181
## lp_ageLE1[7]   1.75 0.88  0.19  1.69  3.65 1.00  3515
## lp_ageLE1[8]   0.00 0.70 -1.39  0.00  1.41 1.00  3481
## lp_ageLE1[9]   0.19 0.66 -1.11  0.19  1.52 1.00  3452
## lp_ageLE1[10]  0.51 0.72 -0.84  0.48  1.96 1.00  3402
## lp_ageLE1[11]  0.88 0.80 -0.61  0.85  2.52 1.00  3567
## lp_ageLE1[12]  0.17 0.61 -1.03  0.18  1.37 1.00  3259
## lp_ageLE1[13] -0.20 0.68 -1.53 -0.20  1.13 1.00  3366
## lp_ageLE1[14]  1.04 0.80 -0.43  1.01  2.71 1.00  3115
## lp_ageLE1[15]  1.87 0.89  0.35  1.81  3.83 1.00  3259
## lp_ageLE1[16] -0.49 0.72 -1.97 -0.49  0.86 1.00  3309
## lp_ageLE1[17] -0.47 0.58 -1.64 -0.46  0.65 1.00  3309
## lp_ageLE1[18] -0.01 0.69 -1.35 -0.01  1.36 1.00  3461
## lp_ageLE1[19]  0.32 0.60 -0.85  0.32  1.51 1.00  3371
## lp_ageLE1[20]  0.19 0.61 -1.00  0.18  1.44 1.00  3241
## lp_ageLE1[21]  0.29 0.76 -1.19  0.28  1.83 1.00  3420
## lp_ageLE1[22]  0.97 1.03 -0.91  0.93  3.11 1.00  3375
## lp_ageLE1[23]  0.85 0.81 -0.65  0.82  2.55 1.00  3259
## lp_ageLE1[24]  0.49 0.71 -0.87  0.50  1.94 1.00  3434
## lp_ageLE1[25]  0.65 0.83 -0.93  0.63  2.36 1.00  3359
## lp_ageLE1[26] -0.29 0.76 -1.84 -0.28  1.19 1.00  3791
## lp_ageLE1[27]  0.39 0.90 -1.39  0.37  2.22 1.00  3151
## lp_ageLE1[28] -0.36 1.14 -2.41 -0.45  2.22 1.00  1877
## lp_ageLE1[29]  0.00 2.00 -3.94  0.00  3.98 1.00 14702
## p_ageGT1[1]    0.50 0.31  0.02  0.51  0.98 1.00  3271
## p_ageGT1[2]    0.36 0.17  0.09  0.34  0.71 1.00  3444
## p_ageGT1[3]    0.32 0.13  0.10  0.31  0.60 1.00  3552
## p_ageGT1[4]    0.55 0.11  0.33  0.55  0.76 1.00  3389
## p_ageGT1[5]    0.60 0.09  0.41  0.60  0.77 1.00  3514
## p_ageGT1[6]    0.53 0.09  0.36  0.53  0.70 1.00  3249
## p_ageGT1[7]    0.47 0.09  0.31  0.47  0.64 1.00  3368
## p_ageGT1[8]    0.45 0.08  0.29  0.45  0.60 1.00  3269
## p_ageGT1[9]    0.50 0.08  0.34  0.50  0.65 1.00  3651
## p_ageGT1[10]   0.66 0.07  0.50  0.66  0.79 1.00  3548
## p_ageGT1[11]   0.56 0.08  0.41  0.56  0.71 1.00  3357
## p_ageGT1[12]   0.42 0.08  0.27  0.41  0.57 1.00  3444
## p_ageGT1[13]   0.46 0.08  0.31  0.46  0.62 1.00  3642
## p_ageGT1[14]   0.53 0.08  0.39  0.53  0.69 1.00  3505
## p_ageGT1[15]   0.23 0.07  0.12  0.23  0.37 1.00  3578
## p_ageGT1[16]   0.48 0.07  0.34  0.48  0.62 1.00  3406
## p_ageGT1[17]   0.36 0.07  0.23  0.36  0.50 1.00  3407
## p_ageGT1[18]   0.36 0.07  0.24  0.36  0.50 1.00  3575
## p_ageGT1[19]   0.47 0.07  0.33  0.47  0.61 1.00  3295
## p_ageGT1[20]   0.57 0.07  0.44  0.58  0.70 1.00  3448
## p_ageGT1[21]   0.53 0.07  0.39  0.53  0.67 1.00  3581
## p_ageGT1[22]   0.42 0.07  0.28  0.42  0.56 1.00  3964
## p_ageGT1[23]   0.56 0.08  0.41  0.56  0.70 1.00  3419
## p_ageGT1[24]   0.57 0.08  0.42  0.57  0.71 1.00  3426
## p_ageGT1[25]   0.62 0.08  0.47  0.62  0.76 1.00  3422
## p_ageGT1[26]   0.67 0.08  0.52  0.68  0.81 1.00  3380
## p_ageGT1[27]   0.48 0.08  0.32  0.49  0.65 1.00  3390
## p_ageGT1[28]   0.85 0.09  0.66  0.85  0.98 1.00  1768
## p_ageGT1[29]   0.50 0.32  0.02  0.50  0.98 1.00 14725
## p_ageLE1[1]    0.92 0.08  0.70  0.94  1.00 1.00  3731
## p_ageLE1[2]    0.87 0.12  0.57  0.91  0.99 1.00  3336
## p_ageLE1[3]    0.50 0.14  0.23  0.51  0.78 1.00  3507
## p_ageLE1[4]    0.50 0.14  0.23  0.50  0.78 1.00  3457
## p_ageLE1[5]    0.80 0.13  0.49  0.82  0.97 1.00  3428
## p_ageLE1[6]    0.44 0.17  0.14  0.43  0.77 1.01  3162
## p_ageLE1[7]    0.82 0.11  0.55  0.84  0.97 1.00  3655
## p_ageLE1[8]    0.50 0.16  0.20  0.50  0.80 1.00  3522
## p_ageLE1[9]    0.54 0.15  0.25  0.55  0.82 1.00  3472
## p_ageLE1[10]   0.61 0.15  0.30  0.62  0.88 1.00  3427
## p_ageLE1[11]   0.68 0.15  0.35  0.70  0.93 1.00  3660
## p_ageLE1[12]   0.54 0.14  0.26  0.54  0.80 1.00  3292
## p_ageLE1[13]   0.45 0.15  0.18  0.45  0.76 1.00  3402
## p_ageLE1[14]   0.71 0.14  0.39  0.73  0.94 1.00  3293
## p_ageLE1[15]   0.84 0.11  0.59  0.86  0.98 1.00  3387
## p_ageLE1[16]   0.39 0.15  0.12  0.38  0.70 1.00  3307
## p_ageLE1[17]   0.39 0.13  0.16  0.39  0.66 1.00  3358
## p_ageLE1[18]   0.50 0.16  0.21  0.50  0.80 1.00  3492
## p_ageLE1[19]   0.57 0.13  0.30  0.58  0.82 1.00  3406
## p_ageLE1[20]   0.54 0.14  0.27  0.54  0.81 1.00  3344
## p_ageLE1[21]   0.56 0.17  0.23  0.57  0.86 1.00  3445
## p_ageLE1[22]   0.69 0.18  0.29  0.72  0.96 1.00  3487
## p_ageLE1[23]   0.68 0.16  0.34  0.69  0.93 1.00  3448
## p_ageLE1[24]   0.61 0.15  0.30  0.62  0.87 1.00  3435
## p_ageLE1[25]   0.64 0.17  0.28  0.65  0.91 1.00  3430
## p_ageLE1[26]   0.44 0.16  0.14  0.43  0.77 1.00  3846
## p_ageLE1[27]   0.58 0.19  0.20  0.59  0.90 1.00  3226
## p_ageLE1[28]   0.42 0.22  0.08  0.39  0.90 1.00  1928
## p_ageLE1[29]   0.50 0.31  0.02  0.50  0.98 1.00 14706
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-415-1.pdf}}

\section{Age breeding}\label{age-breeding}

dolphins

Results

\begin{verbatim}
##                  mean   sd 2.5%  50% 97.5% Rhat n.eff
## adultsurvival    0.96 0.01 0.94 0.96  0.98 1.00  2069
## beta[1]          0.35 0.04 0.28 0.35  0.45 1.03   656
## beta[2]          0.57 0.10 0.39 0.57  0.78 1.00  1009
## p[1]             0.63 0.03 0.58 0.63  0.69 1.00  1764
## p[2]             0.79 0.03 0.72 0.79  0.86 1.00  1123
## pi[1]            0.49 0.06 0.37 0.49  0.60 1.01  2743
## pi[2]            0.26 0.05 0.17 0.26  0.37 1.00  3412
## pi[3]            0.08 0.03 0.03 0.08  0.16 1.00  2225
## pi[4]            0.14 0.04 0.07 0.14  0.23 1.00  1931
## pi[5]            0.03 0.03 0.00 0.02  0.10 1.02  1315
## q[1]             0.59 0.05 0.49 0.59  0.69 1.01  1091
## q[2]             0.80 0.05 0.70 0.80  0.89 1.01   818
## youngsurvival[1] 0.63 0.05 0.55 0.63  0.73 1.00   808
## youngsurvival[2] 0.69 0.08 0.53 0.69  0.83 1.00  1702
\end{verbatim}

The probability of missing an offspring when a female is actually breeding was high and was greater for YOY than for calves. The probability of observing a YOY was estimated at 0.58 (95\% CI 0.46--0.68) and of observing a calf at 0.79 (95\% CI 0.59--0.90).

The detection probabilities of offspring were clearly below 1 and varied strongly over time (between 0.32 and 0.94). The detection probability also differed depending on the state of the female (Figure 3): females with a YOY or a 1â€yearâ€old calf had a greater detection probability than nonbreeding females or those with an older calf.

Female survival probability was estimated at 0.97 (95\% CI 0.96-- 0.98) and did not differ between states. Youngâ€ofâ€theâ€year and 1â€ yearâ€old calf survival was estimated at 0.66 (95\% CI 0.51--0.79), while 2â€yearâ€old calf survival was estimated at 0.45 (95\% CI 0.30--0.62).

Breeding probability varied depending on the breeding state: nonbreeding females and females that had just lost their YOY or 1â€yearâ€old calf had a low probability of giving birth (0.33, 95\% CI 0.26--0.42), whereas females that had lost their 2â€yearâ€old calf or raised it until the age of 3 had a greater probability of giving birth the next year (0.71, 95\% CI 0.45--0.88).

\section{Stopover duration}\label{stopover-duration}

\citet{guerin_advances_2017} for a comparison of method, would be great to reproduce all analyses.

Can stopover duration be framed as a lifeâ€history trait. Usual LH traits focus primarily on reproductive timing, allocation, survival, and growth. The framework is flexible enough that one could argue it qualifies --- especially when considering it as a key timing and phase length in a migratory life cycle.

From Schaub and Jenni: ``Migrating animals often divide their journey into alternating phases of migration bouts and stopping over. For investigating many questions of migration ecology it is crucial (1) to estimate the duration of stopover phases, and (2) to test whether animals of different groups differ in their stopover behavior.''

This nicely reframes stopover behavior as a stage-specific timing trait --- akin to stages like maturation or reproductive events, but in a migratory context. It influences overall survival, energy budgets, and perhaps reproductive success under certain ecological conditions.

\begin{itemize}
\tightlist
\item
  Stage timing matters: Just as age-at-first-reproduction matters, the length of each migratory phase could have fitness implications via exposure to predation, energetic costs, or reproductive timing.
\item
  Allocating resources: Extended stopovers might be linked to resource accumulation (e.g., fat gain) before migration, similar to capital breeding strategies.
\item
  Trade-offs and constraints: The length of stopovers could reflect a trade-off between rapid migration vs.~energy recovery and safety.
\end{itemize}

Results

Cumulative survival

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-419-1.pdf}}

Stopover duration

\pandocbounded{\includegraphics[keepaspectratio]{banana-book_files/figure-latex/unnamed-chunk-420-1.pdf}}

You can extend to time since marking (Pradel 2009) or even semi-Markov. See \citet{guerin_advances_2017} for more.

\chapter*{Conclusion}\label{conclusion}


I hope to have convinced you that hidden Markov models combined with the Bayesian framework are very flexible to analyse capture-recapture data. With your data, you may ask a myriad of questions. The limit is your imagination (and CPU time).

Let me leave you with a few pieces of advice. This is not rocket science, just a few things based on my own experience of building HMM to analyse data with Bayesian statistics.

\begin{itemize}
\item
  \textbf{Make your ecological question explicit.} First things first. Make sure you've spent some to time to make your ecological question explicit. This step will help you to stay on course, and make the right choices. For example, it's fine to use subsets of your data to address different questions.
\item
  Now in terms of modeling. \textbf{Think of observations and states first.} Don't jump on your keyboard right away. Spend some time thinking about your model with pen and paper. In particular make sure you have the observations and the states of your HMM. \textbf{Then write down the observation and transition matrices on paper.} Write down the transition matrix. You may act as if you had no imperfect detection. This is really what you're after, the ecological process (survival, dispersal, etc). Proceed with the observation matrix.
\item
  When it comes to model fitting with NIMBLE, \textbf{start simple}, with all parameters constant for example. Make sure convergence is reached. Then \textbf{add complexity one step at a time}. Time effect for example, or random effects, or uncertainty in the assignment of states.
\item
  \textbf{Consider doing simulations} to better understand your model. When it comes to model building, consider simulating data to better understand your model. You will always learn something on your model by seeing it an engine to generate data, instead of estimating its parameters. The nice thing with NIMBLE is that you can use your model to simulate data.
\item
  Another advice, quite general in programming, is to not try to optimize your code or to try to make it elegant right away. \textbf{Make your model work first, then think of optimization}.
\end{itemize}

\backmatter

  \bibliography{book.bib}

\printindex

\end{document}
